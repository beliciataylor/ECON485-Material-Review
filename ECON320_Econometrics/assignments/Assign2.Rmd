---
title: "Assingment 2 Econ320"
author: "Belicia Rodriguez"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## Call the necesary packages here, that way the ouput of the package loading will be hidden in the document
library(wooldridge)
library(tidyverse)
library(knitr)
```

### A modification of exercise C4 chapter 3 

Use the data in ATTEND for this exercise, and the Lab7 handout to work on these questions 

(i)	Obtain the minimum, maximum, and average values for the variables atndrte, priGPA, and ACT.  Do it this two ways to practice 

```{r, warning=FALSE}
# download data into R
data("attend", wooldridge)

# first way using summarize
attend %>% summarize(atndrte_min = min(atndrte), priGPA_min=min(priGPA), ACT_min = min(ACT), atndrte_max = max(atndrte), priGPA_max = max(priGPA), ACT_max = max(ACT), atndrte_mean = mean(atndrte), priGPA_mean = mean(priGPA), ACT_mean = mean(ACT)) %>% round(., 3) %>% gather(" ", ".") %>%kable()

# second way using select and summary
attend %>% select(atndrte, priGPA, ACT) %>% summary()
```

(ii)	Estimate the model $$atndrte = \beta_0 + \beta_1 priGPA + \beta_2 ACT + u$$, to make it easier create an object with the model and then show the summary. 
```{r}
mc4<-lm(formula = atndrte ~ priGPA + ACT, attend)

summary(mc4)
```

(iii) Calculate the predicted atndrte for all values and the mean of it. 
Then also calculate the predicted value of atndrte for the average values of the independent variables.

*Compare results.*

The results are the same. The first method finds the mean of all the fitted values, which means the values that are on the line that minimizes the amount of squared residual, where y = atndrte and $x_1$ = priGPA and $x_2$ = ACT. Naturally, that would get the mean of $\hat{y}$.

The second method first finds the mean of all the values of priGPA and ACT and then predicts the means within the linear model created in mc4. Both methods use different steps to arrive at the same conclusion for the mean of $\hat{y}$.

```{r}
# first method
(meanyhat=mean(fitted(mc4)))

# second method
means<-data.frame(priGPA = mean(attend$priGPA), ACT = mean(attend$ACT))

(yhatmeans<-predict(mc4, means))

```


(iv)	What is the predicted atndrte if priGPA= 3.65 and ACT= 20? What is the predicted difference in attendant rates between this and the one for the average independent variables calculated above?

In this case do not use the predict function for your code, retrive the coeffients and calculate the equation $$atndrte = \hat\beta_0 + \hat\beta_1 3.65 + \hat\beta_2 20 + u$$
```{r, results='hold'}

yhat2<- mc4$coefficients[1] + mc4$coefficients[2]*3.65+ mc4$coefficients[3]*20
names(yhat2)<-NULL # removes the names 
print(paste0("yhat= ", round((yhat2), 3)))
print(paste0("Difference= ", round((yhat2-yhatmeans), 3)))

```


(v) Run two more models one only for freshman another one only for sophomore. Save the results in objects mfr, msoph respectibly

```{r}
mfr <- lm(formula = atndrte ~ priGPA + ACT, attend, frosh == 1)
msoph <- lm(formula = atndrte ~ priGPA + ACT, attend, soph == 1)
```

(vi) Using in-line code complete the table below. 

The table below compare $\beta's$, $R^2$ and observations for the models estimated. 

Variables | All | Freshmen | Sophomore
----------|-----|----------|----------
$\beta_0$ | `r round(mc4$coefficient[1], 3)` | `r round(mfr$coefficient[1],3)`| `r round(msoph$coefficient[1],3)`
$\beta_1$ | `r round(mc4$coefficient[2],3)` | `r round(mfr$coefficient[2],3)`| `r round(msoph$coefficient[2],3)`
$\beta_2$ | `r round(mc4$coefficient[3],3)` | `r round(mfr$coefficient[3],3)`| `r round(msoph$coefficient[3],3)`
$R^2$ | `r round(summary(mc4)$r.squared,3)` | `r round(summary(mfr)$r.squared, 3)`| `r round(summary(msoph)$r.squared,3)`
$N$ | `r nobs(mc4)`| `r nobs(mfr)`| `r nobs(msoph)` 

(vii) Calculate the correlation between the residuals and the priGPA for the first model. 
```{r}
round(cor(mc4$residuals, attend$priGPA),10)
```

## From C10 

Use the data in HTV to answer this question. The data set includes information on wages, education, parents’ education, and several other variables for 1,230 working men in 1991. 

(i)  What is the range of the educ variable in the sample? What percentage of men completed twelfth grade but no higher grade? Do the men or their parents have, on average, higher levels of education

```{r, warning=FALSE, results='hold'}
data("htv", wooldridge)

# summary provides the mean and max of the variable therefore the range
summary(htv$educ)

# the mean of the logical expression gives the percentage of men that completed 12th grade but not higher.

print(paste0("Percentage of men with HS= ", round((mean(htv$educ==12)), 3)))

# See answers assignment 1 for this 
htv %>% select(educ, motheduc,fatheduc) %>% summarise_all(mean) %>% gather() %>% setNames(., c(" ", "Average")) %>%kable(digits=3)
```

(ii)  Estimate the regression model $$educ = \beta_0 + \beta_1motheduc + \beta_2fatheduc + u$$ by OLS and report the results in the usual form. 

(iii)  Add the variable abil (a measure of cognitive ability) to the regression from part (ii), and report the results in equation form. 

(iv) Now estimate an equation where abil appears in quadratic form

```{r}
model1 <- lm(educ ~ motheduc + fatheduc, htv)
summary(model1)

model2 <- lm(educ ~ motheduc + fatheduc + abil, htv)
summary(model2)

# model3
model3 <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), htv)
summary(model3)
```

*(ii cont) How much sample variation in educ is explained by parents’ education? Interpret the coefficient on motheduc.*

Parent's education explains `r round(summary(model1)$r.squared,3)` of the variation in education. `r round(model1$coefficient["motheduc"],3)`

*(iii cont) Does “ability” help to explain variations in education, even after controlling for parents’ education? Explain.*

Yes, because for every unit of increase in "ability", a person's years of education increases by `r round(model2$coefficient['abil'],3)`, whereas for mother's education and father's education a person's years of education only increases by less than 0.2 (`r round(model2$coefficient['motheduc'],3) ` and `r round(model2$coefficient['fatheduc'],3)` respectively). Because the coefficient for ability is higher than the coefficients for mother and father's education, therefore, ability explains more of the variation.

(v) If you have access to a statistical program that includes graphing capabilities, use the estimates in part (iv) to graph the relationship between the predicted education and abil. Set motheduc and fatheduc at their average values in the sample, 12.18 and 12.45, respectively. See handout DGP for this. 

Regression equation with values for mother and father edu as stated in the question and with the estimated betas form your regression 

```{r}
dat<-data.frame(educhat=predict(model3), abil=htv$abil)
plot<-ggplot(dat, aes(x=abil, y=educhat))+ # mapping x and y 
  geom_point(col=4, alpha=.5)+ # scatter plot defining atributes for color and transparency
  stat_function(fun=function(abil){(12.18*model3$coefficients['motheduc']) + (12.45*model3$coefficients['fatheduc']) + (abil*model3$coefficients['abil']) + I(abil^2)*model3$coefficients['I(abil^2)'] + model3$coefficients[1]}, geom="line", col=2)  + # plot the function of the relation between educ and ability
  ggtitle(paste("Predicted Education and Ability")) # title of the graph
print(plot)
```
