---
title: "econometrics lab"
author: "Belicia Rodriguez"
date: "10/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(wooldridge)
library(knitr)
library(wooldridge)
library(stargazer)
library(data.table)
library(car)
source("http://peterhaschke.com/Code/multiplot.R") # this adds the code to have many graphs next to each other when the graphs are done separed
```

#LAB2: RMarkdown, data and visualization

setting {r} settings
 - `include = FALSE` prevents code and results from appearing in the finished file; code runs in the chunk; the results can be used by other chunks.
 - `echo = FALSE` prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
  - `eval = FALSE` prevents the evaluation of the code, but not the code from appearing in the finished file. 
 - `message = FALSE` prevents messages that are generated by code from appearing in the finished file.
warning = FALSE prevents warnings that are generated by code from appearing in the finished.
 - `fig.cap = "..."` adds a caption to graphical results.

knitr options
 - `results = 'asis'` shows the results from your code as they are. th allows you to have html code showing nicely in your final document more on this later in the semester. 
 - `results='hold'` hold the results of each line of code in your chunk and shows them all together after running the chunk. 

Creating random variables

```{r}
#Generate a random variable v1 of 100 observations from the normal distribution between 0 and 1  
# set.seed(123)
v1 <- rnorm(100, 0, 1)

# Show variance two ways

## with the formula
sv1 <- (sum(v1-mean(v1))^2/99 - 1) # length of obj, length()
## with the var() function
sigmav1 <- var(v1)

#Generate a 3rd variable v3 that is a function of the first variable with additional noise. Calculate the covariance of v3 and v1. 
u <- rnorm(100, 0, 1) # noise
v3 <- 2*v1 + u
sigmav13<-cov(v1,v3)
```

histogram with density line
```{r}
ggplot(dat, aes(v2)) + 
 geom_histogram(aes(y=..density..), binwidth=1, alpha=.5, fill=3)+
  geom_density(alpha=.2, fill="#FF6666")+
  labs(title="Histogram for v2")+
  theme(plot.title = element_text(hjust = 0.5))
```

idk if this was found here
Loading Data
* you can look at the structure of the data to learn about the class of each variable and use the function `typeof()` to learn about the type within that class.

stata files
* use library(foreign) and read.dta()

shortcuts
* ctrl + L - clear console
* command + shift + K - knit document

#LAB3: Rmarkdown, data, visualization

dplyr package and it’s functions

The use of the package dplyr relies the pipe symbol %>%. This expression roughly translates to “and then”. You use it before every function of this packages in your code to makes things more efficient and not repeat the name of your dataframe every time. Below is a description of the dplyr functions.

function	Purpose
filter()	Select rows based on criteria
arrange()	order rows
select()	choose columns
distinct()	unique values of column(s)
mutate() and transmute()	Create new columns
summarise()	Find a summary statistic
sample_n()	Take a random sample
group_by()	Creates groups based on the values or categories of a variable.

Descriptive statistics tables
table(), prop.table()

```{r}
# Generate "Factors" to attach labels
haskids <- factor(affairs$kids,labels=c("no","yes"))
mlab <- c("very unhappy","unhappy","average","happy", "very happy")
marriage <- factor(affairs$ratemarr, labels=mlab)

# Frequencies for having kids:
table(haskids)

# Marriage ratings (share):
prop.table(table(marriage))

# Contigency table: counts (display & store in var.)
(countstab <- table(marriage,haskids))
prop.table(countstab)

# Share within "marriage" (i.e. within a row):
prop.table(countstab, margin=1)

# Share within "haskids"  (i.e. within a column):
prop.table(countstab, margin=2)

```

```{r}
# Scatther plot using ggplot
ggplot(ceosal1, aes(lsalary, lsales))+
  geom_point(color="red") +
  geom_smooth(method='lm')+ ggtitle('log(Salary) vs log(Sales)') +
  xlab('log(Sales)') +
  ylab('log(Salary)')
```

summarise & sumarise_all chart
```{r}
wage1 %>%
summarise (mean(educ), min(educ), max(educ), average_wage=mean(wage), women=sum(female), men=sum(female==0)) %>%
  gather(variables,values)

meap01%>%select(math4, read4, exppp)%>% summarise_all(list(mean, min, max),  na.rm = TRUE)
```

#LAB 7: Tidy data

The arguments to gather():
- data: Data object
- key: Name of new key column (made from names of data columns)
- value: Name of new value column
- ...: Names of source columns that contain values
- factor_key: Treat the new key column as a factor (instead of character vector) optional

The arguments to spread():
- data: Data object
- key: Name of column containing the new column names
- value: Name of column containing values

#LAB4: Correlations & Visualizations

```{r}
# makes a correlation table
(cormat <- round(cor(ceosal1[,c('roe','salary', 'sales', 'lsalary', 'lsales')]),2))
```

Let’s look at the correlation plots

We have several options to do correlation plots in R.

- pair(variables, data=, main="title") creates a scatter plot matrix of the variables of interest.
- plot() + abline() are the main function in the basic R package for graphs. It translate to scatterplot + line.
- geom_point() and geom_smooth() from the package ggplot are the layers to create the scatter plot and the regression line.

```{r}
# Correlation matrix graph 
pairs(~roe+lsalary+lsales,data=ceosal1, 
   main="Simple Scatterplot Matrix")
```

```{r}
# Scatther plot using ggplot
ggplot(ceosal1, aes(lsalary, lsales))+
  geom_point(color="red") +
  geom_smooth(method='lm', se=FALSE)+ ggtitle('log(Salary) vs log(Sales)') +
  xlab('log(Sales)') +
  ylab('log(Salary)') +theme(plot.title = element_text(hjust = 0.5)) # This layer just centers the title 
```

```{r}
# By gender, average wage, average education and correlation between wages and education, using dplyr package call the object wagebygender the gender variable is female
(wagebygender<-  wage1 %>% group_by(female=as.factor(female)) %>% summarise(Avgwage=mean(wage), Avgeduc=mean(educ), cor=cor(wage, educ)))
```

#LAB 5: Simple Regression

Estimation of a simple regression in R 

DGP example
Let's imagine we know the true values for $\beta_0$ and $\beta_1$ so that the population regresion equation is $$y=2+3x+u$$
and you will estimate the model $$ y = \beta_0 +\beta_1 x + u$$ 

What can you say about your estimates $\hat\beta_0$ and $\hat\beta_1$ and the true parameters $\beta_0$ and $\beta_1$? Let's look at the example:
```{r}
set.seed(123)
# generates a random from the uniform distribution between 0 and 10 
x<- rnorm(500,0,10)
# generates u a random sample of 500 from the normal distribution
u <-  rnorm(500,0,36)
# You know the actual function that relates x to y and you generate y 
y<- 2+3*x+u
# Put all vectors into a data frame because ggplot can only be fed with a dataframe
dat<-data.frame(y,x,u) 
# Run a regresion of x on y, estimate the model y=beta_0+beta_1x lets use the lm() command from R
lm(formula=y~x, dat)  
# Create a graphs where you plot the data, the population regression line and the estimated regression line 
ggplot(dat, aes(x=x,y=y))+ 
  # scatter plot defining atributes for color and transparency
  geom_point(col=4, alpha=.3)+ 
  # regression line suing geom_smooth
   geom_smooth(method="lm", se= FALSE, linetype = "dashed", aes(col="Reg. Line"))+
  # plot the straight line of the population equation
  stat_function(fun=function(x){2+(3*x)}, geom="line", aes(col="Pop. Reg Line")) +
# Use ggtitle for the title and scale_color_manual to create the legend for each line
  ggtitle("A graph of {x,y}, the population regression function, and the sample regression function")+ 
  scale_colour_manual("", 
                      breaks = c("Pop. Reg Line", "Samp. Reg. Line"),
                      values = c("green", "red"))
```

Using the `optim()` function
In R you can also use the optimization function to  estimate the population paramaters.

Remember that you want to estimate the population parameters that minimize the residual sum of squares (RSS or SSR). The equation for RSS is 
$$ SSR= \sum^{n}_{i=1}\hat{u_i}^2=\sum^{n}_{i=1}(y_i - \hat\beta_0 - \hat\beta_1 x_i)^2$$
The funtion `optim()` optimizes a function and finds the parameters. Here you create a vector of parameters called `par` $par[1]=\beta_0 , par[2]=\beta_1$. See the code below:
```{r, eval=FALSE}
# create a data frame with the variables y and x 
dat<-data.frame(x,y) 
# create a function for SSR, where par[1] is beta0 and par[2] is beta1
min.SSR <- function(data, par){sum((y - par[1] - (par[2]*x))^2)}
# Optimizing the function SSR to find the parameters that minimize it. Call the object result
# The vector par=c(b0,b1) will be replace but the correct values of b0 and b1 that minimize your function
result<-optim(par=c(b0,b1) , fn=min.SSR, data=dat)
# retrieves the vector of parameters from the results object
round(result$par,3)
```

The `lm()` command
R has a built in function that finds the OLS estimates of the population parameters . This is the `lm()` command, which stands for linear model. This command will be the workhorse of our class during the semester; it provides you all the information you need for your regression results. Today we will only see how it estimates the parameters. We will learn all about this command in future classes. 

Code to estimate the equation $$y=\beta_0+\beta_1x$$ 
```{r, eval=FALSE}

(regre<-lm(y~x, dat))
summary(regre)
```

#LAB 6: Simple Regression part 2

As mentioned last class the lm() that stands or linear model allows you to do the estimation of the population parameters. The first argument is the formula y ~ x this means that on the left-hand-side is y, the dependent variable and it is explained by x, the independent variable on the right-hand side in a linear fashion.

lm results get stored in a special version of a list object (another type of object in R), see the code below

```{r}
fit <- lm(formula = y ~ x , data=mydata) # An objet called "fit" stores the information of the estimation of the model above
summary(fit) # standard regression output
```

other useful functions
```{r}
# (or coefficients()) extracting the regression coefficients
coef(fit) 

# Confidence Intervals (CIs) for model parameters 
confint(fit, level=0.95) 

# predicted values estimated y
fitted(fit)

# (or resid()) extracting residuals
residuals(fit) 

# predictions for new data
predict(fit) 

# residual sum of squares
deviance(fit) 

# anova table 
anova(fit) 

# covariance matrix for model parameters
anova(fit) 

# regression diagnostics
influence(fit) 

# number of observations
nobs(fit) 

```

#LAB 7: Multiple Regression

```{r, eval=F, warning=FALSE}
fit <- lm(y ~ x1 + x2 + x3 , data=mydata) # An objet called "fit" stores the information of the estimation of the model above
summary(fit) # show results
```

str() is your friend to know how to call any piece of information you want to know from your estimation with the lm() command. The two lines of code below will provide you the structure that describe the elements in object that R creates after doing your estimation with the lm().

str(fit) str(summary(fit))


What if you want to estimate the model whithout the intercept, you use the code below:
lm(formula = y ~ x1 + x2 -1) or lm(formula = y ~ x1 + x2 +0)

Prove some OLS properties
For the lab using the functions above create u with the residuals and ŷ  with the fitted value. To show some properties of the OLS estimation.

Check if residuals sum and mean are equal to zero for model2
```{r}
u=resid(model2)
round(sum(u),2)
## [1] 0
round(mean(u),2) # mean of errors equal to zero
## [1] 0
```

Check if the correlation between residuals and regressors is zero.
In the console type str(model2)

```{r}
# model is the data.frame object that is created with the observations from the dependent and independent variables that will be used to estimate the model. 
# If you need to calculate the correlations between residuals and the regresors you need to use the observations used in the estimation. 
round(cor(u, model2$model$education), 3) 
## [1] 0
round(cor(u, model2$model$experience), 3)
## [1] 0
```
Calculate the predicted y, ŷ  and see how similar y¯ and ŷ ¯ are.

```{r}
yhat=fitted(model2) # use the function fitted to find yhat
yhat1=predict(model2) # another option use the function predict
# Let' compare the means of y and the predicted values
mean(log(PSID1982$wage), na.rm=TRUE) # remember to eliminate the missing values if any
mean(yhat)
mean(yhat1)
```

#LAB 8: Output Presentation

```{r}
# Use the option results=`asis` in your chunk of code so your results show in your html document
meap93set<-meap93 %>% select(salary, benefits, enroll, droprate) 
stargazer(meap93set, type = "html", digits=1)

# Choosing the statistics
stargazer(meap93set, type = "html", summary.stat = c("n", "p75", "sd"))

# Flips the table
stargazer(meap93set, type = "html", flip = TRUE)
```

```{r}
data(meap93, package='wooldridge')

# define new variable within data frame
meap93$b_s <- meap93$benefits / meap93$salary

# Estimate three different models and store them in R objects. 
model1<- lm(log(salary) ~ b_s                       , data=meap93)
model2<- lm(log(salary) ~ b_s+log(enroll)+log(staff), data=meap93)
model3<- lm(log(salary) ~ b_s+log(enroll)+log(staff)+droprate+gradrate
                                                    , data=meap93)
# Display table of results

stargazer(model1,model2,model3, type="text")  # shows results in a text output
```

```{r}
# Results in an html output, keep.stat() chooses what stats to keep

stargazer(list(model1,model2,model3),type="html",keep.stat=c("n","rsq") ) 
```

```{r}
# Using QJE publication style
stargazer(list(model1,model2,model3),type="html",keep.stat=c("n","rsq"), style="qje" ) 
```

```{r}
# AER publication style moves the intercept coefficient to the top. 

stargazer(list(model1,model2,model3),type="html",keep.stat=c("n","rsq"), style="aer", intercept.bottom = FALSE) 
```

```{r}
# Making changes, title, covariates names, changing caption and dependent variable label. Save the output in another file using out, changes the font size, removes the model number. 

stargazer(list(model1,model2,model3), type="html",             title= "These are awesome results!", align=TRUE,
          covariate.labels = c("Benefits Salary Ratio", "Log(Enroll)", "Log(Staff)", "Drop-Out Rate", "Graduation Rate"),
          column.labels = c("Basic Model", "Non-linear","Updated"),
          dep.var.caption  = "A better caption",
          dep.var.labels   = "Log(Salary)", 
          out = "reg_table.html")
```

# LAB 9: for loop stuff

```{r}
# Properties of the $\beta's$
################################
# simulate some data
################################
# population model y = b0 + b1 * x + u
rm(list=ls())
true.beta0 <-2
true.beta1 <-4
set.seed(12345)
# 100000 draws from X ~ N(5,16)
x<-rnorm(100000, mean=5, sd=4)
# 100000 draws from U ~ N(0,9)
u<-rnorm(100000, mean=0, sd=3)

# put into a data frame
data<-as.data.frame(cbind(x,u))

# generate the outcome
data$y <- true.beta0 + true.beta1*data$x + data$u
#######
#######
# Using the for loop
beta0s<-c()
beta1s<-c()
for(i in 1:1000) {
  # draw a sample
  sample <- data[sample(nrow(data),1000), ]
  x <- lm(sample$y~sample$x)
  beta1s <- c(beta1s, x$coefficients["sample$x"])
  betas0s <- c(beta0s, x$coefficients[2])
  
}

# Using lapply
list.lm <- lapply(1:1000, function(x)lm(y~x, data=data[sample(nrow(data),1000), ]))
betas <- as.data.frame(t(sapply(list.lm, coefficients)))

ggplot(betas, aes(x = betas$"(Intercept)")) +
  geom_histogram(fill=2)

ggplot(betas, aes(x=betas$x)) +
  geom_histogram(fill=4)

# using replicate
f <- function() {
  spec <- lm(y~x, data=data[sample(nrow(data), 1000), ])
  coef(spec)
}

betas1 <- as.data.frame(t(replicate(1000, f())))

ggplot(betas1, aes(x=betas1$"(Intercept)"))+
  geom_histogram(fill=2)
ggplot(betas1, aes(x=betas1$x)) +
  geom_histogram(fill=4)

```

```{r}
library(tidyverse)
library(wooldridge)
library(stargazer)
library(car)
```

## Multicolinearity and Histogram of Residuals

### VIF 
*Variance inflation factor (VIF)* is a statistic to evaluate imperfect multicolinearity for individual coeficients. It is obtained from the equation of the variance of $\hat{\beta_j}$: 

$$ Var(\hat{\beta_j})= \frac{1}{n-1}\cdot \frac{\hat{\sigma}^2}{Var(x_j)}\cdot\frac{1}{1-R^2_j}$$
 The VIF is the term in the variance that is determined by the correlation between $x_j$ and the other explanatory variables. 
 $$ VIF=\frac{1}{1-R^2_j}$$

Therefore we can write 
$$ Var(\hat{\beta_j})= \frac{1}{n-1}\cdot \frac{\hat{\sigma}^2}{Var(x_j)}\cdot VIF_j$$
 and this shows that $VIF_j$ is the factor by which $Var(\hat{\beta_j})$ is higher because $x_j$ is not uncorrelated with the other explanatory variables. 
 

Using R we can evaluate multicolinearity of multiple regression model, calculating the variance inflation factor (VIF) from the result of lm(). If VIF is more than 10, multicolinearity is strongly suggested. See the code below. 

```{r}

#model1<-lm(y~ x1+ x2 +x3, data)
#VIF(model1)

# OLS regression: the target multiple regression model 
lmres <- lm(log(wage) ~ educ+exper+tenure, data=wage1)

# Regression output:
stargazer(lmres, type='text')

# Load package "car" (has to be installed):this is done in the first chunk

# Automatically calculate VIF :
vif(lmres)

reduc <- summary(lm(educ~exper+tenure, data=wage1))$r.squared
(vifeduc=1/(1-reduc))

# Let's create a varaible that is perfectly colinear with one of the explanatory variables 
wage1$x=2*wage1$educ+3
# Let's run a regression including that, you see how R reacts!!
summary(lm(log(wage) ~ educ+exper+tenure +x , data=wage1))
stargazer(lm(log(wage) ~ educ+exper+tenure +x , data=wage1), type='text')

# Now check the line below by uncommenting it. What happens? 
# vif(lm(log(wage) ~ educ+exper+tenure +x , data=wage1))
```

## Plotting the residuals

Your residuals of the OLS regression $u~n(0, \hat{\sigma^2})$
The best estimator for $\hat{\sigma}^2=SSR/(n-k-1)$ The result of the lm command in R allow you obtain $\hat{\sigma}$ 

Let's see a histogram of the residuals to show their distribution


```{r, message=FALSE}
# k= 3 regresors 
(rmse<- sqrt(sum(lmres$residuals^2)/(length(lmres$residuals)-4)))
(rmse1<-summary(lmres)$sigma)


resid<-data.frame(lmres$residuals)
ggplot(resid, aes(lmres.residuals))+
  geom_histogram(aes(y =..density..), fill="lightgreen")+geom_density(col=4)+stat_function(fun = dnorm, args = list(mean = 0, sd = rmse1), col=2)
```

# LAB 11: Qualitative Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(wooldridge)
library(AER) 
library(dplyr)
library(stargazer)
```

Many variables of interest are qualitative rather than quantitative. Gender, race, marital status, level of education, ocupation, region, etc. Qualitative information is ussualy represented in regressions as binary or dummy variables which can only take a value zero or one. 

## Dummy variables 

Dummy variables can be used as regressors just as any other variables. 

Let's use an example in which we want to estimate a wage equation, and investigate what are the wage differences by gender. This is include a gender dummy variable in your regression formula. 

```{r}
# load wage1 data from wooldridge package
data(wage1, packakge = "wooldridge")

# show the number of men and women in the sample this is in the variable female
table(wage1$female)
```

The regression equation will be the following formula

$$wage = \beta_0 + \beta_1*female + \beta_2*educ + \beta_3*exper + \beta_4*tenure$$

```{r, message=FALSE, warning=FALSE}
# Run the regression that estimates the equation above
# First by using the variable female as a regressor
m1<-lm(wage~ female + educ + exper + tenure, data=wage1)

# You can also filter your data and create two separate equations but the most efficient way is to add the subset option inside the lm command data=subset()
m2<-lm(wage ~ educ + exper + tenure, data=subset(wage1, female==1))
# Now, what can you say about the coefficients for the dummy variable female?

# You need to interact each regressor with the female variable for the models to be the same when you restrict the sample do it below in model m3
m3<-lm(wage~educ*female + exper*female + tenure*female, data=wage1)

# Put this three models in stargazer table with the intercept at the bottom see the table 
stargazer(m1, m2, m3 , intercept.bottom = FALSE, column.sep.width = "1pt",type="text")
```


## Dummy variables and arithmetic formulas into a regression  

We can run another regression with the following formula 
$$log(wage) = \beta_0 + \beta_1*married + \beta_2*female + \beta_3*educ + \beta_4*exper + \beta_5*exper^2 + \beta_6*tenure + \beta_7*tenure^2$$

Notice how we are adding married and female dummy variables in the regression and also two squared variables into the regression 

These dummy variables are added as they are because they take 1 for category of interest and 0 for the other. 

When you want to add variables that are arithmetic operations of other variables instead of creating a separate variable you can add them just by using `I(formula)`


```{r}

# Run the new regression that estimates the new equation with tenure and experience squared 
m1<-lm(log(wage) ~ married + female + educ + exper + I(exper^1) + tenure + I(tenure^2),data=wage1)
# Run another regression with an interaction term of female and education

# First using I() for the interaction and include the variables alone

lm(log(wage)~ married  + female + educ + I(female*educ) + exper + I(exper^2) + tenure + I(tenure^2), data=wage1)

# The other option use only the interaction, there is no need to include the variables alone R does it. 
lm(log(wage)~married + female*educ + exper + I(exper^2) + tenure + I(tenure^2), data=wage1)

```

## Logical Variables

R uses logical variables to store qualitative yes or no information. In R you don't need to transform these variables into a numerical variable where **TRUE=1** and **FALSE=0**, they can be directly introduce as regressors, in the output their coefficient is then named `varnameTRUE`

```{r}
# replace "female" with logical variable using function as.logical()
wage1$female <- as.logical(wage1$female)
table(wage1$female)
  
# regression with logical variable
lm(wage ~ female+educ+exper+tenure, data=wage1)
```


## Using factor variables

A factor variable is the way R uses to store categorical variables. We transofrm any categorical variable into a factor `as.factor()`. Factor variables can be directly added to the list of regressors. R implicity adds *g-1* dummy variables if the factor has g outcomes (categories). 

When you use categorical variables that have many categories, you have to choose a reference category and this is the ommitted variable that you use to avoid colinearity. By default the first category is left out in R but the command *relevel(var, val)* chooses the outcome `*val*` as the reference variable/category for `*var*`. 

The code below shows how factor variables are used.

```{r}
data(CPS1985,package="AER")

# Table of categories and frequencies for two factor variables gender and occupation:
table(CPS1985$gender)
table(CPS1985$occupation)
# What type of variable is occupation
class(CPS1985$occupation)
```
## Regression with dummies for many categories from a categorical variable {.tabset .tabset-fade .tabset-pills}

### The long way 
To create dummies manually use the package dummies to create a set of dummy variables for each occupation 
```{r, results='hold', warning=FALSE, message=FALSE}
# Long way: create the dummy variable out of your categorical variable and include in your data set. They will be named data_separation
library(dummies)

dummy<-dummy(CPS1985$occupation, sep = "_")
# For some reason this package displays an error when knitting so the names have to be change to fix the error. 
colnames(dummy) <- c("worker", "technical", "services", "office", "sales", "management")
CPS1985<-cbind(CPS1985, dummy)

# Take a look at your global enviroment how does these new variables look now. 
#View(dummy)
# Now run a model with all the dummies for occupation. 
# See what happens with the results 
model<-lm(log(wage) ~ education + experience + gender + worker + technical + services + office + sales + management, data=CPS1985)

stargazer(model, type="text")
 ### What happens in this model???
```

### The most efficient way
```{r}
# Directly using factor variables in regression formula:
lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985)

# Manually redefine the  reference category for gender:
CPS1985$gender <- relevel(______,_______)

# the coefficients for ocupation are now redefine and managment is the reference category 
CPS1985$occupation<-relevel(________,________) 
# Rerun regression with the new based categories:
lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985)
```

See what happens if your categorical variable is not a factor but numerical. Unless you have an interpretation for it and the categories are ordered it is better to make it a factor before introducing it as a regressor 

```{r}
# check class for ocupation again 
class(CPS1985$occupation)
# make it numerical for the exercise
CPS1985$occupation<-___________(CPS1985$occupation)
# run a regression with it.
lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985)
# What do you see happening here?
```

### Numeric variables into categories

Sometimes we need to make numerical variables into categories because a linear relation with the dependent variable seems implausible or the interpretation is inconvenient. Or we simply want to have a different interpretation. 

In the example below the variable `rank` is the rank of the law school as a number between 1 and 175. We would like to compare schools in the different groups like in list below

top 10 
11-25
26-40
41-60
60-100
above 100 

In the code below we create variable for these categories. First define cut point and then create a new factor (categorical) variable based on these cut points using the cut command. 

```{r}
data(lawsch85, package='wooldridge')

# Define cut points for the rank
cutpts <- c(0,10,25,40,60,100,175)

# Create factor variable containing ranges for the rank use function cut()
lawsch85$rankcat <- cut(lawsch85$rank, cutpts)

# Display frequencies using table
table(lawsch85$rankcat)
```

Estimate the following equation $$ log(salary)= \beta_0 +\beta_1*rankcat + \beta_2*LSAT + \beta_3*GPA + \beta_4*log(libvol) + \beta_5*log(cos)$$ But frist follow the instructions to set the reference category, for the school ranking. 
```{r}
# Choose reference category, we want the last group as the reference category, so we use relevel. Save that in a new variable called rankcat
lawsch85$rankcat <- relevel(______,_______)

# Run regression

(res <- lm(__________________, data=lawsch85))
# This regression implies that graduates from the top 10 schools collect a starting salary which is around 70% higher than those of the schools below rank 100. 

```

# LAB 10: Inference

## The t-distribution, the normal distribution and the critical values

t distribution: the distribution used to do inference of our estimated parameters. 

Below is a graph of the t-distribution together with the normal distribution.
See how similiar they are when the sample is big enough.

Let's use the quartile funtion to check for the critical values for both functions. We can see that they are very similar. Therefore for big enough samples you can use the critical values from the normal distribution. 

```{r, message=FALSE, results='hold'}
# Here is the t distribution you can play changing the degrees of freedom (df) and see how similiar it is to the standard nomral. 

# Modify the code changing the df of freedom increasing the degrees of freedom
tdist <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = 2)) + # this layer plots the t distribution dt, with 2 df as the argument
  stat_function(fun = dnorm, args = list(mean = 0, 1), col=4) # this plots the normal distribution with mean=0 sd=1


normaldist <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 0, 1))

multiplot(tdist, normaldist, cols=2)

# if you use df = 1000, then the two plots merge 

```

After seeing this let's see how similar critical values are for both distributions

```{r, results='hold'}
# The value of alpha that is use for hypothesis testing
alpha <- c(0.10, 0.05, 0.025, 0.01 )
# Critical values (CV) for alpha=5% and 1% using the t distribution with 1000 degrees of freedom (d.f.) and the normal aproximation:
n<-qnorm(alpha)
n1<-round(qnorm(1-alpha), 3)
t<-round(qt(1-alpha, 1000), 3)

kable(data.frame(alpha, t , n, n1), digits=3, padding=5L, col.names = c("$\\alpha$", "T-student", "Normal-left","Normal-right"))
```

## The *t* test 

After learning the magnitude and sign of the estimated coefficients in a regression, the next step in empirical research is investigating the statistical significance of these estimates. 

### General Setup

We are often interested in testing whether there is a relation at all between the dependent variable $y$ and a regressor $x_j$ and do not want to impose a sign on the partial effect a *priori*. 
So we have the null hypotesis of the form
$$ H_0: \beta_j = a_j $$ 
where $a_j$ is some given number, very often $a_j=0$. The standard two-tailed where the alternative hypothesis is 
$$ H_1: \beta_j \neq a_j $$
and one- tailed test it is either one of 

$$H_1: \beta_j < a_j  \;\;\;\;  or \;\;\;\;   H_1: \beta_j > a_j$$ 

The hypotheses can be conveniently tested using a t test which is based on the statistic

$$ t= \frac{\hat{\beta_j} - a_j}{se(\hat{\beta_j})}$$
If $H_0$ is in fact true and the CLM assumptions hold, then this statistic has a t distribution with n-k-1 degrees of freedom.

Since the standard case of a t test,$a_j=0$, is so common R provides us with the relevant t and p values directly in the **summary** of the estimation results. 

In other words the formula is: 
$$ t= \frac{estimate - hypothesized \; value }{standard \; error}$$

Let's see an example from Wooldridge Chapter 4. Using gpa1 data 

### t-test step-by-step

```{r ttest, warning=FALSE}

data(gpa1, package='wooldridge')

# Store results under "sumres"
sumres <- lm(colGPA ~ hsGPA+ACT+skipped, data=gpa1)

# Manually confirm the formulas: Extract coefficients and SE
# extract the beta for HSGPA and standar error from the summary of the molde
regtable <- summary(sumres)$coefficients
bhat <- regtable[2,1]
se   <- regtable[2,2]

# Reproduce t statistic
tstat <- (bhat - 0) / se  # Is the same as doing (bhat / se) but it allows you to see where to add the value if different than zero 
print(paste("t-stat", tstat))
 # Reproduce p-value

#First get the absolute value of the tstat
print(paste(" Absolute value of t-stat", abs(tstat)))
# Use the pt() probability of the t-distribution with 137 degrees of freedom 
print(paste("Prob of tstat", pt(-abs(tstat),137)))
# Because this a two tails test you multiply it by two and let's round it so it looks better
print(paste("p-value", round((pval<-2*pt(-abs(tstat),137)), 5)))
```

### t-test in the regression results

the summary of the `lm()` or the stargazer table both provide with inference analysis and they both contained the same information just different. 

`summary(model)` Provides a table that has 4 columns in the coeffients element

|Independent variables |  Estimate  | Std. Error | t value | $Pr(>|t|)$|
|----------------------|-------------|------------|--------------|-------------|
|Names of your vars| You estimated ($\beta's$) | The standard errors of ($\beta's$) used for inference | $(\hat\beta - 0) / se$ | p-value with stars the imply statistical significance at certain $\alpha$|
```{r}
# Also display the regression results from the sumres object 
summary(sumres)
```
`stargazer(model)` Provides a table with one column per model 

|Independent variables |  Estimate|
|---------------------|------------|
|Names of your vars| You estimated ($\beta's$) with stars the imply statistical significance at certain $\alpha$|
|                  |(standard errors in parenthesis)| 

```{r}
# Let's see the regression results in the stargazer table why are the stars there? what do they show? 
stargazer(lm(colGPA ~ hsGPA+ACT+skipped, data=gpa1), type="text")
```

The following table provides some useful functions when doing inference in R. 

Function | Result
---------|---------
`mean(x)` | mean of a sample, $\bar{X} = \sum_{i=1}^n X_i/n$
`sd(x)` | standard deviation of a sample $s_x = \sqrt{\sum_{i=1}^n (X_i-\bar{X})^2 / (n-1)}$
`var(x)` | variance of a sample $s_x^2 = \sum_{i=1}^n (X_i-\bar{X})^2 / (n-1)$
`cov(x, y)` | Sample covariance of two variables $s_{x,y} = \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y}) / (n-1)$
`sqrt()` | Square root of a number
`length()` | Number of observations
`t.test()` | Perform a one- or two-sample *t* test

**Caveat** `t.test()` is not for regression coefficients, it is for test difference in sample means. The t-test for the regression coefficients is perfomed automatically by the lm command or if you have an specific hypothesis 

## Linear restrictions: The *F* test {.tabset} 

This test allows you to test the significance of various parameters at the same time. The test statistic of the `F` test is based on the relative difference between the sum of squared residuals in the general (unrestricted) model and a restricted model in which the hypoteses are imposed $SSR_{ur}$ and $SSR_{r}$, respectively. 
$$F={{\frac{SSR_{r}-SSR_{ur}}{SSR_{ur}}}\cdot{\frac{n-k-1}{q}}}= {{\frac{R^2_{ur}-R^2_{r}}{1-R^2_{ur}}}\cdot{\frac{n-k-1}{q}}}$$

where q is the number of restrictions, n the number of observations and k the number of parameters in the regression. 

Let's play with this regression, from Wooldridge Chapter 4.  
$$log(salary)=\beta_0 + \beta1\cdot years + \beta_2 \cdot gamesyr + \beta_3 \cdot bavg + \beta_4 \cdot hrunsyr +\beta_5 \cdot rbisyr +u$$


### F test step-by-step

```{r Ftest, results='hold'}
library(lmtest)
library(estimatr)
options(scipen=999)
# CV for alpha=1% using the F distribution with 3 and (n-k-1=353-5-1) 347 degrees of fredom d.f. :
print(paste("Critical value at 1% with 3 and 347 df=",qf(1-0.01, 3,347)))

## F test step by step 
# Use mlb1 data about salaries of baseball players 
data(mlb1, package='wooldridge')

# Unrestricted OLS regression:
res.ur <- lm(log(salary) ~ years+gamesyr+bavg+hrunsyr+rbisyr, data=mlb1)
res.ur$df
# Restricted OLS regression:
res.r <- lm(log(salary) ~ years+gamesyr, data=mlb1)

# R2:
r2.ur <- summary(res.ur)$r.squared  # R squared unrestricted
r2.r <- summary(res.r)$r.squared # R squared restricted 
print(paste("$R^2$ unrestricted=", r2.ur))
print(paste("$R^2$ restricted=", r2.r))
print(paste("Model degrees of freedom=", res.ur$df))
# F statistic:
F <- (r2.ur-r2.r) / (1-r2.ur) * res.ur$df/3 
print(paste("F-stat=", F))
# p value = 1-cdf of the appropriate F distribution:

print(paste("p-value=", round(1-pf(F, 3, res.ur$df),9)))
```

### F test using `linearHypothesis()` 

The package car provides the command `linearHypothesis()`which is adequate for this types of test. 

```{r Ftest1}
# F test
myH0 <- c("bavg","hrunsyr","rbisyr") # vector with the names of the variables that you are testing 
linearHypothesis(res.ur, myH0)
```


### F test using `anova()`

Using `anova(restricted, unrestricted)` provides the results for the F-test

```{r Ftest2}
# F test using anova function
anova(res.r, res.ur)

```

## Testing of other hypothesis

We can perform a more complicated hypothesis like that there is a relation between two variables $\beta_i =c*\beta_i$ where c is a constant.

```{r}

myH0 <- c("bavg", "hrunsyr=2*rbisyr")
linearHypothesis(_______,__________)
```

# LAB 12: Heteroskedasticity

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(estimatr) # for robust estimation
library(wooldridge) # for wooldridge data
library(stargazer) # for tables 
library(texreg) # for tables using lm_robust
library(tidyverse) # for data manipulation
library(lmtest) # for BP test for heterosckedasticity
```

## Heteroscedasticity-Robust Inference 
### lmtest package

As you learned in class you need to modify the standard errors to correct for heteroscedasticity. This is having heteroscedasticity-robust standard errors. 

In R we can do this calculations using the package *car* it provides the command *hccm* (for *h*eteroscedastics-*c*orrected *c*ovariance *m*atrices)

Let's say that the results form your regression are store in the object *reg* the code below shows you how to use the *hccm* functions 

```{r, eval=FALSE}
library (lmtest)
reg<- lm(y ~ x1 +x2 + x3, data)

hccm(reg) # For the default refined version of White's robust variance-covariance matrix
hccm(reg, type="hc0") # for the classical version of White's robust variance-covariance matrix presented by Wooldridge(2016, Section 8.2)

# For the regression coefficients, standard errors, t statistics and p-values 
coeftest(reg) # Default homoscedasticity based on standard errors
coeftest(reg, vcov=hccm) # Refined version of White's robust SE
coeftest(reg, vcoc=hccm(reg, type="hcO")) # classical version of White's robust SE. Other versions can be chosen accordingly. 

# For general F test the command linearHypothesis form package car also accepts alternative variance-covariance specifications and it is also compatible with hccm

linearHypothesis(reg, myH0) # for the default homoscedasticity-based covariance matrix
linearHypothesis(reg, myH0, vcov=hccm(reg, type="hccm")) # for the classical version of White's robust covariance matrix
linearHypothesis(reg, myH0, vcov=hccm(reg, type="hc0")) # for the refined version of White's robust covariance matrix. Other types can be chosen accordingly. 
```

### estimatr package

The estimatr package is great to show the robust results in the regression. However the only downside is that it `stargazer` doesn't support estimart results. 

You can use the `texreg` package the code needs a few modification to extract the results and being able to use the package but it also results in nice tables.  

```{r}
# Uisng the lm_robust() function run the same regression as before
# but use extract.lm_robust() using the pipe function to extract the results, all this stored into the object m1. This makes the results are available for the table. 
m1<-lm_robust(cumgpa~sat+hsperc+tothrs+female+black+white, 
                                     data=gpa3, subset=(spring==1))%>% extract.lm_robust(include.ci = FALSE)

# another method for heteroskedasticity correction
m2<-lm_robust(cumgpa~sat+hsperc+tothrs+female+black+white, 
                                     data=gpa3, subset=(spring==1), se_type="classical")%>% extract.lm_robust(include.ci = FALSE)

#summary(m1)
#summary(m2)
m1

# use the function screereg() to make a table fo the results next to each other. 
screenreg(list(m1, m2), stars = c(0.01, 0.05, .1), digits = 3)
```

# LAST LAB

```{r setup, include=FALSE}
library(gapminder)
library(knitr)
library(gtools) 
library(gganimate)
# to remove scientific notation 
  options("scipen"=100, "digits"=4)
```

## Time series data

### geom_line 
 
Graphs for time series data geom_line, geom_area, stats_smooth()
In economics you often need to do graphs of a time series in a line. 
Let's move on and use the `economics` dataset to make a time series graph of the unemployment as proportion of the population in time in The U.S. 

```{r}
# we will work with the dataset economics check what is in it
?economics 
data("economics")
head(economics)
# using geom_line make a graph that show the evolution of unemployment over the years x= date y=pop/unemployment
# name this graph time, make the line red and print the graph. 

(time<-ggplot(data=economics) + 
  geom_line(aes(x=date, y=pop/unemploy), color="red"))
```


```{r}

#In the same graph change the size of the line by  pop/unemploy
(time<-ggplot(data=economics) + 
  geom_line(aes(x=date, y=pop/unemploy, size=pop/unemploy),  color="red")) 

# Add another line of that represents the median duration of unemployment, in weeks, evolution in time add a title "Unemployment" and label the y axis "Unempl"

time+ geom_line(aes(x=date, y=uempmed), color="blue")+
  labs(title="Unemployment")+ ylab("Unempl")+theme_classic()
```

### geom_area & stat_smooth()

```{r}
# graphs 
timearea<-ggplot(data=economics) + 
  geom_area(aes(x=date, y=pop/unemploy), fill="red", col=2, alpha=0.5) 

# add another area and use stat_smooth() to show a smooth trend line.
timearea + 
  geom_area(aes(x=date, y=uempmed), fill=4, col=4, alpha=0.4)+ stat_smooth(aes(x=date, y=pop/unemploy),
  color = "#FC4E07", fill = "#FC4E07",
  method = "loess"
  )+
  labs(title="Unemployment")+ ylab("Unempl")+theme_bw()
```
## gganimate package

Part of this handout is based on [the gganimate website](https://gganimate.com/articles/gganimate.html)

## Transitions {.tabset .tabset-fade .tabset-pills}

With just a small addition you can convert a static plot into a complex animation. 
Transitions are functions that interpret the data in your graph in a way that ins distributed into a number of frames.  


### Reaveal
Letting the data gradually appear with with `transition_reveal()`

```{r}
static<-ggplot(data=economics, aes(x=date, y=pop/unemploy)) + 
  geom_line(color="red")
static +
  transition_reveal(date) #reveal by date
```


```{r}
static +
  geom_point() +
  transition_reveal(date)
  
```

### States

`transition_states()` specifically splits the data into subsets based on a variable in the data.


```{r}
college <- data.frame(read.csv("http://672258.youcanlearnit.net/college.csv") ) 
barstatic<-college %>%
  group_by(control, region) %>%
  summarize(average_tuition=round(mean(tuition), 0)) %>%
  ggplot(mapping=aes(x=region, y=average_tuition, fill=control, label=average_tuition)) +
  geom_col()+
  geom_text(size=4, fontface=2, color="yellow", position = position_stack(vjust = 0.5))+
  ylab("Average Tuition") + xlab("Region")+
  ggtitle("Average Tuition by Region and Type of School") +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
barstatic + transition_states(region, wrap = FALSE) +
  shadow_mark()
```


```{r}
barstatic + transition_states(control, wrap = TRUE)+
  shadow_mark()
```
```{r}
barstatic + transition_states(control, wrap = FALSE)
```

### Labelling, time, ease_aes()

An animated graphs like any visualization is only good if it conveys the information right. Providing the an indication as to what each frame or point in time refers to is important. The information about the each frame is created by gganimate and can be inserted in the labels option by embedding them in curly braces which are then evaluated and inserted into the argument string. See example


`transition_time()` transitions to differents states according to a time variable
`ease_aes()` defines the velocity with which aesthetics change during an animation.

```{r}
ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) +
  geom_point(alpha = 0.7, show.legend = FALSE) +
  scale_colour_manual(values = country_colors) +
  scale_size(range = c(2, 12)) +
  scale_x_log10() +
  facet_wrap(~continent) +
  # Here comes the gganimate specific bits
  labs(title = 'Year: {frame_time}', x = 'GDP per capita', y = 'life expectancy') +
  transition_time(year) +
  ease_aes('linear') # 
```
