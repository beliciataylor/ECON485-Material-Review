{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit88832310db0f46c6ba1b97f6fce48e8e",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier\n",
    "## subsection of _Text Classification_\n",
    "\n",
    "* Building and Evaluating Our Text Classifier\n",
    "    1. Bag of Words Features with Classification Models\n",
    "    2. TF-IDF Features with Classification Models\n",
    "    3. Comparative Model Performance Evaluation\n",
    "    4. Word2Vec Embeddings with Classification Models\n",
    "    5. GloVe Embeddings with Classification Models\n",
    "    6. FastText Embeddings with Classification Models\n",
    "    7. Model Tuning\n",
    "    8. Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Evaluating Our Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# download dataset\n",
    "data_df = pd.read_csv('clean_newsgroups.csv')\n",
    "\n",
    "# split data\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names =\\\n",
    "                                 train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "                                                  np.array(data_df['Target Name']), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Features with Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# build BOW features on train articles\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_train_features = cv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "cv_test_features = cv.transform(test_corpus)\n",
    "\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_train_features, train_label_names)\n",
    "mnb_bow_cv_scores = cross_val_score(mnb, cv_train_features, train_label_names, cv=5)\n",
    "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
    "mnb_bow_test_score = mnb.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_bow_cv_mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=42)\n",
    "lr.fit(cv_train_features, train_label_names)\n",
    "lr_bow_cv_scores = cross_val_score(lr, cv_train_features, train_label_names, cv=5)\n",
    "lr_bow_cv_mean_score = np.mean(lr_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_bow_cv_mean_score)\n",
    "lr_bow_test_score = lr.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(cv_train_features, train_label_names)\n",
    "svm_bow_cv_scores = cross_val_score(svm, cv_train_features, train_label_names, cv=5)\n",
    "svm_bow_cv_mean_score = np.mean(svm_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_bow_cv_mean_score)\n",
    "svm_bow_test_score = svm.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
    "svm_sgd.fit(cv_train_features, train_label_names)\n",
    "svmsgd_bow_cv_scores = cross_val_score(svm_sgd, cv_train_features, train_label_names, cv=5)\n",
    "svmsgd_bow_cv_mean_score = np.mean(svmsgd_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_bow_cv_mean_score)\n",
    "svmsgd_bow_test_score = svm_sgd.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(cv_train_features, train_label_names)\n",
    "rfc_bow_cv_scores = cross_val_score(rfc, cv_train_features, train_label_names, cv=5)\n",
    "rfc_bow_cv_mean_score = np.mean(rfc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_bow_cv_mean_score)\n",
    "rfc_bow_test_score = rfc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File \"/Users/beliciarodriguez/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-7a6ddf99ee58>\", line 4, in <module>\n    gbc.fit(cv_train_features, train_label_names)\nNameError: name 'cv_train_features' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/beliciarodriguez/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n    stb = value._render_traceback_()\nAttributeError: 'NameError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/beliciarodriguez/Library/Python/3.8/lib/python/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n  File \"/Users/beliciarodriguez/Library/Python/3.8/lib/python/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n    return f(*args, **kwargs)\n  File \"/Users/beliciarodriguez/Library/Python/3.8/lib/python/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\", line 708, in getsourcefile\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/inspect.py\", line 754, in getmodule\n    os.path.realpath(f)] = module.__name__\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/posixpath.py\", line 391, in realpath\n    path, ok = _joinrealpath(filename[:0], filename, {})\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/posixpath.py\", line 424, in _joinrealpath\n    newpath = join(path, name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/posixpath.py\", line 83, in join\n    if b.startswith(sep):\nKeyboardInterrupt\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cv_train_features' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(cv_train_features, train_label_names)\n",
    "gbc_bow_cv_scores = cross_val_score(gbc, cv_train_features, train_label_names, cv=5)\n",
    "gbc_bow_cv_mean_score = np.mean(gbc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_bow_cv_mean_score)\n",
    "gbc_bow_test_score = gbc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features with Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# build BOW features on train articles \n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "tv_test_features = tv.transform(test_corpus)\n",
    "\n",
    "print('TFIDF mode:> Train features shape:', tv_train_features.shape, \n",
    "      ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(tv_train_features, train_label_names)\n",
    "mnb_tfidf_cv_scores = cross_val_score(mnb, tv_train_features, train_label_names, cv=5)\n",
    "mnb_tfidf_cv_mean_score = np.mean(mnb_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_tfidf_cv_mean_score)\n",
    "mnb_tfidf_test_score = mnb.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=42)\n",
    "lr.fit(tv_train_features, train_label_names)\n",
    "lr_tfidf_cv_scores = cross_val_score(lr, tv_train_features, train_label_names, cv=5)\n",
    "lr_tfidf_cv_mean_score = np.mean(lr_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_tfidf_cv_mean_score)\n",
    "lr_tfidf_test_score = lr.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, train_label_names)\n",
    "svm_tfidf_cv_scores = cross_val_score(svm, tv_train_features, train_label_names, cv=5)\n",
    "svm_tfidf_cv_mean_score = np.mean(svm_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_tfidf_cv_mean_score)\n",
    "svm_tfidf_test_score = svm.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
    "svm_sgd.fit(tv_train_features, train_label_names)\n",
    "svmsgd_tfidf_cv_scores = cross_val_score(svm_sgd, tv_train_features, train_label_names, cv=5)\n",
    "svmsgd_tfidf_cv_mean_score = np.mean(svmsgd_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_tfidf_cv_mean_score)\n",
    "svmsgd_tfidf_test_score = svm_sgd.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(tv_train_features, train_label_names)\n",
    "rfc_tfidf_cv_scores = cross_val_score(rfc, tv_train_features, train_label_names, cv=5)\n",
    "rfc_tfidf_cv_mean_score = np.mean(rfc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_tfidf_cv_mean_score)\n",
    "rfc_tfidf_test_score = rfc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(tv_train_features, train_label_names)\n",
    "gbc_tfidf_cv_scores = cross_val_score(gbc, tv_train_features, train_label_names, cv=5)\n",
    "gbc_tfidf_cv_mean_score = np.mean(gbc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_tfidf_cv_mean_score)\n",
    "gbc_tfidf_test_score = gbc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([['Naive Bayes', mnb_bow_cv_mean_score, mnb_bow_test_score, \n",
    "               mnb_tfidf_cv_mean_score, mnb_tfidf_test_score],\n",
    "              ['Logistic Regression', lr_bow_cv_mean_score, lr_bow_test_score, \n",
    "               lr_tfidf_cv_mean_score, lr_tfidf_test_score],\n",
    "              ['Linear SVM', svm_bow_cv_mean_score, svm_bow_test_score, \n",
    "               svm_tfidf_cv_mean_score, svm_tfidf_test_score],\n",
    "              ['Linear SVM (SGD)', svmsgd_bow_cv_mean_score, svmsgd_bow_test_score, \n",
    "               svmsgd_tfidf_cv_mean_score, svmsgd_tfidf_test_score],\n",
    "              ['Random Forest', rfc_bow_cv_mean_score, rfc_bow_test_score, \n",
    "               rfc_tfidf_cv_mean_score, rfc_tfidf_test_score],\n",
    "              ['Gradient Boosted Machines', gbc_bow_cv_mean_score, gbc_bow_test_score, \n",
    "               gbc_tfidf_cv_mean_score, gbc_tfidf_test_score]],\n",
    "             columns=['Model', 'CV Score (TF)', 'Test Score (TF)', 'CV Score (TF-IDF)', 'Test Score (TF-IDF)'],\n",
    "             ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings with Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        \n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "            \n",
    "        return feature_vector\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, \n",
    "                                     num_features) for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize corpus\n",
    "tokenized_train = [tn.tokenizer.tokenize(text)\n",
    "                   for text in train_corpus]\n",
    "tokenized_test = [tn.tokenizer.tokenize(text)\n",
    "                  for text in test_corpus]\n",
    "\n",
    "# generate word2vec word embeddings\n",
    "import gensim\n",
    "# build word2vec word embeddings\n",
    "w2v_num_features = 1000\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features,\n",
    "                                   window=100, min_count=2, sample=1e-3, sg=1, \n",
    "                                   iter=5, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate document level embeddings\n",
    "# remember we only use train dataset vocabulary embeddings\n",
    "# so that test dataset truly remains an unseen dataset\n",
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = document_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                            num_features=w2v_num_features)\n",
    "avg_wv_test_features = document_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                           num_features=w2v_num_features)\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape,\n",
    "      ' Test features shape:', avg_wv_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "svm.fit(avg_wv_train_features, train_label_names)\n",
    "svm_w2v_cv_scores = cross_val_score(svm, avg_wv_train_features, train_label_names, cv=5)\n",
    "svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
    "svm_w2v_test_score = svm.score(avg_wv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_w2v_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings with Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering with GloVe model\n",
    "train_nlp = [tn.nlp(item) for item in train_corpus]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "test_nlp = [tn.nlp(item) for item in test_corpus]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])\n",
    "\n",
    "print('GloVe model:> Train features shape:', train_glove_features.shape,\n",
    "      ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building our SVM model\n",
    "svm = SDGClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "svm.fit(train_glove_features, train_label_names)\n",
    "svm_glove_cv_scores = np.mean(svm_glove_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_glove_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_glove_cv_mean_score)\n",
    "svm_glove_test_score = svm.score(test_glove_features, test_label_names)\n",
    "print('Test Accuracy:', svm_glove_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Embeddings with Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "ft_num_features = 1000\n",
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "ft_model = FastText(tokenized_train, size=ft_num_features, window=100,\n",
    "                    min_count=2, sample=1e-3, sg=1, iter=5, workers=10)\n",
    "\n",
    "# generate averaged word vector features from word2vec model\n",
    "avg_ft_train_features = document_vectorizer(corpus=tokenized_train, model=ft_model,\n",
    "                                            num_features=ft_num_features)\n",
    "avg_ft_test_features = document_vectorizer(corpus=tokenized_test, model=ft_model,\n",
    "                                           num_features=ft_num_features)\n",
    "\n",
    "print('FastText model:> Train features shape:', avg_ft_train_features.shape,\n",
    "      ' Test features shape:', avg_ft_test_features.shape)\n",
    "\n",
    "# build SVM model\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "svm.fit(avg_ft_train_features, train_label_names)\n",
    "svm_fit_cv_scores = cross_val_score(svm, avg_ft_train_features, train_label_names, cv=5)\n",
    "svm_ft_cv_mean_score = np.mean(svm_ft_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_ft_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_ft_cv_mean_score)\n",
    "svm_ft_test_score = svm.score(avg_ft_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_ft_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', alpha=1e-5, learning_rate='adaptive', early_stopping=True,\n",
    "                    activation='relu', hidden_layers_sizes=(512, 512), random_state=42)\n",
    "mlp.fit(avg_ft_train_features, train_label_names)\n",
    "\n",
    "svm_ft_test_scores = mlp.score(avg_ft_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_ft_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning our Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "mnb_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('mnb', MultinomialNB())\n",
    "                         ])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "              'mnb__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]}\n",
    "\n",
    "gs_mnb = GridSearchCV(mnb_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_mnb = gs_mnb.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_mnb.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performances across different hyperparameter values in the hyperparameter space\n",
    "cv_results = gs_mnb.cv_results_\n",
    "results_df = pd.DataFrame({'rank': cv_results['rank_test_score'], 'params': cv_results['params'],\n",
    "                           'cv score (mean)': cv_results['mean_test_score'],\n",
    "                           'cv score (std)': cv_results['std_test_score']})\n",
    "results_df = results_df.sort_values(by=['rank'], ascending=True)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mnb_test_score = gs_mnb.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy:', best_mnb_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning our Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('lr', LogisticRegression(penalty='l2', max_iter=100, random_state=42))])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1,1), (1,2)], 'lr__C': [1,5,10]}\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_lr = gs_lr.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate best tuned model on the test dataset\n",
    "best_lr_test_score = gs_lr.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy:', best_lr_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('svm', LinearSVC(random_state=42))])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1,1), (1,2)], 'svm__C': [0.01, 0.1, 1.5]}\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_svm = gs_svm.fit(train_corpus, train_label_names)\n",
    "\n",
    "# evaluating best tuned model on the test dataset\n",
    "best_svm_test_score = gs_svm.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy :', best_svm_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_evaluation_utils as meu\n",
    "\n",
    "mnb_predictions = gs_mnb.predict(test_corpus)\n",
    "unique_classes = list(set(test_label_names))\n",
    "meu.get_metrics(true_labels=test_label_names, predicted_labels=mnb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu.display_classification_report(true_labels=test_label_names,\n",
    "                                  predicted_labels=mnb_predictions,\n",
    "                                  classes=unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data_map = {v:k for k, v in data_labels_map.items()}\n",
    "label_map_df = pd.DataFrame(list(label_data_map.items()),\n",
    "                            columns=['Label Name', 'Label Number'])\n",
    "label_map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Confusion Matrix\n",
    "unique_class_nums = label_map_df['Label Number'].values\n",
    "mnb_prediction_class_nums = [label_data_map[item] for item in mnb_predictions]\n",
    "meu.display_confusion_matrix_pretty(true_labels=test_label_nums,\n",
    "                                    predicted_labels=mnb_prediction_class_nums,\n",
    "                                    classes=unique_class_nums)\n",
    "# can see that class labels 0, 15, 19 have lots of misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_df[label_map_df['Label Number'].isin([0, 15,19])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test document row numbers\n",
    "train_idx, test_idx = train_test_split(np.array(range(len(data_df['Article']))), \n",
    "                                      test_size=0.33, random_state=42)\n",
    "test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probas = gs_mnb.predict_proba(test_corpus).max(axis=1)\n",
    "test_df = data_df.iloc[test_idx]\n",
    "test_df['Predicted Name'] = mnb_predictions\n",
    "test_df['Predicted Confidence'] = predict_probas\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at mode misclassification instance sfor religion.misc and religion.christian\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "res_df = (test_df[(test_df['Target Name'] == 'talk.religion.misc')\n",
    "                 & (test_df['Predicted Name'] == 'soc.religion.christian')]\n",
    "          .sort_values(by=['Predicted Confidence'], ascending=False).head(5))\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "res_df = (test_df[(test_df['Target Name'] == 'talk.religion.misc')\n",
    "              & (test_df['Predicted Name'] == 'alt.atheism')]\n",
    "          .sort_values(by=['Predicted Confidence'], ascending=False).head(5))\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}