{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit88832310db0f46c6ba1b97f6fce48e8e",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: Preprocessing and Evaluation\n",
    "## subsection of _Text Classification_\n",
    "\n",
    "* Data Retrieval\n",
    "* Data Preprocessing and Normalization\n",
    "* Building Train and Test Datasets\n",
    "* Feature Engineering Techniques\n",
    "    1. Traditional\n",
    "    2. Advanced\n",
    "* Classification Models\n",
    "    1. Multinomial Naive Bayes\n",
    "    2. Logistic Regression\n",
    "    3. Support Vector Machines\n",
    "    4. Ensemble Models\n",
    "    5. Random Forest\n",
    "    6. Gradient Boosting Machines\n",
    "* Evaluating Classification Models\n",
    "    1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "data_labels_map = dict(enumerate(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataframe\n",
    "corpus, target_labels, target_names = (data.data, data.target, [data_labels_map[label] for label in data.target])\n",
    "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels, 'Target Name': target_names})\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nulls = data_df[data_df.Article.str.strip() == ''].shape[0]\n",
    "print(\"Empty documents:\", total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[~(data_df.Article.str.strip() == '')]\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# just to keep negation if any in bi-grams\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "# normalize our corpus\n",
    "norm_corpus = tn.normalize_corpus(corpus=data_df['Article'], html_stripping=True, contraction_expansion=True, \n",
    "                                  accented_char_removal=True, text_lower_case=True, text_lemmatization=True, \n",
    "                                  text_stemming=False, special_char_removal=True, remove_digits=True, \n",
    "                                  stopword_removal=True, stopwords=stopword_list)\n",
    "\n",
    "data_df['Clean Article'] = norm_corpus\n",
    "\n",
    "# view sample data\n",
    "data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Clean Article'] = norm_corpus\n",
    "data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.dropna().reset_index(drop=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('clean_newsgroups.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('clean_newsgroups.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names =\\\n",
    "                                 train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "                                                  np.array(data_df['Target Name']), test_size=0.33, random_state=42)\n",
    "\n",
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "trd = dict(Counter(train_label_names))\n",
    "tsd = dict(Counter(test_label_names))\n",
    "\n",
    "(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd], \n",
    "             columns=['Target Label', 'Train Count', 'Test Count'])\n",
    ".sort_values(by=['Train Count', 'Test Count'],\n",
    "             ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Techniques\n",
    "\n",
    "_go over the following_\n",
    "\n",
    "    1. Traditional Feature Engineering Models\n",
    "    2. Advanced Feature Engineering models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models\n",
    "\n",
    "_go over following models_\n",
    "\n",
    "    1. Multinomial Naive Bayes\n",
    "    2. Logistic Regression\n",
    "    3. Support Vector Machines\n",
    "    4. Ensemble Models\n",
    "    5. Random Forest\n",
    "    6. Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# train and build model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data and view confusion matrix\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "y_pred = logistic.predict(X_test)\n",
    "meu.display_confusion_matrix(true_labels=y_test, predicted_labels=y_pred, classes=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on model predictions\n",
    "fw_acc = round(meu.metrics.accuracy_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "mc_acc = round((TP+TN)/(TP+TN+FP+FN),5)\n",
    "print('Framework Accuracy:', fw_acc)\n",
    "print('Manually Computed Accuracy:', mc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute precision on model predictions\n",
    "fw_prec = round(meu.metrics.precision_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "mc_prec = round((TP)/(TP+FP),5)\n",
    "print('Framework Precision:', fw_prec)\n",
    "print('Manually Computed Precision:', mc_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1-score on model predictions\n",
    "fw_f1 = round(meu.metrics.f1_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "mc_f1 = round((2*mc_prec*mc_rec)/(mc_prec+mc_rec), 5)\n",
    "print('Framework F1-Score:', fw_f1)\n",
    "print('Manually Computed F1-Score:', mc_f1)"
   ]
  }
 ]
}