{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Document Summarization\n",
    "## subsection of _Text Summarization and Topic Models_\n",
    "\n",
    "* Automated Document Summarization\n",
    "    1. Text Wrangling\n",
    "    2. Text Representation with Feature Engineering\n",
    "    3. Latent Semantic Analysis\n",
    "    4. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download document.txt and other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import re\n",
    "\n",
    "DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)\n",
    "DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
    "DOCUMENT = DOCUMENT.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement document summarization using Gensim's summarization module\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "print(summarize(DOCUMENT, ratio=0.2, split=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit summarization based on word count instead of proportions\n",
    "print(summarize(DOCUMENT, word_count=75, split=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalized_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "# get sentences in the document\n",
    "sentences = nltk.sent_tokenize(DOCUMENT)\n",
    "\n",
    "# normalize each sentence in the document\n",
    "norm_sentences = normalize_corpus(sentences)\n",
    "norm_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation with Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize normalized sentences using TF-IDF feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "dt_matrix = tv.fit_transform(norm_sentences)\n",
    "dt_matrix = dt_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "td_matrix = dt_matrix.T\n",
    "print(td_matrix.shape)\n",
    "pd.DataFrame(np.round(td_matrix,2), index=vocab).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select number of sentences n that summary will contain\n",
    "# perform low-rank SVD\n",
    "num_sentences = 8\n",
    "num_topics = 3\n",
    "\n",
    "u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)\n",
    "print(u.shape, s.shape, vt.shape)\n",
    "term_topic_mat, singular_values, topic_document_mat = u, s, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remore singular values below threshold\n",
    "sv_threshold = 0.5\n",
    "min_sigma_value = max(singular_values) * sv_threshold\n",
    "singular_values[singular_values < min_sigma_value] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sentence sailency scores for each sentence (document) in game description\n",
    "salience_scores = np.sqrt(np.dot(np.square(singular_values), \n",
    "                                 np.square(topic_document_mat)))\n",
    "salience_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top sentences based on saliency score\n",
    "# display summary of game description\n",
    "top_sentence_indices = (-salience_scores).argsort()[:num_sentences]\n",
    "top_sentence_indices.sort()\n",
    "print('\\n'.join(np.array(sentences)[top_sentence_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse document-term feature matrix from LSA\n",
    "# compute document similarity matrix\n",
    "similarity_matrix = np.matmul(dt_matrix, dt_matrix.T)\n",
    "print(similarity_matrix.shape)\n",
    "np.round(similarity_matrix, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot connected graph among all sentences from document\n",
    "import networkx\n",
    "# build similarity graph\n",
    "similarity_graph = networkx.from_numpy_array(similarity_matrix)\n",
    "similarity_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the similarity graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "networkx.draw_networkx(similarity_graph, node_color='lime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pagerank scores for all the sentences\n",
    "scores = networkx.pagerank(similarity_graph)\n",
    "ranked_sentences = sorted(((score, index) for index, score in scores.items()), reverse=True)\n",
    "ranked_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top sentence indices for our summary\n",
    "top_sentence_indices = [ranked_sentences[index][1]\n",
    "                           for index in range(num_sentences)]\n",
    "top_sentence_indices.sort()\n",
    "\n",
    "# construct the document summary\n",
    "print('\\n'.join(np.array(sentences)[top_sentence_indices]))"
   ]
  }
 ]
}