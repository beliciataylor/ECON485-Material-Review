{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 4: Training a neural network model\n",
    "\n",
    "You'll learn how to update spaCy's statistical models to customize them for your use case - for example, to predict a new entity type in online comments. You'll write your own training loop from scratch, and understand the basics of how training works, along with tips and tricks that can make your custom NLP projects more successful.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "1. Training and updating models \n",
    "2. Purpose of training \n",
    "3. Creating training data (Part 1) \n",
    "4. Creating training data (Part 2) \n",
    "5. The training loop \n",
    "6. Setting up the pipeline \n",
    "7. Building a training loop \n",
    "8. Exploring the model \n",
    "9. Training best practices \n",
    "10. Good data vs. bad data \n",
    "11. Training multiple labels \n",
    "12. Wrapping up\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Training and updating models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Why updating the model?\n",
    "* Better results on your specific domain\n",
    "* Learn classification schemes specifically for your problem\n",
    "* Essential for text classification\n",
    "* Very useful for named entity recognition\n",
    "* Less critical for part-of-speech tagging and dependency parsing\n",
    "\n",
    "**Notes**\n",
    "* why would we want to update the model with our own examples? why can't we just rely on pre-trained models?\n",
    "    - stat models make predictions based on the examples they were trained on\n",
    "    - you can usually make the model more accurate by showing it examples from your domain\n",
    "    - you often also want to predict categories specific to your problem, so the model needs to learn about them\n",
    "* this is essential for text classification, very useful for entity recognition and a little less critical for tagging and parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### How training works (1)\n",
    "1. **Initialize** the model weights randomly with `nlp.begin_training` \n",
    "2. **Predict** a few examples with the current weights by calling `nlp.update` \n",
    "3. **Compare** prediction with true labels \n",
    "4. **Calculate** how to change weights to improve predictions \n",
    "5. **Update** weights slightly \n",
    "6. Go back to 2."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### How training works (2)\n",
    "\n",
    "* DIAGRAM IN SLIDE\n",
    "\n",
    "* **Training data**: examples and their annotations\n",
    "* **Text**: the input text the model should predict a label for\n",
    "* **Label**: the label the model should predict\n",
    "* **Gradient**: how to change the weights\n",
    "\n",
    "**Notes**\n",
    "* the training data are the examples we want to update the model with\n",
    "* the text should be a sentence, paragraph, or longer doc\n",
    "    - for the best results, it should be similar to what the model will see at runtime\n",
    "* the label is what we want the model to predict\n",
    "    - this can be a text category or an entity span and its type\n",
    "* the gradient is how we should change the model to reduce the current error\n",
    "    - computed when we compare the predicted label to the true label\n",
    "* after training, we can then save out an updated model and use it in our application"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example: Training the entity recognizer\n",
    "* the entity recognizer tags words and phrases in context\n",
    "* each token can only be part of one entity\n",
    "* examples need to come with context\n",
    "\n",
    "`(\"iPhone X is coming\", {\"entities\": [(0, 8, \"GADGET\")]})`\n",
    "\n",
    "* texts with no entities are also important\n",
    "\n",
    "`(\"I need a new phone! Any tips?\", {\"entities\": []})`\n",
    "\n",
    "* **Goal**: teach the model to generalize\n",
    "\n",
    "**Notes**\n",
    "* the entity recognizer takes a doc and predicts the phrases and their labels\n",
    "    - meaning that the training data needs to include texts, the entities they contain, and the entity labels\n",
    "* entities can't overlap so each token can only be part of one entity\n",
    "* because the entity recognizer predicts entities _in context_, it also needs to be trained on entities and their surrounding context\n",
    "* the easiest way to do this is to show the model a text and a list of character offsets\n",
    "    - EX: \"iPhone X\" is a gadget, starts at character 0 and ends at character 8\n",
    "* also very important for the model to learn words that _aren't_ entities\n",
    "    - EX: list of span annotations will be empty\n",
    "* GOAL: teach the model to recognize new entities in similar contexts, even if they weren't in the training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The training data\n",
    "* Examples of what we want the model to predict in context\n",
    "* Update an **existing model**: a few hundred to a few thousand examples\n",
    "* Train a **new category**: few thousand to a million examples\n",
    "    - spaCy's english models: 2 million words\n",
    "* usually created manually by human annotators\n",
    "* can be semi-automated - for example, using spaCy's `Matcher`!\n",
    "\n",
    "**Notes**\n",
    "* the training data tells the model what we want to predict\n",
    "    - this could be texts and named entities we want to recognize, or\n",
    "    - tokens and their correct part-of-speech tags\n",
    "* to update an existing model, we can start w/ a few hundred to a few thouand examples\n",
    "* to train a new category we may need to update a million\n",
    "* spaCy's pre-trained English models for instance were trained on 2 million words labelled with part-of-speech tags, dependencies and named entities\n",
    "* training data is usually created by humans who assign labels to texts\n",
    "* this is a lot of work but can be semi-automated \n",
    "    - EX: using spaCy's `Matcher`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Purpose of training\n",
    "While spaCy comes with a range of pre-trained models to predict linguistic annotations, you almost _always_ want to fine-tune them with more examples. You can do this by training them with more labelled data.\n",
    "\n",
    "What does training **note** help with?\n",
    "\n",
    "( ) Improve model accuracy on your data.\n",
    "\n",
    "( ) Learn new classification schemes.\n",
    "\n",
    "(X) Discover patterns in unlabelled data. \n",
    "\n",
    "**Correct!**: spaCy's components are supervised models for text annotations, meaning they can only learn to reproduce examples, not guess new labels from raw text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Creating training data (1)\n",
    "\n",
    "spaCy's rule-based `Matcher` is a great way to quickly create training data for named entity models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/en/iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and check the result\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([doc[start:end] for match_id, start, end in matcher(doc)])"
   ]
  },
  {
   "source": [
    "## 4. Create training data (2)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/en/iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "source": [
    "## 5. The training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The steps of a training loop\n",
    "\n",
    "1. **Loop** for a number of times \n",
    "2. **Shuffle** the training data \n",
    "3. **Divide** the data into batches \n",
    "4. **Update** the model for each batch \n",
    "5. **Save** the updated model\n",
    "\n",
    "**Notes**\n",
    "* the training loop is a series of steps that's performed to train or update a model\n",
    "* usually perform it several times, for multiple iterations, so that the model can learn from it effectively\n",
    "    - if we want to train for 10 iterations, we need to loop 10 times\n",
    "* to prevent the model from getting stuck in a suboptimal solution, we randomly shuffle the data for each iteration\n",
    "    - very common strategy when doing stochastic gradient descent\n",
    "* next, divide the training data into batches of several examples, also known as minibatching\n",
    "    - increases the reliability of the gradient estimates\n",
    "* update the model for each batch, and start the loop again until we've reached the last iteration\n",
    "* then save the model to a directory and use it in spaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {\"entities\": [(20, 28, \"GADGET\")]})\n",
    "    # And many more examples...\n",
    "]\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over time\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the mode\n",
    "        nlp.update(texts, annotations)\n",
    "    \n",
    "# Save the model\n",
    "nlp.to_disk(path_to_model)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* imagine we have a list of training examples consisting of texts and entity annotations\n",
    "* we want to loop for 10 iterations, so we're iterating over a `range` of 10\n",
    "* next, we use the `random` module to randomly shuffle the training data\n",
    "* we then use spaCy's `minibatch` utility function to divide the examples into batches\n",
    "* for each batch, we get the texts and annotations and call the `nlp.update` method to update the model\n",
    "* finally, we call the `nlp.to_disk` method to save the trained model to a directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Updating an existing model\n",
    "* improve the predictions on new data\n",
    "* especially useful to improve existing categories, like `\"PERSON\"`\n",
    "* also possible to add new categories\n",
    "* be careful and make sure the model doesn't \"forget\" the old ones\n",
    "\n",
    "**Notes**\n",
    "* spaCy lets you update an exisiting pre-trained model with more data\n",
    "    - EX: to improve its predictions on different texts\n",
    "* especially useful if you want to improve categories the model already knows like \"person\" or \"organization\"\n",
    "* you can also update a model to add new categories\n",
    "* make sure to always update it with examples of new category AND examples of the other categories it previously predicted correctly; otherwise, improving the new category might hurt the other categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Setting up a new pipeline from scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "# Start with blank English model\n",
    "nlp = spacy.blank()\n",
    "# Create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "# Add a new label\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "# Train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    # Divide examples into batches\n",
    "    for batch in spacy.util.minibatch(examples, size= 2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* the blank model doesn't have any pipeline components, only the language data and tokenization rules\n",
    "* then create a blank entity recognizer and add it to the pipeline\n",
    "* use `add_label` method to add new string labels to the model\n",
    "* can now call `nlp.begin_training` to initialize the model with random weights\n",
    "* to get better accuracy, we want to loop over the examples more than once and randomly shuffle the data on each iteration\n",
    "* on each iteration, we divide the examples into batches using spaCy's `minibatch` utility function\n",
    "    - each example consists of a text and its annotations\n",
    "* finally, we update the model with the texts and annotations and continue the loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6. Setting up the pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank \"en\" model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label \"GADGET\" to the entity recognizer\n",
    "ner.add_label(\"GADGET\")"
   ]
  },
  {
   "source": [
    "## 7. Building a training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"exercises/en/gadgets.json\", encoding=\"utf8\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    print(losses) # amount of work for the operator - the lower the \"losses\" the better"
   ]
  },
  {
   "source": [
    "## 8. Exploring the model\n",
    "\n",
    "(skipped - multiple choice question on model accuracy)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9. Training best practices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Problem 1: Models can \"forget\" things\n",
    "* Existing model can overfit on new data\n",
    "    - EX: if you only update it with `\"WEBSITE\"`, it can \"unlearn\" what a `\"PERSON\"` is\n",
    "* also known as \"catastrophic forgetting\" problem\n",
    "\n",
    "**Notes**\n",
    "* statistical models can learn lots of things - but it doesn't mean that they won't unlearn them\n",
    "* if you're updating an existing model with new data, especially new labels, it can overfit and adjust _too much_ to the new examples\n",
    "* for instance, if you're only updating it with examples of \"website\", it may \"forget\" other labels it previously predicted correctly - like \"person\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution 1: Mix in previously correct predictions\n",
    "* EX: if you're training `\"WEBSITE\"`, also include examples of `\"PERSON\"`\n",
    "* run exisiting spaCy model over data and extract all other relevant entities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD\n",
    "TRAINING_DATA = [\n",
    "    (\"Reddit is a website\", {\"entities\": [0, 6, \"WEBSITE\"]})\n",
    "]\n",
    "\n",
    "# GOOD\n",
    "TRAINING_DATA = [\n",
    "    (\"Reddit is a website\", {\"entities\": [0, 6, \"WEBSITE\"]}),\n",
    "    (\"Obama is a person\", {\"entities\": [0, 5, \"PERSON\"]})\n",
    "]"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* spaCy can help you: you can create those additional examples by running the existing model over data and extracting the entity spans you can about\n",
    "* you can then mix those examples in with your exisiting data and update the model with annotations of all labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Problem 2: Models can't learn everything\n",
    "* spaCy's models make predictions based on **local context**\n",
    "* model can struggle to learn if decision is difficult to make based on context\n",
    "* label scheme needs to be consistent and not too specific\n",
    "    - EX: `\"CLOTHING\"` is better than `\"ADULT_CLOTHING\"` and `\"CHILDRENS_CLOTHING\"`\n",
    "\n",
    "**Notes**\n",
    "* another common problem is that your model just won't learn what you want it to\n",
    "* spaCy's models make predictions based on the local context\n",
    "    - EX: for named entities, the surrounding words are most important\n",
    "* if the decision is difficult to make based on the context, the model can struggle to learn it\n",
    "* the label scheme also needs to be consistent and not too specific\n",
    "    - EX: may be very difficult to teach a model to predict whether something is adult clothing or children's clothing based on the context\n",
    "    - however, just predicting the label \"clothing\" may work better"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution 2: Plan your label scheme carefully\n",
    "* pick categories that are reflected in local context\n",
    "* more generic is better than too specific\n",
    "* use rules to go from generic labels to specific categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD\n",
    "LABELS = [\"ADULT_SHOES\", \"CHILDRENS_SHOES\", \"BANDS_I_LIKE\"]\n",
    "\n",
    "# GOOD\n",
    "LABELS = [\"CLOTHING\", \"BAND\"]"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* before you start training and updating models, it's worth taking a step back and planning your label scheme\n",
    "* try to pick categories that are reflected in the local context and make them more generic if possible\n",
    "* you can always add a rule-based system later to go from generic to specific\n",
    "* generic categories like \"clothing\" or \"band\" are both easier to label and easier to learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 10. Good data vs. bad data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"i went to amsterdem last year and the canals were beautiful\",\n",
    "        {\"entities\": [(10, 19, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
    "        {\"entities\": [(17, 22, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\"There's also a Paris in Arkansas, lol\", {\"entities\": []}),\n",
    "    (\n",
    "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
    "        {\"entities\": [(0, 6, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "source": [
    "### Part 1\n",
    "Why is this data and label scheme problematic?\n",
    "\n",
    "(X) Whether a place is a tourist destination is a subjective judgement and not a definitive category. It will be very difficult for the entity recognizer to learn.\n",
    "\n",
    "( ) Paris should also be labelled as tourist destinations for consistency. Otherwise, the model will be confused.\n",
    "\n",
    "( ) Rare out-of-vocabulary words like the misspelled \"amsterdem\" shouldn't be labelled as entities\n",
    "\n",
    "**Correct!**: A much better approach would be to only label `\"GPE\"` (geopolitical entity) or `\"LOCATION\"` and then use a rule-based system to determine whether the entity is a tourist destination in this context. For example, you could resolve the entities types back to a knowledge base or look them up in a travel wiki"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Part 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"i went to amsterdem last year and the canals were beautiful\",\n",
    "        {\"entities\": [(10, 19, \"GPE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
    "        {\"entities\": [(17, 22, \"GPE\")]},\n",
    "    ),\n",
    "    (\"There's also a Paris in Arkansas, lol\", \n",
    "     {\"entities\": [(15, 20, \"GPE\"), (24, 32, \"GPE\")]}),\n",
    "    (\n",
    "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
    "        {\"entities\": [(0, 6, \"GPE\")]},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "source": [
    "**Correct!**: Once the model achieves good results on detecting GPE entities in the traveler reviews, you could add a rule-based component to determine whether the entity is a tourist destination in this context\n",
    "\n",
    "* EX: you could resolve the entities types back to a knowledge base or look them up in a travel wiki"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 11. Training multiple labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [\n",
    "          (0, len(\"Reddit\"), \"WEBSITE\"), \n",
    "          (len(\"Reddit partners with \"), len(\"Reddit partners with Patreon\"), \"WEBSITE\")\n",
    "        ]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", {\"entities\": [\n",
    "      (len(\"PewDiePie smashes \"), len(\"PewDiePie smashes YouTube\"), \"WEBSITE\")\n",
    "    ]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [\n",
    "          (0, len(\"Reddit\"), \"WEBSITE\")]},\n",
    "    ),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "source": [
    "### Part 2\n",
    "\n",
    "(skipped - multiple choice)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [\n",
    "          (0, 6, \"WEBSITE\"), \n",
    "          (21, 28, \"WEBSITE\")\n",
    "        ]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {\"entities\": [\n",
    "       (0, len(\"PewDiePie\"), \"PERSON\"), \n",
    "       (18, 25, \"WEBSITE\")\n",
    "     ]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [\n",
    "          (0, 6, \"WEBSITE\"), \n",
    "          (len(\"Reddit founder \"), len(\"Reddit founder Alexis Ohanian\"), \"PERSON\")\n",
    "        ]},\n",
    "    ),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "source": [
    "## 12. Wrapping up\n",
    "\n",
    "Recap of what was learned in the course, skipped notes :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}