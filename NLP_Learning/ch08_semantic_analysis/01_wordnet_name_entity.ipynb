{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit88832310db0f46c6ba1b97f6fce48e8e",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet and Name Entity Recognition\n",
    "## subsection of _Semantic Analysis_\n",
    "\n",
    "* Exploring WordNet\n",
    "    1. Understanding Synsets\n",
    "    2. Analyzing Lexical Semantic Relationships\n",
    "* Word Sense Disambiguity\n",
    "* Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring WordNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "term = 'fruit'\n",
    "synsets = wn.synsets(term)\n",
    "# display total synsets\n",
    "print('Total Synsets:', len(synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "fruit_df = pd.DataFrame([{'Synset': synset, \n",
    "                          'Part of Speech': synset.lexname(), \n",
    "                          'Definition': synset.definition(), \n",
    "                          'Lemmas': synset.lemma_names(), \n",
    "                          'Examples': synset.examples()}\n",
    "                                for synset in synsets])\n",
    "fruit_df = fruit_df[['Synset', 'Part of Speech', 'Definition', 'Lemmas', 'Examples']]\n",
    "fruit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Lexical Semantic Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entailments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'wn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f4b50021c4d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'walk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'digest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0maction_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-- entails -->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentailments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wn' is not defined"
     ]
    }
   ],
   "source": [
    "for action in ['walk', 'eat', 'digest']:\n",
    "    action_syn = wn.synsets(action, pos='v')[0]\n",
    "    print(action_syn, '-- entails -->', action_syn.entailments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homonyms and Homographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('bank'):\n",
    "    print(synset.name(), '-', synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'large'\n",
    "synsets = wn.synsets(term)\n",
    "adj_large = synsets[1]\n",
    "adj_large = adj_large.lemmas()[0]\n",
    "adj_large_synonym = adj_large.synset()\n",
    "adj_large_antonym = adj_large.antonyms()[0].synset()\n",
    "\n",
    "print('Synonym:', adj_large_synonym.name())\n",
    "print('Definition:', adj_large_synonym.definition())\n",
    "print('Antonym:', adj_large_antonym.name())\n",
    "print('Definition:', adj_large_antonym.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'rich'\n",
    "synsets = wn.synsets(term)[:3]\n",
    "\n",
    "for synset in synsets:\n",
    "    rich = synset.lemmas()[0]\n",
    "    rich_synonym = rich.synset()\n",
    "    rich_antonym = rich.antonyms()[0].synset()\n",
    "\n",
    "    print('Synonym:', rich_synonym.name())\n",
    "    print('Definition:', rich_synonym.definition())\n",
    "    print('Antonym:', rich_antonym.name())\n",
    "    print('Definition:', rich_antonym.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyponyms and Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'tree'\n",
    "synsets = wn.synsets(term)\n",
    "tree = synsets[0]\n",
    "\n",
    "print('Name:', tree.name())\n",
    "print('Definition:', tree.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyponyms = tree.hyponyms()\n",
    "print('Total Hyponyms:', len(hyponyms))\n",
    "print('Sample Hyponyms')\n",
    "for hyponym in hyponyms[:10]:\n",
    "    print(hyponym.name(), '-', hyponym.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernyms = tree.hypernyms()\n",
    "print(hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total hierarchy pathways for 'tree'\n",
    "hypernym_paths = tree.hypernym_paths()\n",
    "print('Total Hypernym paths:', len(hypernym_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the entire hypernym hierarchy\n",
    "print('Hypernym Hierarchy')\n",
    "print(' -> '.join(synset.name() for synset in hypernym_paths[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_holonyms = tree.member_holonyms()\n",
    "print('Total Member Holonyms:', len(member_holonyms))\n",
    "print('Member Holonyms for [tree]:-')\n",
    "for holonym in member_holonyms:\n",
    "    print(holonym.name(), '-', holonym.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_meronyms = tree.part_meronyms()\n",
    "print('Total Part Meronyms:', len(part_meronyms))\n",
    "print('Part Meronyms for [tree]:-')\n",
    "for meronym in part_meronyms:\n",
    "    print(meronym.name(), '-', meronym.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substance based meronyms for tree\n",
    "substance_meronyms = tree.substance_meronyms()\n",
    "print('Total Substance Meronyms:', len(substance_meronyms))\n",
    "print('Substance Meronyms for [tree]:-')\n",
    "for meronym in substance_meronyms:\n",
    "    print(meronym.name(), '-', meronym.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Relationships and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog = wn.synset('dog.n.01')\n",
    "\n",
    "# create entities and extract names and definitions\n",
    "entities = [tree, lion, tiger, cat, dog]\n",
    "entity_names = [entity.name().split('.')[0] for entity in entities]\n",
    "entity_definitions = [entity.definition() for entity in entities]\n",
    "\n",
    "# print entiries and their definitions\n",
    "for entity, definition in zip(entity_names, entity_definitions):\n",
    "    print(entity, '-', definition)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_hypernyms = []\n",
    "for entity in entities:\n",
    "    # get pairwise lowest common hypernyms\n",
    "    common_hypernyms.append([entity.lowest_common_hypernyms(compared_entity)[0]\n",
    "                                            .name().split('.')[0]\n",
    "                             for compared_entity in entities])\n",
    "\n",
    "# build pairwise lower common hypernym matrix\n",
    "common_hypernym_frame = pd.DataFrame(common_hypernyms,\n",
    "                                     index=entity_names, \n",
    "                                     columns=entity_names)\n",
    "common_hypernym_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for entity in entities:\n",
    "    # get pairwise similarities\n",
    "    similarities.append([round(entity.path_similarity(compared_entity), 2) for compared_entity in entities])\n",
    "\n",
    "# build pairwise similarity matrix\n",
    "similarity_frame = pd.DataFrame(similarities, index=entity_names, columns=entity_names)\n",
    "similarity_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# sample text and word to disambiguate\n",
    "samples = [('The fruits on that plant have ripened', 'n'), \n",
    "            ('He finally reaped the fruit of his hard work as he won the race', 'n')]\n",
    "\n",
    "# perform word sense disambiguity\n",
    "word = 'fruit'\n",
    "for sentence, pos_tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sentence.lower()), word, pos_tag)\n",
    "    print('Sentence:', sentence)\n",
    "    print('Word synset:', word_syn)\n",
    "    print('Corresponding definition:', word_syn.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text and word to disambiguate\n",
    "samples = [('Lead is a very soft, malleable metal', 'n'), \n",
    "            ('John is the actor who plays the lead in that movie', 'n'), \n",
    "            ('This road leads to nowhere', 'v')]\n",
    "word = 'lead'\n",
    "\n",
    "# perform word sense disambiguation\n",
    "for sentence, pos_tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sentence.lower()), word, pos_tag)\n",
    "    print('Sentence:', sentence)\n",
    "    print('Word synset:', word_syn)\n",
    "    print('Corresponding definition:', word_syn.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Three more countries have joined an “international grand committee” of parliaments, adding to calls for \n",
    "Facebook’s boss, Mark Zuckerberg, to give evidence on misinformation to the coalition. Brazil, Latvia and Singapore \n",
    "bring the total to eight different parliaments across the world, with plans to send representatives to London on 27 \n",
    "November with the intention of hearing from Zuckerberg. Since the Cambridge Analytica scandal broke, the Facebook chief \n",
    "has only appeared in front of two legislatures: the American Senate and House of Representatives, and the European parliament. \n",
    "Facebook has consistently rebuffed attempts from others, including the UK and Canadian parliaments, to hear from Zuckerberg. \n",
    "He added that an article in the New York Times on Thursday, in which the paper alleged a pattern of behaviour from Facebook \n",
    "to “delay, deny and deflect” negative news stories, “raises further questions about how recent data breaches were allegedly \n",
    "dealt with within Facebook.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub(r'\\n', '', text) # remove extra newlines\n",
    "\n",
    "# not working\n",
    "#import spacy\n",
    "#nlp = spacy.load('web_en_core_sm')\n",
    "\n",
    "# github help\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "text_nlp = nlp(text)\n",
    "# print named entites in article\n",
    "ner_tagged = [(word.text, word.ent_type_) for word in text_nlp]\n",
    "print(ner_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# visualize named entities\n",
    "displacy.render(text_nlp, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract named entities\n",
    "named_entities = []\n",
    "temp_entity_name = ''\n",
    "temp_named_entity = None\n",
    "for term, tag in ner_tagged:\n",
    "    if tag:\n",
    "        temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "        temp_named_entity = (temp_entity_name, tag)\n",
    "    else:\n",
    "        if temp_named_entity:\n",
    "            named_entities.append(temp_named_entity)\n",
    "            temp_entity_name = ''\n",
    "            temp_named_entity = None\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the top entity types\n",
    "from collections import Counter\n",
    "c = Counter([item[1] for item in named_entities])\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "STANFORD_CLASSIFIER_PATH = r'/Users/beliciarodriguez/Downloads/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "STANFORD_NER_JAR_PATH = r'/Users/beliciarodriguez/Downloads/stanford-ner-2014-08-27/stanford-ner-3.4.1.jar'\n",
    "\n",
    "sn = StanfordNERTagger(STANFORD_CLASSIFIER_PATH, path_to_jar=STANFORD_NER_JAR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform NER tagging & extract relevant entities\n",
    "text_enc = text.encode('ascii', errors='ignore').decode('utf-8')\n",
    "ner_tagged = sn.tag(text_enc.split())\n",
    "\n",
    "named_entities = []\n",
    "temp_entity_name = ''\n",
    "temp_named_entity = None\n",
    "for term, tag in ner_tagged:\n",
    "    if tag != 'O':\n",
    "        temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "        temp_named_entity = (temp_entity_name, tag)\n",
    "    else:\n",
    "        if temp_named_entity:\n",
    "            named_entities.append(temp_named_entity)\n",
    "            temp_entity_name = ''\n",
    "            temp_named_entity = None\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more frequent entities\n",
    "c = Counter([item[1] for item in named_entities])\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Stanford's Core NLP (connected to server)\n",
    "from nltk.parse import CoreNLPParser\n",
    "import nltk\n",
    "\n",
    "# NER Tagging\n",
    "ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "tags = list(ner_tagger.raw_tag_sents(nltk.sent_tokenize(text)))\n",
    "tags = [sublist[0] for sublist in tags]\n",
    "tags = [word_tag for sublist in tags for word_tag in sublist]\n",
    "\n",
    "# Extract Named Entities\n",
    "named_entities = []\n",
    "temp_entity_name = ''\n",
    "temp_named_entity = None\n",
    "for term, tag in tags:\n",
    "    if tag != 'O':\n",
    "        temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "        temp_named_entity = (temp_entity_name, tag)\n",
    "    else:\n",
    "        if temp_named_entity:\n",
    "            named_entities.append(temp_named_entity)\n",
    "            temp_entity_name = ''\n",
    "            temp_named_entity = None\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out top named entity types\n",
    "c = Counter([item[1] for item in named_entities])\n",
    "c.most_common()"
   ]
  }
 ]
}