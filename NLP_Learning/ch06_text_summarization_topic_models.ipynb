{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization and Topic Models\n",
    "* Text Summarization and Information Extraction\n",
    "* Important Concepts\n",
    "* Keyphrase Extractions\n",
    "    1. Collocations\n",
    "    2. Weighted Tag-Based Phrase Extraction\n",
    "* Topic Modeling on Research Papers\n",
    "    1. The Main Objective\n",
    "    2. Data Retrieval\n",
    "    3. Load and View Dataset\n",
    "    4. Basic Text Wrangling\n",
    "* Topic Models with Gensim\n",
    "    1. Text Representation with Feature Engineering\n",
    "    2. Latent Semantic Indexing\n",
    "    3. Implementing LSI Topic Models from Scratch\n",
    "    4. Latent Dirichlet Allocation\n",
    "    5. LDA Models with MALLET\n",
    "    6. LDA Tuning: Finding the Optimal Number of Topics\n",
    "    7. Interpreting Topic Model Results\n",
    "    8. Predicting Topics for New Research Papers\n",
    "* Topic Models with Scikit-Learn\n",
    "    1. Text Representation with Feature Engineering\n",
    "    2. Latent Semantic Indexing\n",
    "    3. Latent Dirichlet Allocation\n",
    "    4. Non-Negative Matrix Factorization\n",
    "    5. Predicting Topics for New Research Papers\n",
    "    6. Visualizing Topic Models\n",
    "* Automated Document Summarization\n",
    "    1. Text Wrangling\n",
    "    2. Text Representation with Feature Engineering\n",
    "    3. Latent Semantic Analysis\n",
    "    4. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if spacy doesn't run\n",
    "#!pip3 install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nltk error\n",
    "#import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top k singular values and return corresponding U, S, & V matrices\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    u,s,vt = svds(matrix, k=singular_count)\n",
    "    return u,s,vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ] \n",
      " alice adventures wonderland lewis carroll\n"
     ]
    }
   ],
   "source": [
    "## Collocations\n",
    "from nltk.corpus import gutenberg\n",
    "import text_normalizer as tn\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "\n",
    "# load corpus\n",
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "norm_alice = list(filter(None,\n",
    "                         tn.normalize_corpus(alice, text_lemmatization=False)))\n",
    "\n",
    "# print and compare first line\n",
    "print(alice[0], '\\n', norm_alice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (2, 3, 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_ngrams(sequence, n):\n",
    "    return list(\n",
    "            zip(*(sequence[index:]\n",
    "                  for index in range(n))))\n",
    "\n",
    "# test function\n",
    "compute_ngrams([1,2,3,4], 2) # bi-grams\n",
    "compute_ngrams([1,2,3,4], 3) # tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to flatten corpus into one big string of text\n",
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip()\n",
    "                    for document in corpus])\n",
    "\n",
    "# get top n-grams for corpus of text\n",
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    \n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),\n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq)\n",
    "                     for text, freq in sorted_ngrams]\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said alice', 123),\n",
       " ('mock turtle', 56),\n",
       " ('march hare', 31),\n",
       " ('said king', 29),\n",
       " ('thought alice', 26),\n",
       " ('white rabbit', 22),\n",
       " ('said hatter', 22),\n",
       " ('said mock', 20),\n",
       " ('said caterpillar', 18),\n",
       " ('said gryphon', 18)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 bigrams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=2, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said mock turtle', 20),\n",
       " ('said march hare', 10),\n",
       " ('poor little thing', 6),\n",
       " ('little golden key', 5),\n",
       " ('certainly said alice', 5),\n",
       " ('white kid gloves', 5),\n",
       " ('march hare said', 5),\n",
       " ('mock turtle said', 5),\n",
       " ('know said alice', 4),\n",
       " ('might well say', 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 trigrams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=3, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7effc9cc0080>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use NLTK's collocation finders\n",
    "# bigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split() for item in norm_alice])\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'alice'),\n",
       " ('mock', 'turtle'),\n",
       " ('march', 'hare'),\n",
       " ('said', 'king'),\n",
       " ('thought', 'alice'),\n",
       " ('said', 'hatter'),\n",
       " ('white', 'rabbit'),\n",
       " ('said', 'mock'),\n",
       " ('said', 'caterpillar'),\n",
       " ('said', 'gryphon')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# raw frequencies\n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abide', 'figures'),\n",
       " ('acceptance', 'elegant'),\n",
       " ('accounting', 'tastes'),\n",
       " ('accustomed', 'usurpation'),\n",
       " ('act', 'crawling'),\n",
       " ('adjourn', 'immediate'),\n",
       " ('adoption', 'energetic'),\n",
       " ('affair', 'trusts'),\n",
       " ('agony', 'terror'),\n",
       " ('alarmed', 'proposal')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pointwise mutual information\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'mock', 'turtle'),\n",
       " ('said', 'march', 'hare'),\n",
       " ('poor', 'little', 'thing'),\n",
       " ('little', 'golden', 'key'),\n",
       " ('march', 'hare', 'said'),\n",
       " ('mock', 'turtle', 'said'),\n",
       " ('white', 'kid', 'gloves'),\n",
       " ('beau', 'ootiful', 'soo'),\n",
       " ('certainly', 'said', 'alice'),\n",
       " ('might', 'well', 'say')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw frequencies\n",
    "finder.nbest(trigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accustomed', 'usurpation', 'conquest'),\n",
       " ('adjourn', 'immediate', 'adoption'),\n",
       " ('adoption', 'energetic', 'remedies'),\n",
       " ('ancient', 'modern', 'seaography'),\n",
       " ('apple', 'roast', 'turkey'),\n",
       " ('arithmetic', 'ambition', 'distraction'),\n",
       " ('brother', 'latin', 'grammar'),\n",
       " ('canvas', 'bag', 'tied'),\n",
       " ('cherry', 'tart', 'custard'),\n",
       " ('circle', 'exact', 'shape')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pointwise mutual information\n",
    "finder.nbest(trigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Weighted Tag-Based Phrase Extraction\n",
    "data = open('elephants.txt', 'r+').readlines()\n",
    "sentences = nltk.sent_tokenize(data[0])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elephants are large mammals of the family Elephantidae and the order Proboscidea.',\n",
       " 'Three species are currently recognised: the African bush elephant (Loxodonta africana), the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus).',\n",
       " 'Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the first three lines\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elephants are large mammals of the family Elephantidae and the order Proboscidea',\n",
       " 'Three species are currently recognised the African bush elephant Loxodonta africana the African forest elephant L cyclotis and the Asian elephant Elephas maximus',\n",
       " 'Elephants are scattered throughout subSaharan Africa South Asia and Southeast Asia']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sentences = tn.normalize_corpus(sentences, text_lower_case=False, text_stemming=False,\n",
    "                                     text_lemmatization=False, stopword_removal=False)\n",
    "norm_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_chunks(sentences, grammar=r'NP: {<DT>? <JJ>* <NN.*>+}', stopword_list=stopwords):\n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))]\n",
    "        chunks = [chunker.parse(tagged_sent)\n",
    "                     for tagged_sent in tagged_sents]\n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                        for chunk in chunks]\n",
    "        flattened_chunks = list(itertools.chain.from_iterable(wtc_sent for wtc_sent in wtc_sents))\n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk])\n",
    "                                    for status, chunk in itertools.groupby(flattened_chunks,\n",
    "                                                      lambda word_pos_chunk: \n",
    "                                                      word_pos_chunk[2] != 'O')]\n",
    "        valid_chunks = [' '.join(word.lower()\n",
    "                                 for word, tag, chunk in wtc_group\n",
    "                                     if word.lower() not in stopword_list)\n",
    "                                        for status, wtc_group in valid_chunks_tagged if status]\n",
    "        all_chunks.append(valid_chunks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'],\n",
       " ['species',\n",
       "  'african bush elephant loxodonta',\n",
       "  'african forest elephant l cyclotis',\n",
       "  'asian elephant elephas maximus'],\n",
       " ['elephants', 'subsaharan africa south asia', 'southeast asia'],\n",
       " ['elephantidae',\n",
       "  'family',\n",
       "  'order proboscidea',\n",
       "  'extinct members',\n",
       "  'order',\n",
       "  'deinotheres gomphotheres mammoths',\n",
       "  'mastodons'],\n",
       " ['elephants',\n",
       "  'several distinctive features',\n",
       "  'long trunk',\n",
       "  'proboscis',\n",
       "  'many purposes',\n",
       "  'water',\n",
       "  'grasping objects'],\n",
       " ['incisors', 'tusks', 'weapons', 'tools', 'objects'],\n",
       " ['elephants', 'flaps', 'body temperature'],\n",
       " ['pillarlike legs', 'great weight'],\n",
       " ['african elephants',\n",
       "  'ears',\n",
       "  'backs',\n",
       "  'asian elephants',\n",
       "  'ears',\n",
       "  'convex',\n",
       "  'level backs'],\n",
       " ['elephants', 'different habitats', 'savannahs forests deserts', 'marshes'],\n",
       " ['water'],\n",
       " ['keystone species', 'impact', 'environments'],\n",
       " ['animals',\n",
       "  'distance',\n",
       "  'elephants',\n",
       "  'predators',\n",
       "  'lions tigers hyenas',\n",
       "  'wild dogs',\n",
       "  'young elephants',\n",
       "  'calves'],\n",
       " ['elephants', 'fissionfusion society', 'multiple family groups'],\n",
       " ['females cows',\n",
       "  'family groups',\n",
       "  'female',\n",
       "  'calves',\n",
       "  'several related females'],\n",
       " ['groups', 'individual known', 'matriarch', 'cow'],\n",
       " ['males bulls', 'family groups', 'males'],\n",
       " ['adult',\n",
       "  'family groups',\n",
       "  'mate',\n",
       "  'enter state',\n",
       "  'increased testosterone',\n",
       "  'aggression',\n",
       "  'musth',\n",
       "  'dominance',\n",
       "  'reproductive success'],\n",
       " ['calves', 'centre', 'attention', 'family groups', 'mothers', 'years'],\n",
       " ['elephants', 'years', 'wild'],\n",
       " ['touch sight smell',\n",
       "  'sound elephants',\n",
       "  'infrasound',\n",
       "  'seismic communication',\n",
       "  'long distances'],\n",
       " ['elephant intelligence', 'primates', 'cetaceans'],\n",
       " ['selfawareness', 'dead individuals', 'kind'],\n",
       " ['african elephants',\n",
       "  'international union',\n",
       "  'conservation',\n",
       "  'nature iucn',\n",
       "  'asian elephant'],\n",
       " ['threats', 'populations', 'ivory trade', 'animals', 'ivory tusks'],\n",
       " ['threats', 'elephants', 'habitat destruction', 'conflicts', 'local people'],\n",
       " ['elephants', 'animals', 'asia'],\n",
       " ['past', 'war today', 'display', 'zoos', 'entertainment', 'circuses'],\n",
       " ['elephants', 'art folklore religion literature', 'popular culture']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = get_chunks(norm_sentences)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "def get_tfidf_weighted_keyphrases(sentences, grammar=r'NP: {<DT>? <JJ>* <NN.*>+}', top_n=10):\n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "    \n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    weighted_phrases = {dictionary.get(idx): value for doc in corpus_tfidf for idx, value in doc}\n",
    "    weighted_phrases = sorted(weighted_phrases.items(),\n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    weighted_phrases = [(term, round(wt,3)) for term, wt in weighted_phrases]\n",
    "    \n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 1.0),\n",
       " ('asia', 0.807),\n",
       " ('wild', 0.764),\n",
       " ('great weight', 0.707),\n",
       " ('pillarlike legs', 0.707),\n",
       " ('southeast asia', 0.693),\n",
       " ('subsaharan africa south asia', 0.693),\n",
       " ('body temperature', 0.693),\n",
       " ('flaps', 0.693),\n",
       " ('fissionfusion society', 0.693),\n",
       " ('multiple family groups', 0.693),\n",
       " ('art folklore religion literature', 0.693),\n",
       " ('popular culture', 0.693),\n",
       " ('ears', 0.681),\n",
       " ('males', 0.653),\n",
       " ('males bulls', 0.653),\n",
       " ('family elephantidae', 0.607),\n",
       " ('large mammals', 0.607),\n",
       " ('years', 0.607),\n",
       " ('environments', 0.577),\n",
       " ('impact', 0.577),\n",
       " ('keystone species', 0.577),\n",
       " ('cetaceans', 0.577),\n",
       " ('elephant intelligence', 0.577),\n",
       " ('primates', 0.577),\n",
       " ('dead individuals', 0.577),\n",
       " ('kind', 0.577),\n",
       " ('selfawareness', 0.577),\n",
       " ('different habitats', 0.57),\n",
       " ('marshes', 0.57)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 30 tf-idf weighted keyphrases\n",
    "get_tfidf_weighted_keyphrases(sentences=norm_sentences, top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('african bush elephant', 0.261),\n",
       " ('including', 0.141),\n",
       " ('family', 0.137),\n",
       " ('cow', 0.124),\n",
       " ('forests', 0.108),\n",
       " ('female', 0.103),\n",
       " ('asia', 0.102),\n",
       " ('objects', 0.098),\n",
       " ('sight', 0.098),\n",
       " ('ivory', 0.098),\n",
       " ('tigers', 0.098),\n",
       " ('males', 0.088),\n",
       " ('folklore', 0.087),\n",
       " ('religion', 0.087),\n",
       " ('known', 0.087),\n",
       " ('larger ears', 0.085),\n",
       " ('water', 0.075),\n",
       " ('highly recognisable', 0.075),\n",
       " ('breathing lifting', 0.074),\n",
       " ('flaps', 0.073),\n",
       " ('africa', 0.072),\n",
       " ('gomphotheres', 0.072),\n",
       " ('animals tend', 0.071),\n",
       " ('success', 0.071),\n",
       " ('south', 0.07)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "key_words = keywords(data[0], ratio=1.0, scores=True, lemmatize=True)\n",
    "[(item, round(score,3)) for item, score in key_words][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling on Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-10 17:42:09--  https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Resolving cs.nyu.edu (cs.nyu.edu)... 128.122.49.30\n",
      "Connecting to cs.nyu.edu (cs.nyu.edu)|128.122.49.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12851423 (12M) [application/x-gzip]\n",
      "Saving to: ‘nips12raw_str602.tgz’\n",
      "\n",
      "nips12raw_str602.tg 100%[===================>]  12.26M  23.3MB/s    in 0.5s    \n",
      "\n",
      "2020-05-10 17:42:09 (23.3 MB/s) - ‘nips12raw_str602.tgz’ saved [12851423/12851423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Data Retrieval\n",
    "!wget https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset\n",
    "!tar -xzf nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nips11', 'nips10', 'nips03', 'README_yann', 'MATLAB_NOTES', 'nips08', 'nips00', 'nips06', 'nips05', 'nips12', 'nips02', 'nips04', 'idx', 'orig', 'nips07', 'nips09', 'nips01', 'RAW_DATA_NOTES']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load and View Dataset\n",
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "# read all texts into a list\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814 \n",
      "NEU1OMO1PHIC NETWORKS BASED \n",
      "ON SPARSE OPTICAL ORTHOGONAL CODES \n",
      "Mario P. Vecchi and Jawad A. Salehl \n",
      "Bell Communications Research \n",
      "435 South Street \n",
      "Morristown, NJ 07960-1961 \n",
      "Abstract \n",
      "A family of neuromorphic networks specifically designed for communications \n",
      "and optical signal processing applications is presented. The information is encoded \n",
      "utilizing sparse Optical Orthogonal Code sequences on the basis of unipolar, binary \n",
      "(0, 1) signals. The generalized synaptic connectivity matrix is also unipolar, and \n",
      "clipped to binary (0, 1) values. In addition to high-capacity associative memory, \n",
      "the resulting neural networks can be used to implement general functions, such as \n",
      "code filtering, code mapping, code joining, code shifting and code projecting. \n",
      "1 Introduction \n",
      "Synthetic neural nets [1,2] represent an active and growing research field. Fundamental \n",
      "issues, as well as practical implementations with electronic and optical devices are being \n",
      "studied. In addition, several lea\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "CPU times: user 28.7 s, sys: 179 ms, total: 28.9 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Basic Text Wrangling\n",
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "        \n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neu1', 'omo1', 'phic', 'network', 'based', 'sparse', 'optical', 'orthogonal', 'code', 'mario', 'vecchi', 'jawad', 'salehl', 'bell', 'communication', 'research', 'south', 'street', 'morristown', 'nj', 'abstract', 'family', 'neuromorphic', 'network', 'specifically', 'designed', 'communication', 'optical', 'signal', 'processing', 'application', 'presented', 'information', 'encoded', 'utilizing', 'sparse', 'optical', 'orthogonal', 'code', 'sequence', 'basis', 'unipolar', 'binary', 'signal', 'generalized', 'synaptic', 'connectivity', 'matrix', 'also', 'unipolar']\n"
     ]
    }
   ],
   "source": [
    "# viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neu1', 'omo1', 'phic', 'network', 'based', 'sparse', 'optical', 'orthogonal', 'code', 'mario', 'vecchi', 'jawad', 'salehl', 'bell', 'communication', 'research', 'south', 'street', 'morristown', 'nj_abstract', 'family', 'neuromorphic', 'network', 'specifically', 'designed', 'communication', 'optical', 'signal_processing', 'application', 'presented', 'information', 'encoded', 'utilizing', 'sparse', 'optical', 'orthogonal', 'code', 'sequence', 'basis', 'unipolar', 'binary', 'signal', 'generalized', 'synaptic', 'connectivity', 'matrix', 'also', 'unipolar', 'clipped', 'binary']\n"
     ]
    }
   ],
   "source": [
    "## Text Representation with Feature Engineering\n",
    "import gensim\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, \n",
    "                               threshold=20, delimiter=b'_') # higher threshold fewer phrases\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# sample demonstration\n",
    "print(bigram_model[norm_papers[0]][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings: [(0, '2f'), (1, '2our'), (2, '3the'), (3, '82o'), (4, '86ch'), (5, '_b'), (6, '_t'), (7, '_u'), (8, '_v'), (9, '_ym'), (10, '_z'), (11, 'ability'), (12, 'absence'), (13, 'ac'), (14, 'acad_sci')]\n",
      "Total Vocabulary Size 78892\n"
     ]
    }
   ],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "# create a dictionary representation of the documents\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 7756\n"
     ]
    }
   ],
   "source": [
    "# filer out words that occur less than 20 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (8, 1), (13, 2), (20, 1), (24, 1), (34, 2), (46, 2), (52, 1), (56, 1), (57, 2), (58, 2), (66, 1), (76, 3), (77, 1), (83, 1), (86, 1), (87, 1), (100, 1), (101, 1), (103, 1), (105, 2), (108, 1), (113, 1), (117, 1), (118, 2), (119, 1), (126, 1), (130, 1), (132, 1), (138, 2), (148, 2), (155, 1), (169, 2), (172, 1), (173, 3), (177, 2), (185, 2), (186, 2), (189, 2), (190, 1), (200, 1), (210, 3), (211, 1), (217, 1), (218, 3), (222, 4), (227, 5), (231, 2), (240, 2), (245, 1)]\n"
     ]
    }
   ],
   "source": [
    "# transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('absence', 1), ('accurately', 1), ('addition', 2), ('american_institute', 1), ('application', 1), ('available', 2), ('body', 2), ('channel', 1), ('class', 1), ('clear', 2), ('clearly', 2), ('combination', 1), ('computational', 3), ('computer', 1), ('condition', 1), ('connected', 1), ('connectivity', 1), ('corresponding', 1), ('corresponds', 1), ('cross', 1), ('cycle', 2), ('defined', 1), ('design', 1), ('determined', 1), ('developed', 2), ('development', 1), ('directly', 1), ('discussion', 1), ('distinct', 1), ('easily', 2), ('end', 2), ('established', 1), ('far', 2), ('feedback', 1), ('fiber', 3), ('filter', 2), ('functional', 2), ('fundamental', 2), ('generally', 2), ('generate', 1), ('hard', 1), ('iii', 3), ('ij', 1), ('importance', 1), ('important', 3), ('indicated', 4), ('intensity', 5), ('interesting', 2), ('le', 2), ('line', 1)]\n"
     ]
    }
   ],
   "source": [
    "# viewing actual terms and their counts\n",
    "print([(dictionary[idx], freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 1740\n"
     ]
    }
   ],
   "source": [
    "# total papers in the corpus\n",
    "print('Total number of papers:', len(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 37s, sys: 39.9 ms, total: 2min 37s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Latent Semantic Indexing\n",
    "TOTAL_TOPICS = 10\n",
    "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS, \n",
    "                                 onepass=True, chunksize=1740, power_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.215*\"unit\" + 0.212*\"state\" + 0.187*\"training\" + 0.177*\"neuron\" + 0.162*\"pattern\" + 0.145*\"image\" + 0.140*\"vector\" + 0.125*\"feature\" + 0.122*\"cell\" + 0.110*\"layer\" + 0.101*\"task\" + 0.097*\"class\" + 0.091*\"probability\" + 0.089*\"signal\" + 0.087*\"step\" + 0.086*\"response\" + 0.085*\"representation\" + 0.083*\"noise\" + 0.082*\"rule\" + 0.081*\"distribution\"\n",
      "\n",
      "Topic #2:\n",
      "0.487*\"neuron\" + 0.396*\"cell\" + -0.257*\"state\" + 0.191*\"response\" + -0.187*\"training\" + 0.170*\"stimulus\" + 0.117*\"activity\" + -0.109*\"class\" + 0.099*\"spike\" + 0.097*\"pattern\" + 0.096*\"circuit\" + 0.096*\"synaptic\" + -0.095*\"vector\" + 0.090*\"signal\" + 0.090*\"firing\" + 0.088*\"visual\" + -0.084*\"classifier\" + -0.083*\"action\" + -0.078*\"word\" + 0.078*\"cortical\"\n",
      "\n",
      "Topic #3:\n",
      "-0.627*\"state\" + 0.395*\"image\" + -0.219*\"neuron\" + 0.209*\"feature\" + -0.188*\"action\" + 0.137*\"unit\" + 0.131*\"object\" + -0.130*\"control\" + 0.129*\"training\" + -0.109*\"policy\" + 0.103*\"classifier\" + 0.090*\"class\" + -0.081*\"step\" + -0.081*\"dynamic\" + 0.080*\"classification\" + 0.078*\"layer\" + 0.076*\"recognition\" + -0.074*\"reinforcement_learning\" + 0.069*\"representation\" + 0.068*\"pattern\"\n",
      "\n",
      "Topic #4:\n",
      "-0.686*\"unit\" + 0.433*\"image\" + -0.182*\"pattern\" + -0.131*\"layer\" + -0.123*\"hidden_unit\" + -0.121*\"net\" + -0.114*\"training\" + 0.112*\"feature\" + -0.109*\"activation\" + -0.107*\"rule\" + 0.097*\"neuron\" + -0.078*\"word\" + 0.070*\"pixel\" + -0.070*\"connection\" + 0.067*\"object\" + 0.065*\"state\" + 0.060*\"distribution\" + 0.059*\"face\" + -0.057*\"architecture\" + 0.055*\"estimate\"\n",
      "\n",
      "Topic #5:\n",
      "-0.428*\"image\" + -0.348*\"state\" + 0.266*\"neuron\" + -0.264*\"unit\" + 0.181*\"training\" + 0.174*\"class\" + -0.168*\"object\" + 0.167*\"classifier\" + -0.147*\"action\" + -0.122*\"visual\" + 0.117*\"vector\" + 0.115*\"node\" + 0.105*\"distribution\" + -0.103*\"motion\" + -0.099*\"feature\" + 0.097*\"classification\" + -0.097*\"control\" + -0.095*\"task\" + -0.087*\"cell\" + -0.083*\"representation\"\n",
      "\n",
      "Topic #6:\n",
      "-0.660*\"cell\" + 0.508*\"neuron\" + 0.213*\"image\" + 0.103*\"chip\" + 0.097*\"unit\" + -0.093*\"response\" + 0.090*\"object\" + -0.083*\"rat\" + -0.076*\"distribution\" + 0.070*\"circuit\" + -0.069*\"probability\" + -0.064*\"stimulus\" + 0.061*\"memory\" + 0.058*\"analog\" + 0.058*\"activation\" + -0.055*\"class\" + 0.053*\"bit\" + 0.052*\"net\" + -0.051*\"cortical\" + -0.050*\"firing\"\n",
      "\n",
      "Topic #7:\n",
      "-0.353*\"word\" + 0.281*\"unit\" + -0.272*\"training\" + -0.257*\"classifier\" + -0.177*\"recognition\" + 0.159*\"distribution\" + -0.152*\"feature\" + -0.144*\"state\" + -0.142*\"pattern\" + 0.141*\"vector\" + -0.128*\"cell\" + -0.128*\"task\" + 0.122*\"approximation\" + 0.121*\"variable\" + 0.110*\"equation\" + -0.107*\"classification\" + 0.106*\"noise\" + -0.103*\"class\" + 0.101*\"matrix\" + -0.098*\"neuron\"\n",
      "\n",
      "Topic #8:\n",
      "-0.303*\"pattern\" + 0.243*\"signal\" + 0.236*\"control\" + 0.202*\"training\" + -0.181*\"rule\" + -0.178*\"state\" + 0.167*\"noise\" + -0.166*\"class\" + 0.162*\"word\" + -0.155*\"cell\" + -0.154*\"feature\" + 0.147*\"motion\" + 0.140*\"task\" + -0.127*\"node\" + -0.124*\"neuron\" + 0.116*\"target\" + 0.114*\"circuit\" + -0.114*\"probability\" + -0.110*\"classifier\" + -0.109*\"image\"\n",
      "\n",
      "Topic #9:\n",
      "-0.472*\"node\" + -0.254*\"circuit\" + 0.214*\"word\" + -0.201*\"chip\" + 0.190*\"neuron\" + 0.172*\"stimulus\" + -0.160*\"classifier\" + -0.152*\"current\" + 0.147*\"feature\" + -0.146*\"voltage\" + 0.145*\"distribution\" + -0.141*\"control\" + -0.124*\"rule\" + -0.110*\"layer\" + -0.105*\"analog\" + -0.091*\"tree\" + 0.084*\"response\" + 0.080*\"state\" + 0.079*\"probability\" + 0.079*\"estimate\"\n",
      "\n",
      "Topic #10:\n",
      "0.518*\"word\" + -0.254*\"training\" + 0.236*\"vector\" + -0.222*\"task\" + -0.194*\"pattern\" + -0.156*\"classifier\" + 0.149*\"node\" + 0.146*\"recognition\" + -0.139*\"control\" + 0.138*\"sequence\" + -0.126*\"rule\" + 0.125*\"circuit\" + 0.123*\"cell\" + -0.113*\"action\" + -0.105*\"neuron\" + 0.094*\"hmm\" + 0.093*\"character\" + 0.088*\"chip\" + 0.088*\"matrix\" + 0.085*\"structure\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lsi_bow.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.215), ('state', 0.212), ('training', 0.187), ('neuron', 0.177), ('pattern', 0.162), ('image', 0.145), ('vector', 0.14), ('feature', 0.125), ('cell', 0.122), ('layer', 0.11), ('task', 0.101), ('class', 0.097), ('probability', 0.091), ('signal', 0.089), ('step', 0.087), ('response', 0.086), ('representation', 0.085), ('noise', 0.083), ('rule', 0.082), ('distribution', 0.081)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.487), ('cell', 0.396), ('response', 0.191), ('stimulus', 0.17), ('activity', 0.117), ('spike', 0.099), ('pattern', 0.097), ('circuit', 0.096), ('synaptic', 0.096), ('signal', 0.09), ('firing', 0.09), ('visual', 0.088), ('cortical', 0.078)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.257), ('training', -0.187), ('class', -0.109), ('vector', -0.095), ('classifier', -0.084), ('action', -0.083), ('word', -0.078)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.395), ('feature', 0.209), ('unit', 0.137), ('object', 0.131), ('training', 0.129), ('classifier', 0.103), ('class', 0.09), ('classification', 0.08), ('layer', 0.078), ('recognition', 0.076), ('representation', 0.069), ('pattern', 0.068)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.627), ('neuron', -0.219), ('action', -0.188), ('control', -0.13), ('policy', -0.109), ('step', -0.081), ('dynamic', -0.081), ('reinforcement_learning', -0.074)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.433), ('feature', 0.112), ('neuron', 0.097), ('pixel', 0.07), ('object', 0.067), ('state', 0.065), ('distribution', 0.06), ('face', 0.059), ('estimate', 0.055)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -0.686), ('pattern', -0.182), ('layer', -0.131), ('hidden_unit', -0.123), ('net', -0.121), ('training', -0.114), ('activation', -0.109), ('rule', -0.107), ('word', -0.078), ('connection', -0.07), ('architecture', -0.057)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.266), ('training', 0.181), ('class', 0.174), ('classifier', 0.167), ('vector', 0.117), ('node', 0.115), ('distribution', 0.105), ('classification', 0.097)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.428), ('state', -0.348), ('unit', -0.264), ('object', -0.168), ('action', -0.147), ('visual', -0.122), ('motion', -0.103), ('feature', -0.099), ('control', -0.097), ('task', -0.095), ('cell', -0.087), ('representation', -0.083)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.508), ('image', 0.213), ('chip', 0.103), ('unit', 0.097), ('object', 0.09), ('circuit', 0.07), ('memory', 0.061), ('analog', 0.058), ('activation', 0.058), ('bit', 0.053), ('net', 0.052)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cell', -0.66), ('response', -0.093), ('rat', -0.083), ('distribution', -0.076), ('probability', -0.069), ('stimulus', -0.064), ('class', -0.055), ('cortical', -0.051), ('firing', -0.05)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.281), ('distribution', 0.159), ('vector', 0.141), ('approximation', 0.122), ('variable', 0.121), ('equation', 0.11), ('noise', 0.106), ('matrix', 0.101)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -0.353), ('training', -0.272), ('classifier', -0.257), ('recognition', -0.177), ('feature', -0.152), ('state', -0.144), ('pattern', -0.142), ('cell', -0.128), ('task', -0.128), ('classification', -0.107), ('class', -0.103), ('neuron', -0.098)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('signal', 0.243), ('control', 0.236), ('training', 0.202), ('noise', 0.167), ('word', 0.162), ('motion', 0.147), ('task', 0.14), ('target', 0.116), ('circuit', 0.114)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('pattern', -0.303), ('rule', -0.181), ('state', -0.178), ('class', -0.166), ('cell', -0.155), ('feature', -0.154), ('node', -0.127), ('neuron', -0.124), ('probability', -0.114), ('classifier', -0.11), ('image', -0.109)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.214), ('neuron', 0.19), ('stimulus', 0.172), ('feature', 0.147), ('distribution', 0.145), ('response', 0.084), ('state', 0.08), ('probability', 0.079), ('estimate', 0.079)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('node', -0.472), ('circuit', -0.254), ('chip', -0.201), ('classifier', -0.16), ('current', -0.152), ('voltage', -0.146), ('control', -0.141), ('rule', -0.124), ('layer', -0.11), ('analog', -0.105), ('tree', -0.091)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.518), ('vector', 0.236), ('node', 0.149), ('recognition', 0.146), ('sequence', 0.138), ('circuit', 0.125), ('cell', 0.123), ('hmm', 0.094), ('character', 0.093), ('chip', 0.088), ('matrix', 0.088), ('structure', 0.085)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -0.254), ('task', -0.222), ('pattern', -0.194), ('classifier', -0.156), ('control', -0.139), ('rule', -0.126), ('action', -0.113), ('neuron', -0.105)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt,3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt,3)))\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7756, 10), (10,), (10, 1740))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get U, S, VT matrices from topic model\n",
    "term_topic = lsi_bow.projection.u\n",
    "singular_values = lsi_bow.projection.s\n",
    "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T1     T2     T3     T4     T5     T6     T7     T8     T9    T10\n",
       "0  0.025 -0.011 -0.001  0.011  0.026  0.015  0.030 -0.025 -0.016  0.073\n",
       "1  0.021  0.028 -0.004  0.003 -0.003 -0.000 -0.008  0.009  0.008 -0.014\n",
       "2  0.027  0.007  0.006  0.017  0.017  0.035 -0.008 -0.001 -0.079  0.075\n",
       "3  0.016  0.017 -0.013  0.008  0.024  0.028  0.000 -0.019  0.008 -0.006\n",
       "4  0.018  0.030 -0.003  0.001  0.006 -0.031 -0.005 -0.001  0.001  0.004"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document topic matrix for our LSI model\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
    "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "document_topics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #13:\n",
      "Dominant Topics (top 3): ['T8', 'T1', 'T4']\n",
      "Paper Summary:\n",
      "412 \n",
      "CAPACITY FOR PATTERNS AND SEQUENCES IN KANERVA'S SDM \n",
      "AS COMPARED TO OTHER ASSOCIATIVE MEMORY MODELS \n",
      "James D. Keeler \n",
      "Chemistry Department, Stanford University, Stanford, CA 94305 \n",
      "and RIACS, NASA-AMES 230-5 Moffett Field, CA 94035. \n",
      "e.rnail: jdk hydra.riacs. edu \n",
      "ABSTRACT \n",
      "The information capacity of Kanerva's Sparse, Distributed Memory (SDM) and Hopfield-type \n",
      "neural networks is investigated. Under the approximations used here, it is shown that the to- \n",
      "tal information stored in these s\n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T8', 'T1', 'T2']\n",
      "Paper Summary:\n",
      "68 Baird \n",
      "Associative Memory in a Simple Model of \n",
      "Oscillating Cortex \n",
      "Bill Baird \n",
      "Dept Molecular and Cell Biology, \n",
      "U.C.Berkeley, Berkeley, Ca. 94720 \n",
      "ABSTRACT \n",
      "A generic model of oscillating cortex, which assumes \"minimal\" \n",
      "coupling justified by known anatomy, is shown to function as an \n",
      "sociative memory, using previously developed theory. The network \n",
      "has explicit excitatory neurons with local inhibitory interneuron \n",
      "feedback that forms a set of nonlinear oscillators coupled only by \n",
      "long ran\n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T10', 'T4', 'T9']\n",
      "Paper Summary:\n",
      "Interpretation of Artificial Neural Networks: \n",
      "Mapping Knowledge-Based Neural Networks into Rules \n",
      "Geoffrey Towell Jude W. Shavlik \n",
      "Computer Sciences Department \n",
      "University of Wisconsin \n",
      "Madison, WI 53706 \n",
      "Abstract \n",
      "We propose and empirically evaluate a method for the extraction of expert- \n",
      "comprehensible rules from trained neural networks. Our method operates in \n",
      "the context of a three-step process for learning that uses rule-based domain \n",
      "knowledge in combination with neural networks. Empirica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_numbers = [13, 250, 500]\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.\n",
    "                      columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7756, 1740)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Implementing LSI Topic Models from Scratch\n",
    "td_matrix = gensim.matutils.corpus2dense(corpus=bow_corpus, num_terms=len(dictionary))\n",
    "print(td_matrix.shape)\n",
    "td_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 7756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['3the', 'ability', 'absence', ..., 'smola', 'mozer_jordan',\n",
       "       'kearns_solla'], dtype='<U28')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = np.array(list(dictionary.values()))\n",
    "print('Total vocabulary size:', len(vocabulary))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7756, 10), (10,), (10, 1740))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "u, s, vt = svds(td_matrix, k=TOTAL_TOPICS, maxiter=10000)\n",
    "term_topic = u\n",
    "singular_values = s\n",
    "topic_document = vt\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7756)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_weights = term_topic.transpose() * singular_values[:,None]\n",
    "tt_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('training', 92.619), ('task', 80.732), ('pattern', 70.619), ('classifier', 56.987), ('control', 50.675), ('rule', 45.926), ('action', 41.202), ('neuron', 38.195)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -188.486), ('vector', -85.973), ('node', -54.38), ('recognition', -53.231), ('sequence', -50.35), ('circuit', -45.396), ('cell', -44.811), ('hmm', -34.085), ('character', -34.022), ('chip', -32.162), ('matrix', -32.093), ('structure', -30.993)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('node', 173.276), ('circuit', 92.999), ('chip', 73.593), ('classifier', 58.718), ('current', 55.844), ('voltage', 53.489), ('control', 51.709), ('rule', 45.295), ('layer', 40.265), ('analog', 38.343), ('tree', 33.483)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -78.351), ('neuron', -69.792), ('stimulus', -63.233), ('feature', -53.819), ('distribution', -53.119), ('response', -30.954), ('state', -29.343), ('probability', -29.1), ('estimate', -28.908)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('pattern', 116.972), ('rule', 69.783), ('state', 68.605), ('class', 64.259), ('cell', 59.979), ('feature', 59.607), ('node', 49.175), ('neuron', 47.998), ('probability', 43.813), ('classifier', 42.612), ('image', 42.061)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('signal', -93.805), ('control', -91.041), ('training', -77.88), ('noise', -64.397), ('word', -62.392), ('motion', -56.699), ('task', -53.882), ('target', -44.765), ('circuit', -44.129)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('word', 147.792), ('training', 113.693), ('classifier', 107.386), ('recognition', 73.948), ('feature', 63.454), ('state', 60.126), ('pattern', 59.561), ('cell', 53.767), ('task', 53.693), ('classification', 44.936), ('class', 43.161), ('neuron', 41.093)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -117.727), ('distribution', -66.72), ('vector', -58.881), ('approximation', -50.931), ('variable', -50.83), ('equation', -46.229), ('noise', -44.247), ('matrix', -42.214)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('cell', 285.803), ('response', 40.216), ('rat', 35.975), ('distribution', 33.085), ('probability', 29.79), ('stimulus', 27.789), ('class', 24.02), ('cortical', 22.185), ('firing', 21.66)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -220.116), ('image', -92.391), ('chip', -44.422), ('unit', -41.922), ('object', -39.001), ('circuit', -30.444), ('memory', -26.475), ('analog', -25.207), ('activation', -24.953), ('bit', -22.997), ('net', -22.699)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('image', 209.794), ('state', 170.207), ('unit', 129.108), ('object', 82.185), ('action', 72.136), ('visual', 59.502), ('motion', 50.605), ('feature', 48.665), ('control', 47.427), ('task', 46.496), ('cell', 42.366), ('representation', 40.564)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -130.054), ('training', -88.668), ('class', -85.213), ('classifier', -81.92), ('vector', -57.532), ('node', -56.341), ('distribution', -51.622), ('classification', -47.645)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('image', 215.857), ('feature', 55.647), ('neuron', 48.495), ('pixel', 35.095), ('object', 33.585), ('state', 32.544), ('distribution', 29.977), ('face', 29.256), ('estimate', 27.555)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -341.829), ('pattern', -90.771), ('layer', -65.337), ('hidden_unit', -61.12), ('net', -60.035), ('training', -56.742), ('activation', -54.268), ('rule', -53.377), ('word', -38.903), ('connection', -34.618), ('architecture', -28.439)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('image', 229.287), ('feature', 121.397), ('unit', 79.44), ('object', 76.204), ('training', 75.153), ('classifier', 59.872), ('class', 52.527), ('classification', 46.696), ('layer', 45.149), ('recognition', 44.192), ('representation', 40.179), ('pattern', 39.252)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -364.388), ('neuron', -127.022), ('action', -109.245), ('control', -75.369), ('policy', -63.103), ('step', -47.226), ('dynamic', -46.907), ('reinforcement_learning', -42.747)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('state', 161.465), ('training', 117.319), ('class', 68.732), ('vector', 59.558), ('classifier', 52.589), ('action', 52.113), ('word', 49.239)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -306.151), ('cell', -249.243), ('response', -119.758), ('stimulus', -106.762), ('activity', -73.499), ('spike', -62.039), ('pattern', -60.957), ('circuit', -60.602), ('synaptic', -60.282), ('signal', -56.665), ('firing', -56.597), ('visual', -55.571), ('cortical', -48.867)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -260.793), ('state', -258.146), ('training', -227.312), ('neuron', -215.681), ('pattern', -197.232), ('image', -175.735), ('vector', -170.154), ('feature', -151.547), ('cell', -148.138), ('layer', -133.593), ('task', -122.389), ('class', -117.849), ('probability', -110.526), ('signal', -108.232), ('step', -105.202), ('response', -104.465), ('representation', -103.255), ('noise', -100.573), ('rule', -99.611), ('distribution', -98.973)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_terms = 20\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(tt_weights), axis=1)[:, :top_terms]\n",
    "topic_keyterm_weights = np.array([tt_weights[row, columns]\n",
    "                                 for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    terms, weights = topic_keyterms_weights[n]\n",
    "    term_weights = sorted([(t,w) for t, w in zip(terms, weights)], key=lambda row: -abs(row[1]))\n",
    "    for term, wt in term_weights:\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #13:\n",
      "Dominant Topics (top 3): ['T3', 'T10', 'T7']\n",
      "Paper Summary:\n",
      "412 \n",
      "CAPACITY FOR PATTERNS AND SEQUENCES IN KANERVA'S SDM \n",
      "AS COMPARED TO OTHER ASSOCIATIVE MEMORY MODELS \n",
      "James D. Keeler \n",
      "Chemistry Department, Stanford University, Stanford, CA 94305 \n",
      "and RIACS, NASA-AMES 230-5 Moffett Field, CA 94035. \n",
      "e.rnail: jdk hydra.riacs. edu \n",
      "ABSTRACT \n",
      "The information capacity of Kanerva's Sparse, Distributed Memory (SDM) and Hopfield-type \n",
      "neural networks is investigated. Under the approximations used here, it is shown that the to- \n",
      "tal information stored in these s\n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T3', 'T10', 'T9']\n",
      "Paper Summary:\n",
      "68 Baird \n",
      "Associative Memory in a Simple Model of \n",
      "Oscillating Cortex \n",
      "Bill Baird \n",
      "Dept Molecular and Cell Biology, \n",
      "U.C.Berkeley, Berkeley, Ca. 94720 \n",
      "ABSTRACT \n",
      "A generic model of oscillating cortex, which assumes \"minimal\" \n",
      "coupling justified by known anatomy, is shown to function as an \n",
      "sociative memory, using previously developed theory. The network \n",
      "has explicit excitatory neurons with local inhibitory interneuron \n",
      "feedback that forms a set of nonlinear oscillators coupled only by \n",
      "long ran\n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T1', 'T7', 'T2']\n",
      "Paper Summary:\n",
      "Interpretation of Artificial Neural Networks: \n",
      "Mapping Knowledge-Based Neural Networks into Rules \n",
      "Geoffrey Towell Jude W. Shavlik \n",
      "Computer Sciences Department \n",
      "University of Wisconsin \n",
      "Madison, WI 53706 \n",
      "Abstract \n",
      "We propose and empirically evaluate a method for the extraction of expert- \n",
      "comprehensible rules from trained neural networks. Our method operates in \n",
      "the context of a three-step process for learning that uses rule-based domain \n",
      "knowledge in combination with neural networks. Empirica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
    "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "document_numbers = [13, 250, 500]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.\n",
    "                      columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
