{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization and Topic Models\n",
    "* Text Summarization and Information Extraction\n",
    "* Important Concepts\n",
    "* Keyphrase Extractions\n",
    "    1. Collocations\n",
    "    2. Weighted Tag-Based Phrase Extraction\n",
    "* Topic Modeling on Research Papers\n",
    "    1. The Main Objective\n",
    "    2. Data Retrieval\n",
    "    3. Load and View Dataset\n",
    "    4. Basic Text Wrangling\n",
    "* Topic Models with Gensim\n",
    "    1. Text Representation with Feature Engineering\n",
    "    2. Latent Semantic Indexing\n",
    "    3. Implementing LSI Topic Models from Scratch\n",
    "    4. Latent Dirichlet Allocation\n",
    "    5. LDA Models with MALLET\n",
    "    6. LDA Tuning: Finding the Optimal Number of Topics\n",
    "    7. Interpreting Topic Model Results\n",
    "    8. Predicting Topics for New Research Papers\n",
    "* Topic Models with Scikit-Learn\n",
    "    1. Text Representation with Feature Engineering\n",
    "    2. Latent Semantic Indexing\n",
    "    3. Latent Dirichlet Allocation\n",
    "    4. Non-Negative Matrix Factorization\n",
    "    5. Predicting Topics for New Research Papers\n",
    "    6. Visualizing Topic Models\n",
    "* Automated Document Summarization\n",
    "    1. Text Wrangling\n",
    "    2. Text Representation with Feature Engineering\n",
    "    3. Latent Semantic Analysis\n",
    "    4. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if spacy doesn't run\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nltk error\n",
    "#import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/bellepracticevm/code/Users/LearningCode/NLP_Learning'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find current working directory\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# for azure ml\n",
    "path_to_users = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/bellepracticevm/code/Users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top k singular values and return corresponding U, S, & V matrices\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    u,s,vt = svds(matrix, k=singular_count)\n",
    "    return u,s,vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ] \n",
      " alice adventures wonderland lewis carroll\n"
     ]
    }
   ],
   "source": [
    "## Collocations\n",
    "from nltk.corpus import gutenberg\n",
    "import text_normalizer as tn\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "\n",
    "# load corpus\n",
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "norm_alice = list(filter(None,\n",
    "                         tn.normalize_corpus(alice, text_lemmatization=False)))\n",
    "\n",
    "# print and compare first line\n",
    "print(alice[0], '\\n', norm_alice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (2, 3, 4)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_ngrams(sequence, n):\n",
    "    return list(\n",
    "            zip(*(sequence[index:]\n",
    "                  for index in range(n))))\n",
    "\n",
    "# test function\n",
    "compute_ngrams([1,2,3,4], 2) # bi-grams\n",
    "compute_ngrams([1,2,3,4], 3) # tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to flatten corpus into one big string of text\n",
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip()\n",
    "                    for document in corpus])\n",
    "\n",
    "# get top n-grams for corpus of text\n",
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    \n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),\n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq)\n",
    "                     for text, freq in sorted_ngrams]\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said alice', 123),\n",
       " ('mock turtle', 56),\n",
       " ('march hare', 31),\n",
       " ('said king', 29),\n",
       " ('thought alice', 26),\n",
       " ('white rabbit', 22),\n",
       " ('said hatter', 22),\n",
       " ('said mock', 20),\n",
       " ('said caterpillar', 18),\n",
       " ('said gryphon', 18)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 bigrams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=2, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said mock turtle', 20),\n",
       " ('said march hare', 10),\n",
       " ('poor little thing', 6),\n",
       " ('little golden key', 5),\n",
       " ('certainly said alice', 5),\n",
       " ('white kid gloves', 5),\n",
       " ('march hare said', 5),\n",
       " ('mock turtle said', 5),\n",
       " ('know said alice', 4),\n",
       " ('might well say', 4)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 trigrams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=3, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7f351d3db6d8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use NLTK's collocation finders\n",
    "# bigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split() for item in norm_alice])\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'alice'),\n",
       " ('mock', 'turtle'),\n",
       " ('march', 'hare'),\n",
       " ('said', 'king'),\n",
       " ('thought', 'alice'),\n",
       " ('said', 'hatter'),\n",
       " ('white', 'rabbit'),\n",
       " ('said', 'mock'),\n",
       " ('said', 'caterpillar'),\n",
       " ('said', 'gryphon')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# raw frequencies\n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abide', 'figures'),\n",
       " ('acceptance', 'elegant'),\n",
       " ('accounting', 'tastes'),\n",
       " ('accustomed', 'usurpation'),\n",
       " ('act', 'crawling'),\n",
       " ('adjourn', 'immediate'),\n",
       " ('adoption', 'energetic'),\n",
       " ('affair', 'trusts'),\n",
       " ('agony', 'terror'),\n",
       " ('alarmed', 'proposal')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pointwise mutual information\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'mock', 'turtle'),\n",
       " ('said', 'march', 'hare'),\n",
       " ('poor', 'little', 'thing'),\n",
       " ('little', 'golden', 'key'),\n",
       " ('march', 'hare', 'said'),\n",
       " ('mock', 'turtle', 'said'),\n",
       " ('white', 'kid', 'gloves'),\n",
       " ('beau', 'ootiful', 'soo'),\n",
       " ('certainly', 'said', 'alice'),\n",
       " ('might', 'well', 'say')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw frequencies\n",
    "finder.nbest(trigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accustomed', 'usurpation', 'conquest'),\n",
       " ('adjourn', 'immediate', 'adoption'),\n",
       " ('adoption', 'energetic', 'remedies'),\n",
       " ('ancient', 'modern', 'seaography'),\n",
       " ('apple', 'roast', 'turkey'),\n",
       " ('arithmetic', 'ambition', 'distraction'),\n",
       " ('brother', 'latin', 'grammar'),\n",
       " ('canvas', 'bag', 'tied'),\n",
       " ('cherry', 'tart', 'custard'),\n",
       " ('circle', 'exact', 'shape')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pointwise mutual information\n",
    "finder.nbest(trigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Weighted Tag-Based Phrase Extraction\n",
    "data = open('data/elephants.txt', 'r+').readlines()\n",
    "sentences = nltk.sent_tokenize(data[0])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elephants are large mammals of the family Elephantidae and the order Proboscidea.',\n",
       " 'Three species are currently recognised: the African bush elephant (Loxodonta africana), the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus).',\n",
       " 'Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the first three lines\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elephants are large mammals of the family Elephantidae and the order Proboscidea',\n",
       " 'Three species are currently recognised the African bush elephant Loxodonta africana the African forest elephant L cyclotis and the Asian elephant Elephas maximus',\n",
       " 'Elephants are scattered throughout subSaharan Africa South Asia and Southeast Asia']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sentences = tn.normalize_corpus(sentences, text_lower_case=False, text_stemming=False,\n",
    "                                     text_lemmatization=False, stopword_removal=False)\n",
    "norm_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_chunks(sentences, grammar=r'NP: {<DT>? <JJ>* <NN.*>+}', stopword_list=stopwords):\n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))]\n",
    "        chunks = [chunker.parse(tagged_sent)\n",
    "                     for tagged_sent in tagged_sents]\n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                        for chunk in chunks]\n",
    "        flattened_chunks = list(itertools.chain.from_iterable(wtc_sent for wtc_sent in wtc_sents))\n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk])\n",
    "                                    for status, chunk in itertools.groupby(flattened_chunks,\n",
    "                                                      lambda word_pos_chunk: \n",
    "                                                      word_pos_chunk[2] != 'O')]\n",
    "        valid_chunks = [' '.join(word.lower()\n",
    "                                 for word, tag, chunk in wtc_group\n",
    "                                     if word.lower() not in stopword_list)\n",
    "                                        for status, wtc_group in valid_chunks_tagged if status]\n",
    "        all_chunks.append(valid_chunks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'],\n",
       " ['species',\n",
       "  'african bush elephant loxodonta',\n",
       "  'african forest elephant l cyclotis',\n",
       "  'asian elephant elephas maximus'],\n",
       " ['elephants', 'subsaharan africa south asia', 'southeast asia'],\n",
       " ['elephantidae',\n",
       "  'family',\n",
       "  'order proboscidea',\n",
       "  'extinct members',\n",
       "  'order',\n",
       "  'deinotheres gomphotheres mammoths',\n",
       "  'mastodons'],\n",
       " ['elephants',\n",
       "  'several distinctive features',\n",
       "  'long trunk',\n",
       "  'proboscis',\n",
       "  'many purposes',\n",
       "  'water',\n",
       "  'grasping objects'],\n",
       " ['incisors', 'tusks', 'weapons', 'tools', 'objects'],\n",
       " ['elephants', 'flaps', 'body temperature'],\n",
       " ['pillarlike legs', 'great weight'],\n",
       " ['african elephants',\n",
       "  'ears',\n",
       "  'backs',\n",
       "  'asian elephants',\n",
       "  'ears',\n",
       "  'convex',\n",
       "  'level backs'],\n",
       " ['elephants', 'different habitats', 'savannahs forests deserts', 'marshes'],\n",
       " ['water'],\n",
       " ['keystone species', 'impact', 'environments'],\n",
       " ['animals',\n",
       "  'distance',\n",
       "  'elephants',\n",
       "  'predators',\n",
       "  'lions tigers hyenas',\n",
       "  'wild dogs',\n",
       "  'young elephants',\n",
       "  'calves'],\n",
       " ['elephants', 'fissionfusion society', 'multiple family groups'],\n",
       " ['females cows',\n",
       "  'family groups',\n",
       "  'female',\n",
       "  'calves',\n",
       "  'several related females'],\n",
       " ['groups', 'individual known', 'matriarch', 'cow'],\n",
       " ['males bulls', 'family groups', 'males'],\n",
       " ['adult',\n",
       "  'family groups',\n",
       "  'mate',\n",
       "  'enter state',\n",
       "  'increased testosterone',\n",
       "  'aggression',\n",
       "  'musth',\n",
       "  'dominance',\n",
       "  'reproductive success'],\n",
       " ['calves', 'centre', 'attention', 'family groups', 'mothers', 'years'],\n",
       " ['elephants', 'years', 'wild'],\n",
       " ['touch sight smell',\n",
       "  'sound elephants',\n",
       "  'infrasound',\n",
       "  'seismic communication',\n",
       "  'long distances'],\n",
       " ['elephant intelligence', 'primates', 'cetaceans'],\n",
       " ['selfawareness', 'dead individuals', 'kind'],\n",
       " ['african elephants',\n",
       "  'international union',\n",
       "  'conservation',\n",
       "  'nature iucn',\n",
       "  'asian elephant'],\n",
       " ['threats', 'populations', 'ivory trade', 'animals', 'ivory tusks'],\n",
       " ['threats', 'elephants', 'habitat destruction', 'conflicts', 'local people'],\n",
       " ['elephants', 'animals', 'asia'],\n",
       " ['past', 'war today', 'display', 'zoos', 'entertainment', 'circuses'],\n",
       " ['elephants', 'art folklore religion literature', 'popular culture']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = get_chunks(norm_sentences)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "def get_tfidf_weighted_keyphrases(sentences, grammar=r'NP: {<DT>? <JJ>* <NN.*>+}', top_n=10):\n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "    \n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    weighted_phrases = {dictionary.get(idx): value for doc in corpus_tfidf for idx, value in doc}\n",
    "    weighted_phrases = sorted(weighted_phrases.items(),\n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    weighted_phrases = [(term, round(wt,3)) for term, wt in weighted_phrases]\n",
    "    \n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 1.0),\n",
       " ('asia', 0.807),\n",
       " ('wild', 0.764),\n",
       " ('great weight', 0.707),\n",
       " ('pillarlike legs', 0.707),\n",
       " ('southeast asia', 0.693),\n",
       " ('subsaharan africa south asia', 0.693),\n",
       " ('body temperature', 0.693),\n",
       " ('flaps', 0.693),\n",
       " ('fissionfusion society', 0.693),\n",
       " ('multiple family groups', 0.693),\n",
       " ('art folklore religion literature', 0.693),\n",
       " ('popular culture', 0.693),\n",
       " ('ears', 0.681),\n",
       " ('males', 0.653),\n",
       " ('males bulls', 0.653),\n",
       " ('family elephantidae', 0.607),\n",
       " ('large mammals', 0.607),\n",
       " ('years', 0.607),\n",
       " ('environments', 0.577),\n",
       " ('impact', 0.577),\n",
       " ('keystone species', 0.577),\n",
       " ('cetaceans', 0.577),\n",
       " ('elephant intelligence', 0.577),\n",
       " ('primates', 0.577),\n",
       " ('dead individuals', 0.577),\n",
       " ('kind', 0.577),\n",
       " ('selfawareness', 0.577),\n",
       " ('different habitats', 0.57),\n",
       " ('marshes', 0.57)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 30 tf-idf weighted keyphrases\n",
    "get_tfidf_weighted_keyphrases(sentences=norm_sentences, top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('african bush elephant', 0.261),\n",
       " ('including', 0.141),\n",
       " ('family', 0.137),\n",
       " ('cow', 0.124),\n",
       " ('forests', 0.108),\n",
       " ('female', 0.103),\n",
       " ('asia', 0.102),\n",
       " ('objects', 0.098),\n",
       " ('sight', 0.098),\n",
       " ('ivory', 0.098),\n",
       " ('tigers', 0.098),\n",
       " ('males', 0.088),\n",
       " ('religion', 0.087),\n",
       " ('folklore', 0.087),\n",
       " ('known', 0.087),\n",
       " ('larger ears', 0.085),\n",
       " ('water', 0.075),\n",
       " ('highly recognisable', 0.075),\n",
       " ('breathing lifting', 0.074),\n",
       " ('flaps', 0.073),\n",
       " ('africa', 0.072),\n",
       " ('gomphotheres', 0.072),\n",
       " ('animals tend', 0.071),\n",
       " ('success', 0.071),\n",
       " ('south', 0.07)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "key_words = keywords(data[0], ratio=1.0, scores=True, lemmatize=True)\n",
    "[(item, round(score,3)) for item, score in key_words][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling on Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Retrieval\n",
    "#!wget https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset\n",
    "#!tar -xzf nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nips00', 'nips01', 'nips02', 'nips03', 'nips04', 'nips05', 'nips06', 'nips07', 'nips08', 'nips09', 'nips10', 'nips11', 'nips12']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = path_to_users + '/nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1719"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load and View Dataset\n",
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "# read all texts into a list\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a problem \n",
      "from examples using a local learning rule, we prove that the entropy of the \n",
      "problem becomes a lower bound for the connectivity of the network. \n",
      "INTRODUCTION \n",
      "The most distinguishing feature of neural networks is their ability to spon- \n",
      "taneously learn the desired function from 'training' samples, i.e., their ability \n",
      "to program themselves. Clearly, a given neural network cannot just learn any \n",
      "function, there must be some restrictions on which networks can learn which \n",
      "functions. One obv\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719\n",
      "CPU times: user 23.9 s, sys: 51.7 ms, total: 23.9 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Basic Text Wrangling\n",
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "        \n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connectivity', 'versus', 'entropy', 'yaser', 'abu', 'mostafa', 'california', 'institute', 'technology', 'pasadena', 'ca', 'abstract', 'doe', 'connectivity', 'neural', 'network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean', 'function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using']\n"
     ]
    }
   ],
   "source": [
    "# viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connectivity', 'versus', 'entropy', 'yaser', 'abu_mostafa', 'california_institute', 'technology_pasadena', 'ca_abstract', 'doe', 'connectivity', 'neural_network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean_function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using', 'local', 'learning', 'rule', 'prove', 'entropy', 'problem']\n"
     ]
    }
   ],
   "source": [
    "## Text Representation with Feature Engineering\n",
    "import gensim\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, \n",
    "                               threshold=20, delimiter=b'_') # higher threshold fewer phrases\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# sample demonstration\n",
    "print(bigram_model[norm_papers[0]][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings: [(0, '0a'), (1, '2h'), (2, '2h2'), (3, '2he'), (4, '2n'), (5, '__c'), (6, '_c'), (7, '_k'), (8, 'a2'), (9, 'ability'), (10, 'abu_mostafa'), (11, 'access'), (12, 'accommodate'), (13, 'according'), (14, 'accumulated')]\n",
      "Total Vocabulary Size 78252\n"
     ]
    }
   ],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "# create a dictionary representation of the documents\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 7692\n"
     ]
    }
   ],
   "source": [
    "# filer out words that occur less than 20 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (12, 3), (14, 1), (15, 1), (16, 1), (17, 16), (20, 1), (24, 1), (26, 1), (31, 3), (35, 1), (36, 1), (40, 3), (41, 5), (42, 1), (48, 1), (53, 3), (55, 1), (56, 2), (58, 1), (60, 3), (63, 5), (64, 4), (65, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 3), (82, 1), (83, 4), (84, 1), (85, 1), (86, 2), (93, 1), (95, 2), (96, 3), (105, 1), (109, 1), (118, 2), (119, 4), (120, 2), (123, 2), (126, 1), (127, 1), (131, 1), (132, 1), (134, 6), (135, 1), (143, 1)]\n"
     ]
    }
   ],
   "source": [
    "# transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ability', 1), ('aip', 3), ('although', 1), ('american_institute', 1), ('amount', 1), ('analog', 16), ('appears', 1), ('architecture', 1), ('aspect', 1), ('available', 3), ('become', 1), ('becomes', 1), ('binary', 3), ('biological', 5), ('bit', 1), ('cannot', 1), ('circuit', 3), ('collective', 1), ('compare', 2), ('complex', 1), ('computing', 3), ('conference', 5), ('connected', 4), ('connectivity', 2), ('define', 1), ('defined', 1), ('defines', 1), ('definition', 1), ('denker', 3), ('designed', 1), ('desired', 4), ('diagonal', 1), ('difference', 1), ('directly', 2), ('ed', 1), ('el', 2), ('element', 3), ('equivalent', 1), ('eventually', 1), ('feature', 2), ('final', 4), ('find', 2), ('fixed', 2), ('frequency', 1), ('furthermore', 1), ('generating', 1), ('get', 1), ('global', 6), ('go', 1), ('hence', 1)]\n"
     ]
    }
   ],
   "source": [
    "# viewing actual terms and their counts\n",
    "print([(dictionary[idx], freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 1719\n"
     ]
    }
   ],
   "source": [
    "# total papers in the corpus\n",
    "print('Total number of papers:', len(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 57s, sys: 3min 41s, total: 9min 38s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Latent Semantic Indexing\n",
    "TOTAL_TOPICS = 10\n",
    "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS, \n",
    "                                 onepass=True, chunksize=1740, power_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.217*\"unit\" + 0.204*\"state\" + 0.189*\"training\" + 0.180*\"neuron\" + 0.164*\"pattern\" + 0.143*\"image\" + 0.140*\"vector\" + 0.123*\"feature\" + 0.123*\"cell\" + 0.112*\"layer\" + 0.100*\"task\" + 0.095*\"class\" + 0.090*\"probability\" + 0.090*\"signal\" + 0.087*\"response\" + 0.086*\"step\" + 0.085*\"representation\" + 0.083*\"rule\" + 0.083*\"noise\" + 0.081*\"node\"\n",
      "\n",
      "Topic #2:\n",
      "-0.501*\"neuron\" + -0.401*\"cell\" + 0.206*\"training\" + 0.193*\"state\" + -0.190*\"response\" + -0.172*\"stimulus\" + -0.117*\"activity\" + 0.115*\"class\" + 0.102*\"vector\" + -0.101*\"spike\" + -0.099*\"circuit\" + -0.098*\"synaptic\" + 0.095*\"classifier\" + -0.092*\"firing\" + -0.089*\"signal\" + -0.086*\"pattern\" + -0.085*\"visual\" + 0.084*\"word\" + -0.078*\"cortical\" + 0.076*\"task\"\n",
      "\n",
      "Topic #3:\n",
      "-0.626*\"state\" + 0.411*\"image\" + 0.210*\"feature\" + -0.194*\"neuron\" + -0.188*\"action\" + 0.144*\"object\" + 0.139*\"unit\" + -0.135*\"control\" + 0.100*\"training\" + -0.095*\"policy\" + 0.092*\"classifier\" + -0.089*\"step\" + 0.085*\"layer\" + -0.082*\"dynamic\" + 0.073*\"representation\" + 0.073*\"recognition\" + 0.072*\"pattern\" + 0.072*\"classification\" + -0.071*\"optimal\" + 0.070*\"class\"\n",
      "\n",
      "Topic #4:\n",
      "0.735*\"unit\" + -0.305*\"image\" + 0.166*\"pattern\" + -0.160*\"neuron\" + 0.128*\"layer\" + 0.118*\"hidden_unit\" + 0.112*\"net\" + 0.107*\"activation\" + 0.092*\"rule\" + -0.090*\"class\" + -0.084*\"vector\" + -0.083*\"distribution\" + -0.081*\"feature\" + -0.069*\"classifier\" + -0.068*\"linear\" + 0.067*\"connection\" + 0.065*\"word\" + -0.065*\"estimate\" + 0.062*\"activity\" + -0.061*\"sample\"\n",
      "\n",
      "Topic #5:\n",
      "0.506*\"image\" + 0.385*\"state\" + -0.229*\"neuron\" + -0.203*\"training\" + 0.174*\"object\" + -0.166*\"class\" + 0.166*\"action\" + -0.163*\"classifier\" + 0.123*\"visual\" + 0.119*\"control\" + 0.116*\"feature\" + -0.112*\"node\" + -0.095*\"pattern\" + -0.095*\"vector\" + 0.095*\"motion\" + -0.090*\"distribution\" + -0.089*\"classification\" + 0.087*\"task\" + 0.080*\"cell\" + 0.080*\"face\"\n",
      "\n",
      "Topic #6:\n",
      "-0.661*\"cell\" + 0.509*\"neuron\" + 0.208*\"image\" + 0.112*\"unit\" + 0.102*\"chip\" + -0.094*\"response\" + 0.094*\"object\" + -0.084*\"rat\" + 0.071*\"circuit\" + -0.070*\"distribution\" + -0.068*\"probability\" + -0.067*\"stimulus\" + 0.059*\"activation\" + 0.058*\"analog\" + 0.058*\"memory\" + -0.055*\"class\" + 0.054*\"bit\" + 0.053*\"net\" + 0.052*\"feature\" + 0.052*\"vector\"\n",
      "\n",
      "Topic #7:\n",
      "0.358*\"word\" + -0.275*\"unit\" + 0.272*\"training\" + 0.255*\"classifier\" + 0.178*\"recognition\" + -0.161*\"distribution\" + 0.145*\"feature\" + 0.145*\"pattern\" + -0.142*\"vector\" + 0.139*\"state\" + 0.129*\"task\" + -0.123*\"variable\" + -0.122*\"approximation\" + -0.111*\"equation\" + 0.111*\"cell\" + 0.109*\"neuron\" + 0.106*\"classification\" + -0.101*\"noise\" + 0.099*\"class\" + -0.099*\"matrix\"\n",
      "\n",
      "Topic #8:\n",
      "0.317*\"pattern\" + -0.244*\"signal\" + -0.199*\"control\" + -0.196*\"training\" + 0.194*\"rule\" + 0.180*\"state\" + -0.175*\"word\" + -0.168*\"noise\" + 0.160*\"cell\" + -0.154*\"motion\" + 0.152*\"class\" + 0.143*\"node\" + 0.143*\"feature\" + -0.138*\"task\" + 0.121*\"classifier\" + -0.120*\"target\" + 0.117*\"image\" + 0.116*\"vector\" + 0.112*\"memory\" + -0.111*\"circuit\"\n",
      "\n",
      "Topic #9:\n",
      "-0.475*\"node\" + -0.271*\"circuit\" + -0.215*\"chip\" + 0.198*\"neuron\" + 0.176*\"word\" + 0.176*\"stimulus\" + -0.164*\"current\" + -0.159*\"voltage\" + -0.141*\"classifier\" + 0.138*\"distribution\" + 0.128*\"feature\" + -0.125*\"control\" + -0.113*\"analog\" + -0.109*\"rule\" + -0.107*\"layer\" + 0.092*\"state\" + -0.087*\"tree\" + 0.085*\"response\" + 0.079*\"probability\" + 0.079*\"estimate\"\n",
      "\n",
      "Topic #10:\n",
      "0.528*\"word\" + -0.254*\"training\" + 0.242*\"vector\" + -0.227*\"task\" + -0.180*\"pattern\" + -0.161*\"classifier\" + 0.150*\"recognition\" + -0.147*\"control\" + 0.142*\"sequence\" + -0.133*\"rule\" + -0.124*\"action\" + 0.123*\"node\" + 0.122*\"cell\" + 0.104*\"circuit\" + 0.098*\"hmm\" + -0.093*\"neuron\" + 0.092*\"character\" + 0.090*\"matrix\" + 0.086*\"structure\" + 0.080*\"phoneme\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lsi_bow.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.217), ('state', 0.204), ('training', 0.189), ('neuron', 0.18), ('pattern', 0.164), ('image', 0.143), ('vector', 0.14), ('feature', 0.123), ('cell', 0.123), ('layer', 0.112), ('task', 0.1), ('class', 0.095), ('probability', 0.09), ('signal', 0.09), ('response', 0.087), ('step', 0.086), ('representation', 0.085), ('rule', 0.083), ('noise', 0.083), ('node', 0.081)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('training', 0.206), ('state', 0.193), ('class', 0.115), ('vector', 0.102), ('classifier', 0.095), ('word', 0.084), ('task', 0.076)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -0.501), ('cell', -0.401), ('response', -0.19), ('stimulus', -0.172), ('activity', -0.117), ('spike', -0.101), ('circuit', -0.099), ('synaptic', -0.098), ('firing', -0.092), ('signal', -0.089), ('pattern', -0.086), ('visual', -0.085), ('cortical', -0.078)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.411), ('feature', 0.21), ('object', 0.144), ('unit', 0.139), ('training', 0.1), ('classifier', 0.092), ('layer', 0.085), ('representation', 0.073), ('recognition', 0.073), ('pattern', 0.072), ('classification', 0.072), ('class', 0.07)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.626), ('neuron', -0.194), ('action', -0.188), ('control', -0.135), ('policy', -0.095), ('step', -0.089), ('dynamic', -0.082), ('optimal', -0.071)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.735), ('pattern', 0.166), ('layer', 0.128), ('hidden_unit', 0.118), ('net', 0.112), ('activation', 0.107), ('rule', 0.092), ('connection', 0.067), ('word', 0.065), ('activity', 0.062)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.305), ('neuron', -0.16), ('class', -0.09), ('vector', -0.084), ('distribution', -0.083), ('feature', -0.081), ('classifier', -0.069), ('linear', -0.068), ('estimate', -0.065), ('sample', -0.061)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.506), ('state', 0.385), ('object', 0.174), ('action', 0.166), ('visual', 0.123), ('control', 0.119), ('feature', 0.116), ('motion', 0.095), ('task', 0.087), ('cell', 0.08), ('face', 0.08)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -0.229), ('training', -0.203), ('class', -0.166), ('classifier', -0.163), ('node', -0.112), ('pattern', -0.095), ('vector', -0.095), ('distribution', -0.09), ('classification', -0.089)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.509), ('image', 0.208), ('unit', 0.112), ('chip', 0.102), ('object', 0.094), ('circuit', 0.071), ('activation', 0.059), ('analog', 0.058), ('memory', 0.058), ('bit', 0.054), ('net', 0.053), ('feature', 0.052), ('vector', 0.052)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cell', -0.661), ('response', -0.094), ('rat', -0.084), ('distribution', -0.07), ('probability', -0.068), ('stimulus', -0.067), ('class', -0.055)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.358), ('training', 0.272), ('classifier', 0.255), ('recognition', 0.178), ('feature', 0.145), ('pattern', 0.145), ('state', 0.139), ('task', 0.129), ('cell', 0.111), ('neuron', 0.109), ('classification', 0.106), ('class', 0.099)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -0.275), ('distribution', -0.161), ('vector', -0.142), ('variable', -0.123), ('approximation', -0.122), ('equation', -0.111), ('noise', -0.101), ('matrix', -0.099)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('pattern', 0.317), ('rule', 0.194), ('state', 0.18), ('cell', 0.16), ('class', 0.152), ('node', 0.143), ('feature', 0.143), ('classifier', 0.121), ('image', 0.117), ('vector', 0.116), ('memory', 0.112)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('signal', -0.244), ('control', -0.199), ('training', -0.196), ('word', -0.175), ('noise', -0.168), ('motion', -0.154), ('task', -0.138), ('target', -0.12), ('circuit', -0.111)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.198), ('word', 0.176), ('stimulus', 0.176), ('distribution', 0.138), ('feature', 0.128), ('state', 0.092), ('response', 0.085), ('probability', 0.079), ('estimate', 0.079)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('node', -0.475), ('circuit', -0.271), ('chip', -0.215), ('current', -0.164), ('voltage', -0.159), ('classifier', -0.141), ('control', -0.125), ('analog', -0.113), ('rule', -0.109), ('layer', -0.107), ('tree', -0.087)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.528), ('vector', 0.242), ('recognition', 0.15), ('sequence', 0.142), ('node', 0.123), ('cell', 0.122), ('circuit', 0.104), ('hmm', 0.098), ('character', 0.092), ('matrix', 0.09), ('structure', 0.086), ('phoneme', 0.08)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -0.254), ('task', -0.227), ('pattern', -0.18), ('classifier', -0.161), ('control', -0.147), ('rule', -0.133), ('action', -0.124), ('neuron', -0.093)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt,3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt,3)))\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7692, 10), (10,), (10, 1719))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get U, S, VT matrices from topic model\n",
    "term_topic = lsi_bow.projection.u\n",
    "singular_values = lsi_bow.projection.s\n",
    "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    T1    T2    T3    T4    T5   T6    T7    T8    T9   T10\n",
       "0 0.02 -0.02 -0.01 -0.01 -0.02 0.03  0.00  0.02  0.01 -0.01\n",
       "1 0.04 -0.03 -0.02  0.01 -0.02 0.06 -0.02 -0.01 -0.02 -0.01\n",
       "2 0.02 -0.00 -0.02 -0.01 -0.01 0.02 -0.01  0.02  0.00  0.01\n",
       "3 0.03 -0.04 -0.01  0.00 -0.04 0.05  0.02  0.04  0.01 -0.03\n",
       "4 0.04  0.00 -0.02  0.00 -0.02 0.02 -0.03  0.02 -0.05  0.03"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document topic matrix for our LSI model\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
    "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "document_topics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #13:\n",
      "Dominant Topics (top 3): ['T3', 'T8', 'T9']\n",
      "Paper Summary:\n",
      "137 \n",
      "On the \n",
      "Power of Neural Networks for \n",
      "Solving Hard Problems \n",
      "Jehoshua Bruck \n",
      "Joseph W. Goodman \n",
      "Information Systems Laboratory \n",
      "Department of Electrical Engineering \n",
      "Stanford University \n",
      "Stanford, CA 94305 \n",
      "Abstract \n",
      "This paper deals with a neural network model in which each neuron \n",
      "performs a threshold logic function. An important property of the model \n",
      "is that it always converges to a stable state when operating in a serial \n",
      "mode [2,5]. This property is the basis of the potential applicat\n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T9', 'T1', 'T8']\n",
      "Paper Summary:\n",
      "542 Kassebaum, Tenorio and Schaefers \n",
      "The Cocktail Party Problem: \n",
      "Speech/Data Signal Separation Comparison \n",
      "between Backpropagation and SONN \n",
      "John Kassebaum \n",
      "jakec.ecn.purdue.edu \n",
      "Manoel Fernando Tenorio \n",
      "tenorioee.ecn.purdue.edu \n",
      "Chrlstoph Schaefers \n",
      "Parallel Distributed Structures Laboratory \n",
      "School of Electrical Engineering \n",
      "Purdue University \n",
      "W. Lafayette, IN. 47907 \n",
      "ABSTRACT \n",
      "This work introduces a new method called Self Organizing Neural \n",
      "Network (SONN) algorithm and compares its perfor\n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T1', 'T10', 'T7']\n",
      "Paper Summary:\n",
      "Learning Global Direct Inverse Kinematics \n",
      "David DeMers* \n",
      "Computer Science & Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0114 \n",
      "Kenneth Kreutz-Deigado I \n",
      "Electrical & Computer Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0407 \n",
      "Abstract \n",
      "We introduce and demonstrate a bootstrap method for construction of an in- \n",
      "verse function for the robot kinematic mapping using only sample configuration- \n",
      "space/workspace data. Unsupervised learning (clustering) techniques are used on \n",
      "pre-image neighborhoods in order to l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_numbers = [13, 250, 500]\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.\n",
    "                      columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7692, 1719)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 2.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Implementing LSI Topic Models from Scratch\n",
    "td_matrix = gensim.matutils.corpus2dense(corpus=bow_corpus, num_terms=len(dictionary))\n",
    "print(td_matrix.shape)\n",
    "td_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 7692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['2n', '_c', 'a2', ..., 'smola', 'support_vector', 'mozer_jordan'],\n",
       "      dtype='<U28')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = np.array(list(dictionary.values()))\n",
    "print('Total vocabulary size:', len(vocabulary))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7692, 10), (10,), (10, 1719))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "u, s, vt = svds(td_matrix, k=TOTAL_TOPICS, maxiter=10000)\n",
    "term_topic = u\n",
    "singular_values = s\n",
    "topic_document = vt\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7692)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_weights = term_topic.transpose() * singular_values[:,None]\n",
    "tt_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('word', 192.091), ('vector', 87.984), ('recognition', 54.779), ('sequence', 51.589), ('node', 44.675), ('cell', 44.443), ('circuit', 37.746), ('hmm', 35.687), ('character', 33.4), ('matrix', 32.903), ('structure', 31.385), ('phoneme', 29.164)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -92.608), ('task', -82.594), ('pattern', -65.608), ('classifier', -58.539), ('control', -53.672), ('rule', -48.237), ('action', -45.226), ('neuron', -33.821)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('node', 173.925), ('circuit', 99.14), ('chip', 78.507), ('current', 60.015), ('voltage', 58.008), ('classifier', 51.738), ('control', 45.65), ('analog', 41.497), ('rule', 39.847), ('layer', 38.983), ('tree', 31.974)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -72.423), ('word', -64.314), ('stimulus', -64.259), ('distribution', -50.538), ('feature', -46.737), ('state', -33.514), ('response', -31.249), ('probability', -28.968), ('estimate', -28.805)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('pattern', 121.716), ('rule', 74.484), ('state', 69.207), ('cell', 61.285), ('class', 58.256), ('node', 54.879), ('feature', 54.749), ('classifier', 46.271), ('image', 44.802), ('vector', 44.358), ('memory', 42.816)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('signal', -93.44), ('control', -76.254), ('training', -75.278), ('word', -66.977), ('noise', -64.383), ('motion', -59.04), ('task', -52.805), ('target', -46.133), ('circuit', -42.614)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('word', 149.262), ('training', 113.556), ('classifier', 106.543), ('recognition', 74.38), ('feature', 60.347), ('pattern', 60.337), ('state', 58.219), ('task', 53.807), ('cell', 46.212), ('neuron', 45.314), ('classification', 44.164), ('class', 41.427)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -114.68), ('distribution', -67.215), ('vector', -59.339), ('variable', -51.544), ('approximation', -50.792), ('equation', -46.537), ('noise', -42.356), ('matrix', -41.236)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('cell', 285.359), ('response', 40.75), ('rat', 36.177), ('distribution', 30.042), ('probability', 29.384), ('stimulus', 28.982), ('class', 23.922)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -219.643), ('image', -89.86), ('unit', -48.125), ('chip', -44.104), ('object', -40.51), ('circuit', -30.528), ('activation', -25.461), ('analog', -24.975), ('memory', -24.949), ('bit', -23.194), ('net', -23.056), ('feature', -22.588), ('vector', -22.568)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('image', 245.541), ('state', 186.77), ('object', 84.486), ('action', 80.4), ('visual', 59.883), ('control', 57.839), ('feature', 56.34), ('motion', 46.116), ('task', 42.263), ('cell', 39.008), ('face', 38.852)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -110.965), ('training', -98.36), ('class', -80.447), ('classifier', -78.938), ('node', -54.172), ('pattern', -46.315), ('vector', -46.29), ('distribution', -43.832), ('classification', -43.267)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('image', 150.735), ('neuron', 79.358), ('class', 44.762), ('vector', 41.613), ('distribution', 41.139), ('feature', 40.029), ('classifier', 34.36), ('linear', 33.64), ('estimate', 32.035), ('sample', 30.226)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -363.506), ('pattern', -82.11), ('layer', -63.329), ('hidden_unit', -58.425), ('net', -55.303), ('activation', -53.023), ('rule', -45.595), ('connection', -32.958), ('word', -32.214), ('activity', -30.573)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('state', 355.529), ('neuron', 110.342), ('action', 107.014), ('control', 76.658), ('policy', 53.881), ('step', 50.776), ('dynamic', 46.722), ('optimal', 40.181)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -233.375), ('feature', -119.436), ('object', -82.111), ('unit', -78.827), ('training', -56.783), ('classifier', -52.05), ('layer', -48.369), ('representation', -41.772), ('recognition', -41.249), ('pattern', -40.998), ('classification', -40.766), ('class', -39.97)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('training', 128.835), ('state', 120.508), ('class', 71.916), ('vector', 63.539), ('classifier', 59.119), ('word', 52.675), ('task', 47.36)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -312.992), ('cell', -250.545), ('response', -118.88), ('stimulus', -107.685), ('activity', -73.19), ('spike', -63.159), ('circuit', -61.711), ('synaptic', -61.453), ('firing', -57.408), ('signal', -55.526), ('pattern', -53.985), ('visual', -53.333), ('cortical', -49.023)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('unit', 263.066), ('state', 246.357), ('training', 228.515), ('neuron', 217.953), ('pattern', 198.612), ('image', 172.954), ('vector', 169.706), ('feature', 148.946), ('cell', 148.485), ('layer', 134.877), ('task', 121.328), ('class', 114.898), ('probability', 108.796), ('signal', 108.451), ('response', 105.337), ('step', 103.534), ('representation', 102.265), ('rule', 100.164), ('noise', 99.84), ('node', 97.592)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_terms = 20\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(tt_weights), axis=1)[:, :top_terms]\n",
    "topic_keyterm_weights = np.array([tt_weights[row, columns]\n",
    "                                 for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    terms, weights = topic_keyterms_weights[n]\n",
    "    term_weights = sorted([(t,w) for t, w in zip(terms, weights)], key=lambda row: -abs(row[1]))\n",
    "    for term, wt in term_weights:\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #13:\n",
      "Dominant Topics (top 3): ['T8', 'T3', 'T2']\n",
      "Paper Summary:\n",
      "137 \n",
      "On the \n",
      "Power of Neural Networks for \n",
      "Solving Hard Problems \n",
      "Jehoshua Bruck \n",
      "Joseph W. Goodman \n",
      "Information Systems Laboratory \n",
      "Department of Electrical Engineering \n",
      "Stanford University \n",
      "Stanford, CA 94305 \n",
      "Abstract \n",
      "This paper deals with a neural network model in which each neuron \n",
      "performs a threshold logic function. An important property of the model \n",
      "is that it always converges to a stable state when operating in a serial \n",
      "mode [2,5]. This property is the basis of the potential applicat\n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T2', 'T3', 'T10']\n",
      "Paper Summary:\n",
      "542 Kassebaum, Tenorio and Schaefers \n",
      "The Cocktail Party Problem: \n",
      "Speech/Data Signal Separation Comparison \n",
      "between Backpropagation and SONN \n",
      "John Kassebaum \n",
      "jakec.ecn.purdue.edu \n",
      "Manoel Fernando Tenorio \n",
      "tenorioee.ecn.purdue.edu \n",
      "Chrlstoph Schaefers \n",
      "Parallel Distributed Structures Laboratory \n",
      "School of Electrical Engineering \n",
      "Purdue University \n",
      "W. Lafayette, IN. 47907 \n",
      "ABSTRACT \n",
      "This work introduces a new method called Self Organizing Neural \n",
      "Network (SONN) algorithm and compares its perfor\n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T10', 'T1', 'T4']\n",
      "Paper Summary:\n",
      "Learning Global Direct Inverse Kinematics \n",
      "David DeMers* \n",
      "Computer Science & Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0114 \n",
      "Kenneth Kreutz-Deigado I \n",
      "Electrical & Computer Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0407 \n",
      "Abstract \n",
      "We introduce and demonstrate a bootstrap method for construction of an in- \n",
      "verse function for the robot kinematic mapping using only sample configuration- \n",
      "space/workspace data. Unsupervised learning (clustering) techniques are used on \n",
      "pre-image neighborhoods in order to l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
    "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "document_numbers = [13, 250, 500]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.\n",
    "                      columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 4.44 s, total: 1min 18s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Latent Dirichlet Allocation\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, \n",
    "                                   chunksize=1740, alpha='auto', eta='auto', random_state=42, \n",
    "                                   iterations=500, num_topics=TOTAL_TOPICS, \n",
    "                                   passes=20, eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.017*\"circuit\" + 0.010*\"chip\" + 0.010*\"analog\" + 0.009*\"voltage\" + 0.009*\"current\" + 0.006*\"threshold\" + 0.006*\"bit\" + 0.006*\"code\" + 0.005*\"vector\" + 0.005*\"neuron\" + 0.005*\"element\" + 0.005*\"computation\" + 0.005*\"node\" + 0.004*\"gate\" + 0.004*\"signal\" + 0.004*\"memory\" + 0.004*\"transistor\" + 0.004*\"size\" + 0.004*\"device\" + 0.004*\"synapse\"\n",
      "\n",
      "Topic #2:\n",
      "0.016*\"control\" + 0.008*\"state\" + 0.008*\"task\" + 0.007*\"controller\" + 0.006*\"position\" + 0.005*\"training\" + 0.005*\"robot\" + 0.005*\"prediction\" + 0.004*\"dynamic\" + 0.004*\"motor\" + 0.004*\"trajectory\" + 0.004*\"movement\" + 0.004*\"move\" + 0.004*\"environment\" + 0.003*\"goal\" + 0.003*\"step\" + 0.003*\"adaptive\" + 0.003*\"search\" + 0.003*\"arm\" + 0.003*\"region\"\n",
      "\n",
      "Topic #3:\n",
      "0.008*\"distribution\" + 0.007*\"class\" + 0.006*\"probability\" + 0.006*\"training\" + 0.005*\"variable\" + 0.005*\"estimate\" + 0.005*\"sample\" + 0.005*\"approximation\" + 0.004*\"gaussian\" + 0.004*\"prior\" + 0.004*\"linear\" + 0.004*\"vector\" + 0.003*\"bound\" + 0.003*\"prediction\" + 0.003*\"density\" + 0.003*\"classification\" + 0.003*\"kernel\" + 0.003*\"bayesian\" + 0.003*\"let\" + 0.003*\"variance\"\n",
      "\n",
      "Topic #4:\n",
      "0.035*\"state\" + 0.014*\"action\" + 0.009*\"policy\" + 0.008*\"step\" + 0.008*\"optimal\" + 0.006*\"reinforcement_learning\" + 0.005*\"control\" + 0.004*\"probability\" + 0.004*\"rate\" + 0.004*\"reward\" + 0.004*\"machine\" + 0.004*\"agent\" + 0.004*\"student\" + 0.004*\"task\" + 0.004*\"equation\" + 0.004*\"transition\" + 0.004*\"teacher\" + 0.003*\"rl\" + 0.003*\"training\" + 0.003*\"convergence\"\n",
      "\n",
      "Topic #5:\n",
      "0.014*\"classifier\" + 0.010*\"layer\" + 0.009*\"neuron\" + 0.008*\"memory\" + 0.008*\"processor\" + 0.008*\"training\" + 0.007*\"bit\" + 0.007*\"pattern\" + 0.007*\"chip\" + 0.007*\"node\" + 0.007*\"connection\" + 0.005*\"classification\" + 0.005*\"architecture\" + 0.005*\"application\" + 0.005*\"parallel\" + 0.005*\"unit\" + 0.004*\"machine\" + 0.004*\"vector\" + 0.004*\"computer\" + 0.004*\"feature\"\n",
      "\n",
      "Topic #6:\n",
      "0.018*\"neuron\" + 0.016*\"cell\" + 0.010*\"response\" + 0.009*\"stimulus\" + 0.008*\"activity\" + 0.007*\"pattern\" + 0.005*\"spike\" + 0.005*\"unit\" + 0.005*\"signal\" + 0.005*\"synaptic\" + 0.004*\"neural\" + 0.004*\"cortical\" + 0.004*\"effect\" + 0.004*\"connection\" + 0.004*\"visual\" + 0.004*\"frequency\" + 0.004*\"firing\" + 0.004*\"layer\" + 0.004*\"et_al\" + 0.003*\"cortex\"\n",
      "\n",
      "Topic #7:\n",
      "0.020*\"unit\" + 0.013*\"training\" + 0.009*\"pattern\" + 0.007*\"hidden_unit\" + 0.007*\"rule\" + 0.007*\"node\" + 0.007*\"net\" + 0.006*\"task\" + 0.006*\"layer\" + 0.006*\"representation\" + 0.006*\"trained\" + 0.005*\"architecture\" + 0.005*\"word\" + 0.005*\"activation\" + 0.004*\"sequence\" + 0.004*\"training_set\" + 0.004*\"structure\" + 0.004*\"level\" + 0.003*\"character\" + 0.003*\"connectionist\"\n",
      "\n",
      "Topic #8:\n",
      "0.010*\"vector\" + 0.008*\"neuron\" + 0.008*\"equation\" + 0.008*\"matrix\" + 0.006*\"solution\" + 0.006*\"dynamic\" + 0.005*\"state\" + 0.005*\"pattern\" + 0.005*\"convergence\" + 0.004*\"rate\" + 0.004*\"gradient\" + 0.004*\"linear\" + 0.004*\"rule\" + 0.004*\"eq\" + 0.004*\"noise\" + 0.003*\"unit\" + 0.003*\"energy\" + 0.003*\"constraint\" + 0.003*\"nonlinear\" + 0.003*\"fixed_point\"\n",
      "\n",
      "Topic #9:\n",
      "0.013*\"training\" + 0.011*\"speech\" + 0.010*\"word\" + 0.010*\"recognition\" + 0.008*\"feature\" + 0.008*\"hmm\" + 0.008*\"state\" + 0.006*\"speaker\" + 0.006*\"vector\" + 0.005*\"frame\" + 0.005*\"class\" + 0.005*\"sequence\" + 0.005*\"mlp\" + 0.005*\"rbf\" + 0.004*\"classifier\" + 0.004*\"probability\" + 0.004*\"experiment\" + 0.004*\"speech_recognition\" + 0.004*\"context\" + 0.004*\"trained\"\n",
      "\n",
      "Topic #10:\n",
      "0.031*\"image\" + 0.012*\"object\" + 0.011*\"feature\" + 0.007*\"visual\" + 0.006*\"filter\" + 0.006*\"representation\" + 0.006*\"pixel\" + 0.006*\"signal\" + 0.005*\"face\" + 0.004*\"motion\" + 0.004*\"view\" + 0.004*\"location\" + 0.004*\"region\" + 0.004*\"local\" + 0.003*\"source\" + 0.003*\"position\" + 0.003*\"field\" + 0.003*\"scale\" + 0.003*\"vector\" + 0.003*\"shape\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view topics in trained topic model\n",
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score: -1.091462223513131\n"
     ]
    }
   ],
   "source": [
    "# view overall mean coherence score of model\n",
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('classifier', 0.014), ('layer', 0.01), ('neuron', 0.009), ('memory', 0.008), ('processor', 0.008), ('training', 0.008), ('bit', 0.007), ('pattern', 0.007), ('chip', 0.007), ('node', 0.007), ('connection', 0.007), ('classification', 0.005), ('architecture', 0.005), ('application', 0.005), ('parallel', 0.005), ('unit', 0.005), ('machine', 0.004), ('vector', 0.004), ('computer', 0.004), ('feature', 0.004)]\n",
      "\n",
      "Topic #2:\n",
      "[('neuron', 0.018), ('cell', 0.016), ('response', 0.01), ('stimulus', 0.009), ('activity', 0.008), ('pattern', 0.007), ('spike', 0.005), ('unit', 0.005), ('signal', 0.005), ('synaptic', 0.005), ('neural', 0.004), ('cortical', 0.004), ('effect', 0.004), ('connection', 0.004), ('visual', 0.004), ('frequency', 0.004), ('firing', 0.004), ('layer', 0.004), ('et_al', 0.004), ('cortex', 0.003)]\n",
      "\n",
      "Topic #3:\n",
      "[('unit', 0.02), ('training', 0.013), ('pattern', 0.009), ('hidden_unit', 0.007), ('rule', 0.007), ('node', 0.007), ('net', 0.007), ('task', 0.006), ('layer', 0.006), ('representation', 0.006), ('trained', 0.006), ('architecture', 0.005), ('word', 0.005), ('activation', 0.005), ('sequence', 0.004), ('training_set', 0.004), ('structure', 0.004), ('level', 0.004), ('character', 0.003), ('connectionist', 0.003)]\n",
      "\n",
      "Topic #4:\n",
      "[('image', 0.031), ('object', 0.012), ('feature', 0.011), ('visual', 0.007), ('filter', 0.006), ('representation', 0.006), ('pixel', 0.006), ('signal', 0.006), ('face', 0.005), ('motion', 0.004), ('view', 0.004), ('location', 0.004), ('region', 0.004), ('local', 0.004), ('source', 0.003), ('position', 0.003), ('field', 0.003), ('scale', 0.003), ('vector', 0.003), ('shape', 0.003)]\n",
      "\n",
      "Topic #5:\n",
      "[('distribution', 0.008), ('class', 0.007), ('probability', 0.006), ('training', 0.006), ('variable', 0.005), ('estimate', 0.005), ('sample', 0.005), ('approximation', 0.005), ('gaussian', 0.004), ('prior', 0.004), ('linear', 0.004), ('vector', 0.004), ('bound', 0.003), ('prediction', 0.003), ('density', 0.003), ('classification', 0.003), ('kernel', 0.003), ('bayesian', 0.003), ('let', 0.003), ('variance', 0.003)]\n",
      "\n",
      "Topic #6:\n",
      "[('vector', 0.01), ('neuron', 0.008), ('equation', 0.008), ('matrix', 0.008), ('solution', 0.006), ('dynamic', 0.006), ('state', 0.005), ('pattern', 0.005), ('convergence', 0.005), ('rate', 0.004), ('gradient', 0.004), ('linear', 0.004), ('rule', 0.004), ('eq', 0.004), ('noise', 0.004), ('unit', 0.003), ('energy', 0.003), ('constraint', 0.003), ('nonlinear', 0.003), ('fixed_point', 0.003)]\n",
      "\n",
      "Topic #7:\n",
      "[('training', 0.013), ('speech', 0.011), ('word', 0.01), ('recognition', 0.01), ('feature', 0.008), ('hmm', 0.008), ('state', 0.008), ('speaker', 0.006), ('vector', 0.006), ('frame', 0.005), ('class', 0.005), ('sequence', 0.005), ('mlp', 0.005), ('rbf', 0.005), ('classifier', 0.004), ('probability', 0.004), ('experiment', 0.004), ('speech_recognition', 0.004), ('context', 0.004), ('trained', 0.004)]\n",
      "\n",
      "Topic #8:\n",
      "[('circuit', 0.017), ('chip', 0.01), ('analog', 0.01), ('voltage', 0.009), ('current', 0.009), ('threshold', 0.006), ('bit', 0.006), ('code', 0.006), ('vector', 0.005), ('neuron', 0.005), ('element', 0.005), ('computation', 0.005), ('node', 0.005), ('gate', 0.004), ('signal', 0.004), ('memory', 0.004), ('transistor', 0.004), ('size', 0.004), ('device', 0.004), ('synapse', 0.004)]\n",
      "\n",
      "Topic #9:\n",
      "[('control', 0.016), ('state', 0.008), ('task', 0.008), ('controller', 0.007), ('position', 0.006), ('training', 0.005), ('robot', 0.005), ('prediction', 0.005), ('dynamic', 0.004), ('motor', 0.004), ('trajectory', 0.004), ('movement', 0.004), ('move', 0.004), ('environment', 0.004), ('goal', 0.003), ('step', 0.003), ('adaptive', 0.003), ('search', 0.003), ('arm', 0.003), ('region', 0.003)]\n",
      "\n",
      "Topic #10:\n",
      "[('state', 0.035), ('action', 0.014), ('policy', 0.009), ('step', 0.008), ('optimal', 0.008), ('reinforcement_learning', 0.006), ('control', 0.005), ('probability', 0.004), ('rate', 0.004), ('reward', 0.004), ('machine', 0.004), ('agent', 0.004), ('student', 0.004), ('task', 0.004), ('equation', 0.004), ('transition', 0.004), ('teacher', 0.004), ('rl', 0.003), ('training', 0.003), ('convergence', 0.003)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output of topic models as tuples of terms and weights\n",
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([(term, round(wt,3)) for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics without Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "['classifier', 'layer', 'neuron', 'memory', 'processor', 'training', 'bit', 'pattern', 'chip', 'node', 'connection', 'classification', 'architecture', 'application', 'parallel', 'unit', 'machine', 'vector', 'computer', 'feature']\n",
      "\n",
      "Topic #2:\n",
      "['neuron', 'cell', 'response', 'stimulus', 'activity', 'pattern', 'spike', 'unit', 'signal', 'synaptic', 'neural', 'cortical', 'effect', 'connection', 'visual', 'frequency', 'firing', 'layer', 'et_al', 'cortex']\n",
      "\n",
      "Topic #3:\n",
      "['unit', 'training', 'pattern', 'hidden_unit', 'rule', 'node', 'net', 'task', 'layer', 'representation', 'trained', 'architecture', 'word', 'activation', 'sequence', 'training_set', 'structure', 'level', 'character', 'connectionist']\n",
      "\n",
      "Topic #4:\n",
      "['image', 'object', 'feature', 'visual', 'filter', 'representation', 'pixel', 'signal', 'face', 'motion', 'view', 'location', 'region', 'local', 'source', 'position', 'field', 'scale', 'vector', 'shape']\n",
      "\n",
      "Topic #5:\n",
      "['distribution', 'class', 'probability', 'training', 'variable', 'estimate', 'sample', 'approximation', 'gaussian', 'prior', 'linear', 'vector', 'bound', 'prediction', 'density', 'classification', 'kernel', 'bayesian', 'let', 'variance']\n",
      "\n",
      "Topic #6:\n",
      "['vector', 'neuron', 'equation', 'matrix', 'solution', 'dynamic', 'state', 'pattern', 'convergence', 'rate', 'gradient', 'linear', 'rule', 'eq', 'noise', 'unit', 'energy', 'constraint', 'nonlinear', 'fixed_point']\n",
      "\n",
      "Topic #7:\n",
      "['training', 'speech', 'word', 'recognition', 'feature', 'hmm', 'state', 'speaker', 'vector', 'frame', 'class', 'sequence', 'mlp', 'rbf', 'classifier', 'probability', 'experiment', 'speech_recognition', 'context', 'trained']\n",
      "\n",
      "Topic #8:\n",
      "['circuit', 'chip', 'analog', 'voltage', 'current', 'threshold', 'bit', 'code', 'vector', 'neuron', 'element', 'computation', 'node', 'gate', 'signal', 'memory', 'transistor', 'size', 'device', 'synapse']\n",
      "\n",
      "Topic #9:\n",
      "['control', 'state', 'task', 'controller', 'position', 'training', 'robot', 'prediction', 'dynamic', 'motor', 'trajectory', 'movement', 'move', 'environment', 'goal', 'step', 'adaptive', 'search', 'arm', 'region']\n",
      "\n",
      "Topic #10:\n",
      "['state', 'action', 'policy', 'step', 'optimal', 'reinforcement_learning', 'control', 'probability', 'rate', 'reward', 'machine', 'agent', 'student', 'task', 'equation', 'transition', 'teacher', 'rl', 'training', 'convergence']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view topics as a list of terms without weights, understand context or theme of each topic\n",
    "print('LDA Topics without Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.4886609386278148\n",
      "Avg. Coherence Score (UMass): -1.0914622235131308\n",
      "Model Perplexity: -7.787589872271927\n"
     ]
    }
   ],
   "source": [
    "# use perplexity and coherence scores as measures to evaluate topic model\n",
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                      texts=norm_corpus_bigrams, \n",
    "                                                      dictionary=dictionary, coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                         texts=norm_corpus_bigrams, \n",
    "                                                         dictionary=dictionary, coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA Models with MALLET\n",
    "# download MALLET framework\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract contents from archive\n",
    "# !unzip -q mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['unit', 'layer', 'node', 'rule', 'hidden_unit', 'pattern', 'net', 'architecture', 'training', 'activation', 'representation', 'task', 'structure', 'recurrent', 'sequence', 'trained', 'module', 'connection', 'back_propagation', 'connectionist']\n",
      "\n",
      "Topic #2:\n",
      "['vector', 'class', 'bound', 'linear', 'theorem', 'matrix', 'size', 'defined', 'condition', 'approximation', 'theory', 'xi', 'complexity', 'threshold', 'constant', 'proof', 'assume', 'property', 'loss', 'polynomial']\n",
      "\n",
      "Topic #3:\n",
      "['state', 'control', 'action', 'step', 'trajectory', 'task', 'controller', 'environment', 'policy', 'optimal', 'path', 'transition', 'goal', 'dynamic', 'reinforcement_learning', 'search', 'trial', 'change', 'robot', 'learned']\n",
      "\n",
      "Topic #4:\n",
      "['response', 'cell', 'stimulus', 'visual', 'motion', 'signal', 'filter', 'direction', 'receptive_field', 'map', 'spatial', 'target', 'activity', 'eye', 'unit', 'field', 'orientation', 'subject', 'velocity', 'location']\n",
      "\n",
      "Topic #5:\n",
      "['equation', 'noise', 'solution', 'vector', 'rate', 'dynamic', 'gradient', 'eq', 'training', 'matrix', 'average', 'linear', 'convergence', 'curve', 'optimal', 'nonlinear', 'energy', 'correlation', 'minimum', 'theory']\n",
      "\n",
      "Topic #6:\n",
      "['distribution', 'probability', 'estimate', 'variable', 'gaussian', 'prediction', 'sample', 'prior', 'mixture', 'estimation', 'density', 'kernel', 'variance', 'approximation', 'bayesian', 'component', 'regression', 'log', 'likelihood', 'statistical']\n",
      "\n",
      "Topic #7:\n",
      "['image', 'feature', 'object', 'representation', 'pixel', 'local', 'region', 'distance', 'cluster', 'view', 'face', 'vector', 'transformation', 'surface', 'map', 'scale', 'structure', 'shape', 'part', 'location']\n",
      "\n",
      "Topic #8:\n",
      "['neuron', 'cell', 'pattern', 'synaptic', 'activity', 'spike', 'connection', 'neural', 'firing', 'dynamic', 'response', 'synapsis', 'delay', 'frequency', 'effect', 'threshold', 'et_al', 'simulation', 'type', 'fig']\n",
      "\n",
      "Topic #9:\n",
      "['circuit', 'chip', 'signal', 'current', 'bit', 'analog', 'memory', 'code', 'voltage', 'implementation', 'channel', 'design', 'neural', 'parallel', 'operation', 'processor', 'computation', 'element', 'source', 'application']\n",
      "\n",
      "Topic #10:\n",
      "['training', 'classification', 'word', 'class', 'classifier', 'feature', 'recognition', 'test', 'trained', 'training_set', 'pattern', 'experiment', 'speech', 'table', 'character', 'accuracy', 'hmm', 'test_set', 'vector', 'task']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MALLET_PATH = path_to_users + '/mallet-2.0.8/bin/mallet'\n",
    "lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus,\n",
    "                                              num_topics=TOTAL_TOPICS, id2word=dictionary, \n",
    "                                              iterations=500, workers=16)\n",
    "\n",
    "topics=[[(term, round(wt,3)) \n",
    "         for term, wt in lda_mallet.show_topic(n, topn=20)]\n",
    "             for n in range(0, TOTAL_TOPICS)]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.5240900426563829\n",
      "Avg. Coherence Score (UMass): -1.0462128724374544\n",
      "Model Perplexity: -8.53533\n"
     ]
    }
   ],
   "source": [
    "# evaluate model using perplexity and coherence metrics\n",
    "cv_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, corpus=bow_corpus, \n",
    "                                                             texts=norm_corpus_bigrams, \n",
    "                                                             dictionary=dictionary, \n",
    "                                                             coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda_mallet.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, \n",
    "                                                                corpus=bow_corpus, \n",
    "                                                                texts=norm_corpus_bigrams, \n",
    "                                                                dictionary=dictionary, \n",
    "                                                                coherence='u_mass')\n",
    "\n",
    "avg_coherence_umass = umass_coherence_model_lda_mallet.get_coherence()\n",
    "\n",
    "# from STDOUT: <500> LL/token: -8.53533\n",
    "perplexity = -8.53533\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA Tuning: Finding the Optimal Number of Topics\n",
    "from tqdm import tqdm\n",
    "\n",
    "# iterate and build several models with differing number of topics\n",
    "# select one that has highest coherence score\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary, start_topic_count=2, \n",
    "                                    end_topic_count=10, step=1, cpus=1):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, \n",
    "                                                            corpus=corpus, num_topics=topic_nums, \n",
    "                                                            id2word=dictionary, iterations=500, \n",
    "                                                            workers=cpus)\n",
    "        cv_coherence_model_mallet_lda = gensim.models.CoherenceModel(model=mallet_lda_model, \n",
    "                                                                     corpus=corpus, texts=texts, \n",
    "                                                                     dictionary=dictionary, \n",
    "                                                                     coherence='c_v')\n",
    "        coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(mallet_lda_model)\n",
    "    \n",
    "    return models, coherence_scores\n",
    "\n",
    "#lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, \n",
    "#                                                               texts=norm_corpus_bigrams, \n",
    "#                                                               dictionary=dictionary, \n",
    "#                                                               start_topic_count=2, \n",
    "#                                                               end_topic_count=30, step=1, \n",
    "#                                                               cpus=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#import numpy\n",
    "\n",
    "# save model for later use\n",
    "#filename = 'lda_models.sav'\n",
    "#pickle.dump(lda_models, open(filename, 'wb'))\n",
    "\n",
    "# save coherence scores\n",
    "#np.savetxt(\"coherence_scores.csv\", coherence_scores, delimiter=\",\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/lda_models.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-1e259b2ed08d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models/lda_models.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlda_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcoherence_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/coherence_scores.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/lda_models.sav'"
     ]
    }
   ],
   "source": [
    "# load model and scores\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "filename = 'models/lda_models.sav'\n",
    "lda_models = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "coherence_scores = np.genfromtxt('data/coherence_scores.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 31, 1), \n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph showing number of topics per model and corresponding coherence scores\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(2,31,1)\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x_ax, y_ax,c='r')\n",
    "plt.axhline(y=0.535, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "x1 = plt.xlabel('Number of Topics')\n",
    "y1 = plt.ylabel('Coherence Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on graph, choose optimal number of topics as 20\n",
    "# retrieve best model\n",
    "best_model_idx = coherence_df[coherence_df['Number of Topics'] == 20].index[0]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all the 20 topics generated by selected best model\n",
    "topics = [[(term, round(wt, 3)) \n",
    "           for term, wt in best_lda_model.show_topic(n, topn=20)] \n",
    "              for n in range(0, best_lda_model.num_topics)]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build term topic dataframe\n",
    "topics_df = pd.DataFrame([[term for term, wt in topic] \n",
    "                              for topic in topics], \n",
    "                         columns=['Term'+str(i) for i in range(1,21)], \n",
    "                         index=['Topic '+str(t) for t in range(1, best_lda_model.num_topics+1)]).T\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic term dataframe: each topic represented in a row with terms of topic\n",
    "# represented as comma-separated string\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic]) \n",
    "                              for topic in topics], \n",
    "                         columns=['Terms per Topic'], \n",
    "                         index=['Topic'+str(t) for t in range (1, best_lda_model.num_topics+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpreting Topic Model Results\n",
    "tm_results = best_lda_model[bow_corpus]\n",
    "\n",
    "# get most dominant topic per research paper\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] for topics in tm_results]\n",
    "corpus_topics[:5]\n",
    "\n",
    "# construct master dataframe that holds base statistics\n",
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(papers))\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "corpus_topic_df['Paper'] = papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dominant Topics Distribution Across Corpus\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "topic_stats_df = corpus_topic_df.groupby('Dominant Topic').agg(\n",
    "    {'Dominant Topic': {'Doc Count': np.size, '% Total Docs': np.size }})\n",
    "\n",
    "topic_stats_df = topic_stats_df['Dominant Topic'].reset_index()\n",
    "topic_stats_df['% Total Docs'] = topic_stats_df['% Total Docs'].\n",
    "apply(lambda row: round((row*100) / len(papers), 2))\n",
    "topic_stats_df['Topic Desc'] = [topics_df.iloc[t]['Terms per Topic'] \n",
    "    for t in range(len(topic_stats_df))]\n",
    "topic_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dominant Topics in Specific Research Papers\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "(corpus_topic_df[corpus_topic_df['Document'].\n",
    "    isin([681, 9, 392, 1622, 17, 906, 996, 503, 13, 733])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relevant Research Papers per Topic Based on Dominance\n",
    "corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set:\n",
    "    (topic_set.sort_values(by=['Contribution %'], ascending=False).iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting Topics for New Research Papers\n",
    "import glob\n",
    "# papers manually downloaded from NIPS 16\n",
    "# https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016\n",
    "\n",
    "new_paper_files = glob.glob('nips16*.txt')\n",
    "new_papers = []\n",
    "for fn in new_paper_files:\n",
    "    with open(fn, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "        new_papers.append(data)\n",
    "print('Total New Papers', len(new_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build text wrangling and feature engineering pipeline\n",
    "def text_preprocessing_pipeline(documents, normalizer_fn, bigram_model):\n",
    "    norm_docs = normalizer_fn(documents)\n",
    "    norm_docs_bigrams = bigram_model[norm_docs]\n",
    "    return norm_docs_bigrams\n",
    "\n",
    "def bow_features_pipeline(tokenized_docs, dictionary):\n",
    "    paper_bow_features = [dictionary.doc2bow(text) \n",
    "        for text in tokenized_docs]\n",
    "    return paper_bow_features\n",
    "\n",
    "norm_new_papers = text_preprocessing_pipeline(documents=new_papers, normalizer_fn=normalize_corpus, bigram_model=bigram_model)\n",
    "\n",
    "norm_bow_features = bow_features_pipeline(tokenized_docs=norm_new_papers, dictionary=dictionary)\n",
    "\n",
    "print(norm_new_papers[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_bow_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build generic fuction to extract top N topics from any research paper using trained model\n",
    "def get_topic_predictions(topic_model, corpus, topn=3):\n",
    "    topic_predictions = topic_model[corpus]\n",
    "    best_topics = [[(topic, round(wt,3)) \n",
    "                        for topic, wt in sorted(topic_predictions[i],\n",
    "                        key=lamda row: -row[1])[:topn]]\n",
    "                            for i in range(len(topic_predictions))]\n",
    "    return best_topics\n",
    "\n",
    "# putting the function in action\n",
    "topic_preds = get_topic_predictions(topic_model=best_lda_model, corpus=norm_bow_features, topn=2)\n",
    "topic_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review results for each paper\n",
    "results_df = pd.DataFrame()\n",
    "results_df['Papers'] = range(1, len(new_papers)+1)\n",
    "results_df['Dominant Topics'] = [[topic_num+1 for topic_num, wt in term] \n",
    "                                    for item in topic_preds]\n",
    "res = results_df.set_index(['Papers'])['Dominant Topics'].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "stack().reset_index(level=1, drop=True)\n",
    "results_df = pd.DataFrame({'Dominant Topics': res.values}, index=res.index)\n",
    "results_df['Contribution %'] = [topic_wt for topic_list in [[round(wt*100,2) for topic_num, wt in item] for item in topic_preds] for topic_wt in topic_list]\n",
    "results_df['Topic Desc'] = [topics_df.iloc[t-1]['Terms per Topic'] for t in results_df['Dominant Topics'].values]\n",
    "results_df['Paper Desc'] = [new_papers[i-1][:200] for i in results_df.index.values]\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Representation with Feature Engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2), token_pattern=None, tokenizer=lambda doc: doc, preprocessor=lambda doc: doc)\n",
    "cv_features = cv.fit_transform(norm_papers)\n",
    "cv_features.shape\n",
    "\n",
    "# validating vocabulary size\n",
    "vocabulary = np.array(cv.get_feature_names())\n",
    "print('Total Vocabulary Size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Latent Semantic Indexing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "TOTAL_TOPICS=20\n",
    "lsi_model = TruncatedSVD(n_components=TOTAL_TOPICS, n_iter=500, random_state=42)\n",
    "document_topics = lsi_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = lsi_model.components_\n",
    "topic_terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse previously implemented code to display topics and terms\n",
    "top_terms = 20\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterm_weights = np.array([topic_terms[row, columns] for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    terms, weights = topic_keyterms_weight[n]\n",
    "    term_weights = sorted([(t,w) for t, w in zip(terms, weights)],\n",
    "        key = lambda row: -abs(row[1]))\n",
    "    for term, wt in term_weights:\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt,3)))\n",
    "        else:\n",
    "            print('Direction 1:', d1)\n",
    "            print('-'*50)\n",
    "            print('Direction 2:', d2)\n",
    "            print('-'*50)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract key topics for specific research papers\n",
    "dt_df = pd.DataFrame(np.round(document_topics,3), \n",
    "            columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "\n",
    "document_numbers = [13, 250, 500]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(dt_df.columns[np.argsort(-np.absolute(dt_df.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50, learning_method='online', match_size=1740, learning_offset=50., random_state=42, n_jobs=16)\n",
    "document_topics = lda_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain topic-term matrix\n",
    "# build dataframe from it to showcase topics and terms\n",
    "topic_terms = lda_model.components_\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics, column=['Terms per Topic'], index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view research papers having max contribution of each of the 20 topics\n",
    "dt_df = pd.DataFrame(document_topics, columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "pd.options.display.float_format = '{:, .5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_contrib_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_contrib_topics.index\n",
    "contrib_perc = max_contrib_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0]\n",
    "                        for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
    "                            'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'],\n",
    "                            'Paper Name': documents})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Non-Negative Matrix Factorization\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=TOTAL_TOPICS, solver='cd', max_iter=500, random_state=42, alpha=.1, l1_ratio=.85)\n",
    "document_topics = nmf_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = nmf_model.components_\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:,:top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics, columns=['Terms per Topic'],\n",
    "                            index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine dominance of topics in research papers by absolute scores\n",
    "pd.options.display.float_format = '{:, .3f}'.format\n",
    "dt_df = pd.DataFrame(document_topics, columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine most relevant paper for each topic based on topic dominance scores\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_score_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_score_topics.index\n",
    "term_score = max_score_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_score_topics.loc[t]].index[0]\n",
    "                        for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Max Score': term_score,\n",
    "                            'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'],\n",
    "                            'Paper Name': documents})\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
