{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "* Data Retrieval\n",
    "* Data Preprocessing and Normalization\n",
    "* Building Train and Test Datasets\n",
    "* Feature Engineering Techniques\n",
    "    1. Traditional\n",
    "    2. Advanced\n",
    "* Classification Models\n",
    "    1. Multinomial Naive Bayes\n",
    "    2. Logistic Regression\n",
    "    3. Support Vector Machines\n",
    "    4. Ensemble Models\n",
    "    5. Random Forest\n",
    "    6. Gradient Boosting Machines\n",
    "* Evaluating Classification Models\n",
    "    1. Confusion Matrix\n",
    "* Building and Evaluating Our Text Classifier\n",
    "    1. Bag of Words Features with Classification Models\n",
    "    2. TF-IDF Features with Classification Models\n",
    "    3. Comparative Model Performance Evaluation\n",
    "    4. Word2Vec Embeddings with Classification Models\n",
    "    5. GloVe Embeddings with Classification Models\n",
    "    6. FastText Embeddings with Classification Models\n",
    "    7. Model Tuning\n",
    "    8. Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case spacy doesn't work, run:\n",
    "\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "data_labels_map = dict(enumerate(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nIf a Christian means someone who believes in...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Target Label  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...            10   \n",
       "1  My brother is in the market for a high-perform...             3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...            17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...             3   \n",
       "4  1)    I have an old Jasmine drive which I cann...             4   \n",
       "5  \\n\\nBack in high school I worked as a lab assi...            12   \n",
       "6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...             4   \n",
       "7  \\n[stuff deleted]\\n\\nOk, here's the solution t...            10   \n",
       "8  \\n\\n\\nYeah, it's the second one.  And I believ...            10   \n",
       "9  \\nIf a Christian means someone who believes in...            19   \n",
       "\n",
       "                Target Name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  \n",
       "5           sci.electronics  \n",
       "6     comp.sys.mac.hardware  \n",
       "7          rec.sport.hockey  \n",
       "8          rec.sport.hockey  \n",
       "9        talk.religion.misc  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the dataframe\n",
    "corpus, target_labels, target_names = (data.data, data.target, [data_labels_map[label] for label in data.target])\n",
    "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels, 'Target Name': target_names})\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty documents: 515\n"
     ]
    }
   ],
   "source": [
    "total_nulls = data_df[data_df.Article.str.strip() == ''].shape[0]\n",
    "print(\"Empty documents:\", total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18331, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df[~(data_df.Article.str.strip() == '')]\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>sure basher pens fan pretty confused lack kind...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>brother market high performance video card sup...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>finally say dream mediterranean new area great...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>think scsi card dma transfer not disk scsi car...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>old jasmine drive not use new system understan...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n",
       "      <td>back high school work lab assistant bunch expe...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n",
       "      <td>ae dallas try tech support may line one get start</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n",
       "      <td>stuff delete ok solution problem move canada y...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n",
       "      <td>yeah second one believe price try get good loo...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nIf a Christian means someone who believes in...</td>\n",
       "      <td>christian mean someone believe divinity jesus ...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1  My brother is in the market for a high-perform...   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4  1)    I have an old Jasmine drive which I cann...   \n",
       "5  \\n\\nBack in high school I worked as a lab assi...   \n",
       "6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...   \n",
       "7  \\n[stuff deleted]\\n\\nOk, here's the solution t...   \n",
       "8  \\n\\n\\nYeah, it's the second one.  And I believ...   \n",
       "9  \\nIf a Christian means someone who believes in...   \n",
       "\n",
       "                                       Clean Article  Target Label  \\\n",
       "0  sure basher pens fan pretty confused lack kind...            10   \n",
       "1  brother market high performance video card sup...             3   \n",
       "2  finally say dream mediterranean new area great...            17   \n",
       "3  think scsi card dma transfer not disk scsi car...             3   \n",
       "4  old jasmine drive not use new system understan...             4   \n",
       "5  back high school work lab assistant bunch expe...            12   \n",
       "6  ae dallas try tech support may line one get start             4   \n",
       "7  stuff delete ok solution problem move canada y...            10   \n",
       "8  yeah second one believe price try get good loo...            10   \n",
       "9  christian mean someone believe divinity jesus ...            19   \n",
       "\n",
       "                Target Name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  \n",
       "5           sci.electronics  \n",
       "6     comp.sys.mac.hardware  \n",
       "7          rec.sport.hockey  \n",
       "8          rec.sport.hockey  \n",
       "9        talk.religion.misc  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# just to keep negation if any in bi-grams\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "# normalize our corpus\n",
    "norm_corpus = tn.normalize_corpus(corpus=data_df['Article'], html_stripping=True, contraction_expansion=True, \n",
    "                                  accented_char_removal=True, text_lower_case=True, text_lemmatization=True, \n",
    "                                  text_stemming=False, special_char_removal=True, remove_digits=True, \n",
    "                                  stopword_removal=True, stopwords=stopword_list)\n",
    "\n",
    "data_df['Clean Article'] = norm_corpus\n",
    "\n",
    "# view sample data\n",
    "data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18331 entries, 0 to 18845\n",
      "Data columns (total 4 columns):\n",
      "Article          18331 non-null object\n",
      "Clean Article    18300 non-null object\n",
      "Target Label     18331 non-null int64\n",
      "Target Name      18331 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 716.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df['Clean Article'] = norm_corpus\n",
    "data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18300 entries, 0 to 18299\n",
      "Data columns (total 4 columns):\n",
      "Article          18300 non-null object\n",
      "Clean Article    18300 non-null object\n",
      "Target Label     18300 non-null int64\n",
      "Target Name      18300 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 572.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df.dropna().reset_index(drop=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('clean_newsgroups.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('clean_newsgroups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12261,), (6039,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names =\\\n",
    "                                 train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "                                                  np.array(data_df['Target Name']), test_size=0.33, random_state=42)\n",
    "\n",
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Train Count</th>\n",
       "      <th>Test Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>667</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>662</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>660</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>654</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>653</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>651</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>649</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sci.med</td>\n",
       "      <td>648</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>648</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>647</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>646</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>642</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>640</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>612</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>606</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rec.autos</td>\n",
       "      <td>590</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>571</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>512</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>499</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>404</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Target Label  Train Count  Test Count\n",
       "15                 sci.crypt          667         295\n",
       "0     soc.religion.christian          662         312\n",
       "5            rec.motorcycles          660         309\n",
       "10  comp.sys.ibm.pc.hardware          654         309\n",
       "8             comp.windows.x          653         327\n",
       "11          rec.sport.hockey          651         322\n",
       "19                 sci.space          649         304\n",
       "7                    sci.med          648         312\n",
       "17        rec.sport.baseball          648         303\n",
       "4            sci.electronics          647         309\n",
       "2              comp.graphics          646         307\n",
       "1    comp.os.ms-windows.misc          642         304\n",
       "13              misc.forsale          640         319\n",
       "3      comp.sys.mac.hardware          612         315\n",
       "18     talk.politics.mideast          606         311\n",
       "16                 rec.autos          590         343\n",
       "14        talk.politics.guns          571         314\n",
       "9                alt.atheism          512         267\n",
       "12        talk.politics.misc          499         256\n",
       "6         talk.religion.misc          404         201"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "trd = dict(Counter(train_label_names))\n",
    "tsd = dict(Counter(test_label_names))\n",
    "\n",
    "(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd], \n",
    "             columns=['Target Label', 'Train Count', 'Test Count'])\n",
    ".sort_values(by=['Train Count', 'Test Count'],\n",
    "             ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models\n",
    "have trouble getting section of code to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Evaluating Our Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (12261, 65914)  Test features shape: (6039, 65914)\n"
     ]
    }
   ],
   "source": [
    "# referencing above code in more succinct way to solely access this information\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# download dataset\n",
    "data_df = pd.read_csv('clean_newsgroups.csv')\n",
    "\n",
    "# split data\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names =\\\n",
    "                                 train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "                                                  np.array(data_df['Target Name']), test_size=0.33, random_state=42)\n",
    "\n",
    "## Bag of Words Features with Classification Models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# build BOW features on train articles\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_train_features = cv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "cv_test_features = cv.transform(test_corpus)\n",
    "\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.68590004 0.67887668 0.68665851 0.68504902 0.67184943]\n",
      "Mean CV Accuracy: 0.6816667346037866\n",
      "Test Accuracy: 0.6816667346037866\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_train_features, train_label_names)\n",
    "mnb_bow_cv_scores = cross_val_score(mnb, cv_train_features, train_label_names, cv=5)\n",
    "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
    "mnb_bow_test_score = mnb.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_bow_cv_mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=42)\n",
    "lr.fit(cv_train_features, train_label_names)\n",
    "lr_bow_cv_scores = cross_val_score(lr, cv_train_features, train_label_names, cv=5)\n",
    "lr_bow_cv_mean_score = np.mean(lr_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_bow_cv_mean_score)\n",
    "lr_bow_test_score = lr.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_bow_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.63388866 0.64102564 0.64422685 0.64910131 0.64729951]\n",
      "Mean CV Accuracy: 0.6431083933094228\n",
      "Test Accuracy: 0.6522603079980129\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(cv_train_features, train_label_names)\n",
    "svm_bow_cv_scores = cross_val_score(svm, cv_train_features, train_label_names, cv=5)\n",
    "svm_bow_cv_mean_score = np.mean(svm_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_bow_cv_mean_score)\n",
    "svm_bow_test_score = svm.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_bow_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.63185697 0.62596663 0.61321909 0.64787582 0.64729951]\n",
      "Mean CV Accuracy: 0.6332436029841757\n",
      "Test Accuracy: 0.6511011756913396\n"
     ]
    }
   ],
   "source": [
    "# SVM with Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
    "svm_sgd.fit(cv_train_features, train_label_names)\n",
    "svmsgd_bow_cv_scores = cross_val_score(svm_sgd, cv_train_features, train_label_names, cv=5)\n",
    "svmsgd_bow_cv_mean_score = np.mean(svmsgd_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_bow_cv_mean_score)\n",
    "svmsgd_bow_test_score = svm_sgd.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_bow_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.52824055 0.50956451 0.53855569 0.52205882 0.51718494]\n",
      "Mean CV Accuracy: 0.5231209039972265\n",
      "Test Accuracy: 0.5418115582050008\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(cv_train_features, train_label_names)\n",
    "rfc_bow_cv_scores = cross_val_score(rfc, cv_train_features, train_label_names, cv=5)\n",
    "rfc_bow_cv_mean_score = np.mean(rfc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_bow_cv_mean_score)\n",
    "rfc_bow_test_score = rfc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_bow_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.55221455 0.55474155 0.5630355  0.5502451  0.54500818]\n",
      "Mean CV Accuracy: 0.5530489757470004\n",
      "Test Accuracy: 0.5553899652260308\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machines\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(cv_train_features, train_label_names)\n",
    "gbc_bow_cv_scores = cross_val_score(gbc, cv_train_features, train_label_names, cv=5)\n",
    "gbc_bow_cv_mean_score = np.mean(gbc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_bow_cv_mean_score)\n",
    "gbc_bow_test_score = gbc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_bow_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF mode:> Train features shape: (12261, 65914)  Test features shape: (6039, 65914)\n"
     ]
    }
   ],
   "source": [
    "## TF-IDF Features with Classification Models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# build BOW features on train articles \n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "tv_test_features = tv.transform(test_corpus)\n",
    "\n",
    "print('TFIDF mode:> Train features shape:', tv_train_features.shape, \n",
    "      ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.70337261 0.7049247  0.71113831 0.69812092 0.71890344]\n",
      "Mean CV Accuracy: 0.7072919961196964\n",
      "Test Accuracy: 0.7072362974002319\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(tv_train_features, train_label_names)\n",
    "mnb_tfidf_cv_scores = cross_val_score(mnb, tv_train_features, train_label_names, cv=5)\n",
    "mnb_tfidf_cv_mean_score = np.mean(mnb_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_tfidf_cv_mean_score)\n",
    "mnb_tfidf_test_score = mnb.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tv_train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6e7c5815024d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Logistic Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtv_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlr_tfidf_cv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtv_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlr_tfidf_cv_mean_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_tfidf_cv_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tv_train_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=42)\n",
    "lr.fit(tv_train_features, train_label_names)\n",
    "lr_tfidf_cv_scores = cross_val_score(lr, tv_train_features, train_label_names, cv=5)\n",
    "lr_tfidf_cv_mean_score = np.mean(lr_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_tfidf_cv_mean_score)\n",
    "lr_tfidf_test_score = lr.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.75375863 0.75539276 0.76336189 0.75531046 0.75327332]\n",
      "Mean CV Accuracy: 0.75621941262751\n",
      "Test Accuracy: 0.7597284318595794\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, train_label_names)\n",
    "svm_tfidf_cv_scores = cross_val_score(svm, tv_train_features, train_label_names, cv=5)\n",
    "svm_tfidf_cv_mean_score = np.mean(svm_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_tfidf_cv_mean_score)\n",
    "svm_tfidf_test_score = svm.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.75457131 0.75376475 0.75968992 0.75245098 0.75900164]\n",
      "Mean CV Accuracy: 0.7558957211546691\n",
      "Test Accuracy: 0.7595628415300546\n"
     ]
    }
   ],
   "source": [
    "# SVM with Stochastic Gradient Descent\n",
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
    "svm_sgd.fit(tv_train_features, train_label_names)\n",
    "svmsgd_tfidf_cv_scores = cross_val_score(svm_sgd, tv_train_features, train_label_names, cv=5)\n",
    "svmsgd_tfidf_cv_mean_score = np.mean(svmsgd_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_tfidf_cv_mean_score)\n",
    "svmsgd_tfidf_test_score = svm_sgd.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.53311662 0.51322751 0.53529172 0.53390523 0.51841244]\n",
      "Mean CV Accuracy: 0.526790703507522\n",
      "Test Accuracy: 0.5447921841364465\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(tv_train_features, train_label_names)\n",
    "rfc_tfidf_cv_scores = cross_val_score(rfc, tv_train_features, train_label_names, cv=5)\n",
    "rfc_tfidf_cv_mean_score = np.mean(rfc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_tfidf_cv_mean_score)\n",
    "rfc_tfidf_test_score = rfc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.55302722 0.56654457 0.55161159 0.54575163 0.5413257 ]\n",
      "Mean CV Accuracy: 0.5516521415850433\n",
      "Test Accuracy: 0.5534028812717338\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(tv_train_features, train_label_names)\n",
    "gbc_tfidf_cv_scores = cross_val_score(gbc, tv_train_features, train_label_names, cv=5)\n",
    "gbc_tfidf_cv_mean_score = np.mean(gbc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_tfidf_cv_mean_score)\n",
    "gbc_tfidf_test_score = gbc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>Linear SVM (SGD)</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Gradient Boosted Machines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV Score (TF)</th>\n",
       "      <td>0.681667</td>\n",
       "      <td>0.700353</td>\n",
       "      <td>0.643108</td>\n",
       "      <td>0.633244</td>\n",
       "      <td>0.523121</td>\n",
       "      <td>0.553049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (TF)</th>\n",
       "      <td>0.690677</td>\n",
       "      <td>0.701275</td>\n",
       "      <td>0.65226</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>0.541812</td>\n",
       "      <td>0.55539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV Score (TF-IDF)</th>\n",
       "      <td>0.707292</td>\n",
       "      <td>0.742113</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.755896</td>\n",
       "      <td>0.526791</td>\n",
       "      <td>0.551652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (TF-IDF)</th>\n",
       "      <td>0.707236</td>\n",
       "      <td>0.738202</td>\n",
       "      <td>0.759728</td>\n",
       "      <td>0.759563</td>\n",
       "      <td>0.544792</td>\n",
       "      <td>0.553403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0                    1           2  \\\n",
       "Model                Naive Bayes  Logistic Regression  Linear SVM   \n",
       "CV Score (TF)           0.681667             0.700353    0.643108   \n",
       "Test Score (TF)         0.690677             0.701275     0.65226   \n",
       "CV Score (TF-IDF)       0.707292             0.742113    0.756219   \n",
       "Test Score (TF-IDF)     0.707236             0.738202    0.759728   \n",
       "\n",
       "                                    3              4  \\\n",
       "Model                Linear SVM (SGD)  Random Forest   \n",
       "CV Score (TF)                0.633244       0.523121   \n",
       "Test Score (TF)              0.651101       0.541812   \n",
       "CV Score (TF-IDF)            0.755896       0.526791   \n",
       "Test Score (TF-IDF)          0.759563       0.544792   \n",
       "\n",
       "                                             5  \n",
       "Model                Gradient Boosted Machines  \n",
       "CV Score (TF)                         0.553049  \n",
       "Test Score (TF)                        0.55539  \n",
       "CV Score (TF-IDF)                     0.551652  \n",
       "Test Score (TF-IDF)                   0.553403  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Comparative Model Performance Evaluation\n",
    "pd.DataFrame([['Naive Bayes', mnb_bow_cv_mean_score, mnb_bow_test_score, \n",
    "               mnb_tfidf_cv_mean_score, mnb_tfidf_test_score],\n",
    "              ['Logistic Regression', lr_bow_cv_mean_score, lr_bow_test_score, \n",
    "               lr_tfidf_cv_mean_score, lr_tfidf_test_score],\n",
    "              ['Linear SVM', svm_bow_cv_mean_score, svm_bow_test_score, \n",
    "               svm_tfidf_cv_mean_score, svm_tfidf_test_score],\n",
    "              ['Linear SVM (SGD)', svmsgd_bow_cv_mean_score, svmsgd_bow_test_score, \n",
    "               svmsgd_tfidf_cv_mean_score, svmsgd_tfidf_test_score],\n",
    "              ['Random Forest', rfc_bow_cv_mean_score, rfc_bow_test_score, \n",
    "               rfc_tfidf_cv_mean_score, rfc_tfidf_test_score],\n",
    "              ['Gradient Boosted Machines', gbc_bow_cv_mean_score, gbc_bow_test_score, \n",
    "               gbc_tfidf_cv_mean_score, gbc_tfidf_test_score]],\n",
    "             columns=['Model', 'CV Score (TF)', 'Test Score (TF)', 'CV Score (TF-IDF)', 'Test Score (TF-IDF)'],\n",
    "             ).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2Vec Embeddings with Classification Models\n",
    "def document_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        \n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "            \n",
    "        return feature_vector\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, \n",
    "                                     num_features) for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize corpus\n",
    "tokenized_train = [tn.tokenizer.tokenize(text)\n",
    "                   for text in train_corpus]\n",
    "tokenized_test = [tn.tokenizer.tokenize(text)\n",
    "                  for text in test_corpus]\n",
    "\n",
    "# generate word2vec word embeddings\n",
    "import gensim\n",
    "# build word2vec word embeddings\n",
    "w2v_num_features = 1000\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features,\n",
    "                                   window=100, min_count=2, sample=1e-3, sg=1, \n",
    "                                   iter=5, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (12261, 1000)  Test features shape: (6039, 1000)\n"
     ]
    }
   ],
   "source": [
    "# generate document level embeddings\n",
    "# remember we only use train dataset vocabulary embeddings\n",
    "# so that test dataset truly remains an unseen dataset\n",
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = document_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                            num_features=w2v_num_features)\n",
    "avg_wv_test_features = document_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                           num_features=w2v_num_features)\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape,\n",
    "      ' Test features shape:', avg_wv_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.74766355 0.74928775 0.74745002 0.74550654 0.75      ]\n",
      "Mean CV Accuracy: 0.7479815714074336\n",
      "Test Accuracy: 0.7348898824308661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "svm.fit(avg_wv_train_features, train_label_names)\n",
    "svm_w2v_cv_scores = cross_val_score(svm, avg_wv_train_features, train_label_names, cv=5)\n",
    "svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
    "svm_w2v_test_score = svm.score(avg_wv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_w2v_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GloVe Embeddings with Classification Models\n",
    "# feature engineering with GloVe model\n",
    "#train_nlp = [tn.nlp(item) for item in train_corpus]\n",
    "#train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "#test_nlp = [tn.nlp(item) for item in test_corpus]\n",
    "#test_glove_features = np.array([item.vector for item in test_nlp])\n",
    "\n",
    "#print('GloVe model:> Train features shape:', train_glove_features.shape,\n",
    "#      ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building our SVM model\n",
    "#svm = SDGClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "#svm.fit(train_glove_features, train_label_names)\n",
    "#svm_glove_cv_scores = np.mean(svm_glove_cv_scores)\n",
    "#print('CV Accuracy (5-fold):', svm_glove_cv_scores)\n",
    "#print('Mean CV Accuracy:', svm_glove_cv_mean_score)\n",
    "#svm_glove_test_score = svm.score(test_glove_features, test_label_names)\n",
    "#print('Test Accuracy:', svm_glove_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastText Embeddings with Classification Models\n",
    "#from gensim.models.fasttext import FastText\n",
    "\n",
    "#ft_num_features = 1000\n",
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "#ft_model = FastText(tokenized_train, size=ft_num_features, window=100,\n",
    "#                    min_count=2, sample=1e-3, sg=1, iter=5, workers=10)\n",
    "\n",
    "# generate averaged word vector features from word2vec model\n",
    "#avg_ft_train_features = document_vectorizer(corpus=tokenized_train, model=ft_model,\n",
    "#                                            num_features=ft_num_features)\n",
    "#avg_ft_test_features = document_vectorizer(corpus=tokenized_test, model=ft_model,\n",
    "#                                           num_features=ft_num_features)\n",
    "\n",
    "#print('FastText model:> Train features shape:', avg_ft_train_features.shape,\n",
    "#      ' Test features shape:', avg_ft_test_features.shape)\n",
    "\n",
    "# build SVM model\n",
    "#svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "#svm.fit(avg_ft_train_features, train_label_names)\n",
    "#svm_fit_cv_scores = cross_val_score(svm, avg_ft_train_features, train_label_names, cv=5)\n",
    "#svm_ft_cv_mean_score = np.mean(svm_ft_cv_scores)\n",
    "#print('CV Accuracy (5-fold):', svm_ft_cv_scores)\n",
    "#print('Mean CV Accuracy:', svm_ft_cv_mean_score)\n",
    "#svm_ft_test_score = svm.score(avg_ft_test_features, test_label_names)\n",
    "#print('Test Accuracy:', svm_ft_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#mlp = MLPClassifier(solver='adam', alpha=1e-5, learning_rate='adaptive', early_stopping=True,\n",
    "#                    activation='relu', hidden_layers_sizes=(512, 512), random_state=42)\n",
    "#mlp.fit(avg_ft_train_features, train_label_names)\n",
    "\n",
    "#svm_ft_test_scores = mlp.score(avg_ft_test_features, test_label_names)\n",
    "#print('Test Accuracy:', svm_ft_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   6.2s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   7.4s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  4.2min finished\n"
     ]
    }
   ],
   "source": [
    "## Model Tuning\n",
    "# Tuning our Multinomial Naive Bayes model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "mnb_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('mnb', MultinomialNB())\n",
    "                         ])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "              'mnb__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]}\n",
    "\n",
    "gs_mnb = GridSearchCV(mnb_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_mnb = gs_mnb.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None)),\n",
       "  ('mnb', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
       " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'mnb': MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 1),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'mnb__alpha': 0.01,\n",
       " 'mnb__class_prior': None,\n",
       " 'mnb__fit_prior': True}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>params</th>\n",
       "      <th>cv score (mean)</th>\n",
       "      <th>cv score (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>{'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.770410</td>\n",
       "      <td>0.007948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>{'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.770247</td>\n",
       "      <td>0.009625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>{'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.757279</td>\n",
       "      <td>0.006842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>{'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.752059</td>\n",
       "      <td>0.007080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>{'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.750510</td>\n",
       "      <td>0.012631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>{'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.742517</td>\n",
       "      <td>0.011306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>{'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.742191</td>\n",
       "      <td>0.009478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>{'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.729141</td>\n",
       "      <td>0.008456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>{'mnb__alpha': 1, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.708914</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>{'mnb__alpha': 1, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.699943</td>\n",
       "      <td>0.005237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                                                params  \\\n",
       "4     1    {'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 1)}   \n",
       "5     2    {'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 2)}   \n",
       "6     3     {'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 1)}   \n",
       "7     4     {'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 2)}   \n",
       "3     5  {'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 2)}   \n",
       "1     6   {'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 2)}   \n",
       "2     7  {'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 1)}   \n",
       "0     8   {'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 1)}   \n",
       "8     9       {'mnb__alpha': 1, 'tfidf__ngram_range': (1, 1)}   \n",
       "9    10       {'mnb__alpha': 1, 'tfidf__ngram_range': (1, 2)}   \n",
       "\n",
       "   cv score (mean)  cv score (std)  \n",
       "4         0.770410        0.007948  \n",
       "5         0.770247        0.009625  \n",
       "6         0.757279        0.006842  \n",
       "7         0.752059        0.007080  \n",
       "3         0.750510        0.012631  \n",
       "1         0.742517        0.011306  \n",
       "2         0.742191        0.009478  \n",
       "0         0.729141        0.008456  \n",
       "8         0.708914        0.006629  \n",
       "9         0.699943        0.005237  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model performances across different hyperparameter values in the hyperparameter space\n",
    "cv_results = gs_mnb.cv_results_\n",
    "results_df = pd.DataFrame({'rank': cv_results['rank_test_score'], 'params': cv_results['params'],\n",
    "                           'cv score (mean)': cv_results['mean_test_score'],\n",
    "                           'cv score (std)': cv_results['std_test_score']})\n",
    "results_df = results_df.sort_values(by=['rank'], ascending=True)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7736380195396589\n"
     ]
    }
   ],
   "source": [
    "best_mnb_test_score = gs_mnb.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy:', best_mnb_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   3.3s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   3.2s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   3.1s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   3.2s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   3.1s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  21.4s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  21.4s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  21.8s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  20.5s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  20.2s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   3.9s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   3.8s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   3.7s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   3.8s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   3.7s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  25.3s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  24.5s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  24.7s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  25.1s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  24.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   4.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   4.1s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   4.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   4.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   4.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  28.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  27.4s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  27.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  27.0s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  27.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  7.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Tuning our Logistic Regression model\n",
    "lr_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('lr', LogisticRegression(penalty='l2', max_iter=100, random_state=42))])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1,1), (1,2)], 'lr__C': [1,5,10]}\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_lr = gs_lr.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.763371419109124\n"
     ]
    }
   ],
   "source": [
    "# evaluate best tuned model on the test dataset\n",
    "best_lr_test_score = gs_lr.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy:', best_lr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   7.0s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   7.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   7.0s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   6.7s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   6.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   7.0s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   7.2s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   7.2s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   7.2s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   7.1s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 1), total=   1.9s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 1), total=   1.9s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 1), total=   2.0s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 1), total=   1.9s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 1), total=   1.9s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 2), total=   9.3s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 2), total=   9.1s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 2), total=   9.1s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 2), total=   9.1s\n",
      "[CV] svm__C=1.5, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=1.5, tfidf__ngram_range=(1, 2), total=   9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.7751283325053817\n"
     ]
    }
   ],
   "source": [
    "# Tuning the Linear SVM model\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('svm', LinearSVC(random_state=42))])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1,1), (1,2)], 'svm__C': [0.01, 0.1, 1.5]}\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_svm = gs_svm.fit(train_corpus, train_label_names)\n",
    "\n",
    "# evaluating best tuned model on the test dataset\n",
    "best_svm_test_score = gs_svm.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy :', best_svm_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7736\n",
      "Precision: 0.7784\n",
      "Recall: 0.7736\n",
      "F1 Score: 0.7714\n"
     ]
    }
   ],
   "source": [
    "## Model Performance Evaluation\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "mnb_predictions = gs_mnb.predict(test_corpus)\n",
    "unique_classes = list(set(test_label_names))\n",
    "meu.get_metrics(true_labels=test_label_names, predicted_labels=mnb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "   comp.sys.mac.hardware       0.81      0.74      0.77       315\n",
      "           comp.graphics       0.67      0.74      0.70       307\n",
      "               rec.autos       0.85      0.77      0.81       343\n",
      "             alt.atheism       0.68      0.65      0.66       267\n",
      "comp.sys.ibm.pc.hardware       0.63      0.76      0.69       309\n",
      "        rec.sport.hockey       0.93      0.92      0.92       322\n",
      "          comp.windows.x       0.86      0.80      0.83       327\n",
      "         rec.motorcycles       0.77      0.77      0.77       309\n",
      "      rec.sport.baseball       0.92      0.90      0.91       303\n",
      "                 sci.med       0.88      0.88      0.88       312\n",
      "            misc.forsale       0.80      0.69      0.74       319\n",
      "               sci.crypt       0.76      0.85      0.80       295\n",
      "      talk.religion.misc       0.77      0.34      0.47       201\n",
      "  soc.religion.christian       0.71      0.88      0.78       312\n",
      "               sci.space       0.82      0.85      0.83       304\n",
      " comp.os.ms-windows.misc       0.72      0.70      0.71       304\n",
      "         sci.electronics       0.71      0.72      0.71       309\n",
      "      talk.politics.guns       0.72      0.82      0.76       314\n",
      "   talk.politics.mideast       0.85      0.86      0.86       311\n",
      "      talk.politics.misc       0.65      0.67      0.66       256\n",
      "\n",
      "               micro avg       0.77      0.77      0.77      6039\n",
      "               macro avg       0.78      0.76      0.76      6039\n",
      "            weighted avg       0.78      0.77      0.77      6039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meu.display_classification_report(true_labels=test_label_names,\n",
    "                                  predicted_labels=mnb_predictions,\n",
    "                                  classes=unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Name</th>\n",
       "      <th>Label Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rec.autos</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sci.med</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Label Name  Label Number\n",
       "0                alt.atheism             0\n",
       "1              comp.graphics             1\n",
       "2    comp.os.ms-windows.misc             2\n",
       "3   comp.sys.ibm.pc.hardware             3\n",
       "4      comp.sys.mac.hardware             4\n",
       "5             comp.windows.x             5\n",
       "6               misc.forsale             6\n",
       "7                  rec.autos             7\n",
       "8            rec.motorcycles             8\n",
       "9         rec.sport.baseball             9\n",
       "10          rec.sport.hockey            10\n",
       "11                 sci.crypt            11\n",
       "12           sci.electronics            12\n",
       "13                   sci.med            13\n",
       "14                 sci.space            14\n",
       "15    soc.religion.christian            15\n",
       "16        talk.politics.guns            16\n",
       "17     talk.politics.mideast            17\n",
       "18        talk.politics.misc            18\n",
       "19        talk.religion.misc            19"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data_map = {v:k for k, v in data_labels_map.items()}\n",
    "label_map_df = pd.DataFrame(list(label_data_map.items()),\n",
    "                            columns=['Label Name', 'Label Number'])\n",
    "label_map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"20\" halign=\"left\">Predicted:</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">Actual:</th>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>226</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>212</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>236</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>233</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>265</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>272</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>221</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>257</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>275</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>267</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predicted:                                                         \\\n",
       "                   0    1    2    3    4    5    6    7    8    9    10   11   \n",
       "Actual: 0         174    2    0    2    0    1    0    2    4    0    3    3   \n",
       "        1           2  226   15   13    8   13    3    0    1    3    1    8   \n",
       "        2           0   15  212   39    7   13    4    0    0    0    0    3   \n",
       "        3           0   10   26  236    8    3    9    0    0    0    0    2   \n",
       "        4           0   11    9   25  233    2   10    2    1    0    0    7   \n",
       "        5           0   32   12    6    3  262    1    0    2    0    1    2   \n",
       "        6           0    4    7   27   13    2  219   12    3    1    1    8   \n",
       "        7           0    0    2    3    4    1    6  265   27    1    2    1   \n",
       "        8           1    0    0    1    2    1    5   17  239    2    4    3   \n",
       "        9           2    2    1    1    0    1    2    0    3  272    9    1   \n",
       "        10          2    2    1    2    0    0    0    0    1   10  296    0   \n",
       "        11          2    3    1    1    1    1    1    0    3    0    0  250   \n",
       "        12          1   11    4   17    7    0   10    4    3    2    0   12   \n",
       "        13          4    4    1    0    0    0    1    1    5    1    0    2   \n",
       "        14          7   10    2    0    0    1    2    2    2    0    1    3   \n",
       "        15         13    1    0    1    1    1    0    0    0    0    0    2   \n",
       "        16          1    1    1    0    0    0    1    1    2    1    1    9   \n",
       "        17          7    0    0    1    0    0    0    2    4    0    1    7   \n",
       "        18          6    2    0    0    0    0    0    1    5    0    0    7   \n",
       "        19         35    1    0    0    0    1    0    1    4    2    0    1   \n",
       "\n",
       "                                                   \n",
       "             12   13   14   15   16   17   18  19  \n",
       "Actual: 0     1    1    2   28    8   12   15   9  \n",
       "        1     7    3    3    0    1    0    0   0  \n",
       "        2     5    0    3    0    0    1    2   0  \n",
       "        3    15    0    0    0    0    0    0   0  \n",
       "        4    13    1    1    0    0    0    0   0  \n",
       "        5     3    0    1    0    1    1    0   0  \n",
       "        6    13    1    4    1    2    0    1   0  \n",
       "        7    10    2    4    2    5    1    7   0  \n",
       "        8     4    4    2    6    8    2    7   1  \n",
       "        9     1    0    0    3    0    3    2   0  \n",
       "        10    0    1    1    1    0    1    4   0  \n",
       "        11    6    0    3    2   11    4    5   1  \n",
       "        12  221    5   10    0    2    0    0   0  \n",
       "        13    3  273    8    3    2    1    3   0  \n",
       "        14    6    3  257    1    3    2    2   0  \n",
       "        15    2    3    1  275    4    1    3   4  \n",
       "        16    1    1    3    5  256    5   22   3  \n",
       "        17    1    0    1    3    6  267   11   0  \n",
       "        18    1   10    8    3   32    8  171   2  \n",
       "        19    0    2    3   57   15    4    7  68  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix\n",
    "unique_class_nums = label_map_df['Label Number'].values\n",
    "mnb_prediction_class_nums = [label_data_map[item] for item in mnb_predictions]\n",
    "meu.display_confusion_matrix_pretty(true_labels=test_label_nums,\n",
    "                                    predicted_labels=mnb_prediction_class_nums,\n",
    "                                    classes=unique_class_nums)\n",
    "# can see that class labels 0, 15, 19 have lots of misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Name</th>\n",
       "      <th>Label Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Label Name  Label Number\n",
       "0              alt.atheism             0\n",
       "15  soc.religion.christian            15\n",
       "19      talk.religion.misc            19"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map_df[label_map_df['Label Number'].isin([0, 15,19])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4097,  8528,  7621, ..., 14979,  4772,  7800])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test document row numbers\n",
    "train_idx, test_idx = train_test_split(np.array(range(len(data_df['Article']))), \n",
    "                                      test_size=0.33, random_state=42)\n",
    "test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Predicted Name</th>\n",
       "      <th>Predicted Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>\\nDid you watch the games????\\n\\n</td>\n",
       "      <td>watch game</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>0.529729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8528</th>\n",
       "      <td>I too have been watching the IIsi speedup reports and plan to upgrade in\\nthe next few weeks.  T...</td>\n",
       "      <td>watch iisi speedup report plan upgrade next week plan build small board different crystal able s...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>0.446850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7621</th>\n",
       "      <td>\\nI think one (not ideal) solution is to use the\\ntracing utility (can't remember the name, sorr...</td>\n",
       "      <td>think one not ideal solution use trace utility not remember name sorry include corel draw w pack...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0.978992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>\\n     I am curious about knowing which commericial cars today\\n   have v engines.\\n\\n   V4 - I ...</td>\n",
       "      <td>curious know commericial car today v engine v not know v legend mr mr vw golf passat l vr inline...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15903</th>\n",
       "      <td>DH&gt;&gt;Does anyone out their have a mountain tape backup that I could compare\\nDH&gt;&gt;notes with, (jum...</td>\n",
       "      <td>dhdoes anyone mountain tape backup could compare dhnotes jumper setting software ect dhor anyone...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>0.374575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Article  \\\n",
       "4097                                                                     \\nDid you watch the games????\\n\\n   \n",
       "8528   I too have been watching the IIsi speedup reports and plan to upgrade in\\nthe next few weeks.  T...   \n",
       "7621   \\nI think one (not ideal) solution is to use the\\ntracing utility (can't remember the name, sorr...   \n",
       "4754   \\n     I am curious about knowing which commericial cars today\\n   have v engines.\\n\\n   V4 - I ...   \n",
       "15903  DH>>Does anyone out their have a mountain tape backup that I could compare\\nDH>>notes with, (jum...   \n",
       "\n",
       "                                                                                             Clean Article  \\\n",
       "4097                                                                                            watch game   \n",
       "8528   watch iisi speedup report plan upgrade next week plan build small board different crystal able s...   \n",
       "7621   think one not ideal solution use trace utility not remember name sorry include corel draw w pack...   \n",
       "4754   curious know commericial car today v engine v not know v legend mr mr vw golf passat l vr inline...   \n",
       "15903  dhdoes anyone mountain tape backup could compare dhnotes jumper setting software ect dhor anyone...   \n",
       "\n",
       "       Target Label               Target Name            Predicted Name  \\\n",
       "4097             10          rec.sport.hockey          rec.sport.hockey   \n",
       "8528              4     comp.sys.mac.hardware     comp.sys.mac.hardware   \n",
       "7621              1             comp.graphics             comp.graphics   \n",
       "4754              7                 rec.autos                 rec.autos   \n",
       "15903             3  comp.sys.ibm.pc.hardware  comp.sys.ibm.pc.hardware   \n",
       "\n",
       "       Predicted Confidence  \n",
       "4097               0.529729  \n",
       "8528               0.446850  \n",
       "7621               0.978992  \n",
       "4754               0.999800  \n",
       "15903              0.374575  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_probas = gs_mnb.predict_proba(test_corpus).max(axis=1)\n",
    "test_df = data_df.iloc[test_idx]\n",
    "test_df['Predicted Name'] = mnb_predictions\n",
    "test_df['Predicted Confidence'] = predict_probas\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Predicted Name</th>\n",
       "      <th>Predicted Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4304</th>\n",
       "      <td>\\nOK, here's at least one Christian's answer:\\n\\nJesus was a JEW, not a Christian.  In this context Matthew 5:14-19 makes\\nsense.  Matt 5:17 \"Do not think that I [Jesus] came to abolish the Law or...</td>\n",
       "      <td>ok least one christians answer jesus jew not christian context matthew make sense matt not think jesus come abolish law prophet not come abolish fulfill jesus live jewish law however culmination p...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.991289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>The Nicene Creed\\n\\nWE BELIEVE in one God the Father Almighty, Maker of heaven and earth, and of all things visible and invisible.\\nAnd in one Lord Jesus Christ, the only-begotten Son of God, bego...</td>\n",
       "      <td>nicene creed believe one god father almighty maker heaven earth thing visible invisible one lord jesus christ begotten son god begotten father world god god light light god god begotten not make o...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.989182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14513</th>\n",
       "      <td>iank@microsoft.com (Ian Kennedy) writes...\\n\\n\\nMore along the lines of Hebrews 12:25-29, I reckon...\\n\\n\\tSee that you refuse not him that speaks. For if they\\n\\tescaped not who refused him that ...</td>\n",
       "      <td>iankmicrosoft com ian kennedy write along line hebrews reckon see refuse not speak escape not refuse spake earth much shall not escape turn away speak heaven whose voice shake earth promise say ye...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.988875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16678</th>\n",
       "      <td>\\nJesus did not say that he was the fulfillment of the Law, and, unless\\nI'm mistaken, heaven and earth have not yet passed away. Am I mistaken?\\nAnd, even assuming that one can just gloss over th...</td>\n",
       "      <td>jesus not say fulfillment law unless mistaken heaven earth not yet pass away mistaken even assume one gloss portion word jesus really think accomplish not jesus say jew annul v say jesus record wo...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.985892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13764</th>\n",
       "      <td>: \\n: I am a Mormon.  I believe in Christ, that he is alive.  He raised himself\\n: [Text deleted]\\n:\\n: I learned that the concept of the Holy Trinity was never taught by Jesus\\n: Christ, that it ...</td>\n",
       "      <td>mormon believe christ alive raise text delete learn concept holy trinity never teach jesus christ agree council clergyman long christ ascend man no authority speak jesus never teach concept trinit...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.974706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       Article  \\\n",
       "4304   \\nOK, here's at least one Christian's answer:\\n\\nJesus was a JEW, not a Christian.  In this context Matthew 5:14-19 makes\\nsense.  Matt 5:17 \"Do not think that I [Jesus] came to abolish the Law or...   \n",
       "4237   The Nicene Creed\\n\\nWE BELIEVE in one God the Father Almighty, Maker of heaven and earth, and of all things visible and invisible.\\nAnd in one Lord Jesus Christ, the only-begotten Son of God, bego...   \n",
       "14513  iank@microsoft.com (Ian Kennedy) writes...\\n\\n\\nMore along the lines of Hebrews 12:25-29, I reckon...\\n\\n\\tSee that you refuse not him that speaks. For if they\\n\\tescaped not who refused him that ...   \n",
       "16678  \\nJesus did not say that he was the fulfillment of the Law, and, unless\\nI'm mistaken, heaven and earth have not yet passed away. Am I mistaken?\\nAnd, even assuming that one can just gloss over th...   \n",
       "13764  : \\n: I am a Mormon.  I believe in Christ, that he is alive.  He raised himself\\n: [Text deleted]\\n:\\n: I learned that the concept of the Holy Trinity was never taught by Jesus\\n: Christ, that it ...   \n",
       "\n",
       "                                                                                                                                                                                                 Clean Article  \\\n",
       "4304   ok least one christians answer jesus jew not christian context matthew make sense matt not think jesus come abolish law prophet not come abolish fulfill jesus live jewish law however culmination p...   \n",
       "4237   nicene creed believe one god father almighty maker heaven earth thing visible invisible one lord jesus christ begotten son god begotten father world god god light light god god begotten not make o...   \n",
       "14513  iankmicrosoft com ian kennedy write along line hebrews reckon see refuse not speak escape not refuse spake earth much shall not escape turn away speak heaven whose voice shake earth promise say ye...   \n",
       "16678  jesus not say fulfillment law unless mistaken heaven earth not yet pass away mistaken even assume one gloss portion word jesus really think accomplish not jesus say jew annul v say jesus record wo...   \n",
       "13764  mormon believe christ alive raise text delete learn concept holy trinity never teach jesus christ agree council clergyman long christ ascend man no authority speak jesus never teach concept trinit...   \n",
       "\n",
       "       Target Label         Target Name          Predicted Name  \\\n",
       "4304             19  talk.religion.misc  soc.religion.christian   \n",
       "4237             19  talk.religion.misc  soc.religion.christian   \n",
       "14513            19  talk.religion.misc  soc.religion.christian   \n",
       "16678            19  talk.religion.misc  soc.religion.christian   \n",
       "13764            19  talk.religion.misc  soc.religion.christian   \n",
       "\n",
       "       Predicted Confidence  \n",
       "4304               0.991289  \n",
       "4237               0.989182  \n",
       "14513              0.988875  \n",
       "16678              0.985892  \n",
       "13764              0.974706  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at mode misclassification instance sfor religion.misc and religion.christian\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "res_df = (test_df[(test_df['Target Name'] == 'talk.religion.misc')\n",
    "                 & (test_df['Predicted Name'] == 'soc.religion.christian')]\n",
    "          .sort_values(by=['Predicted Confidence'], ascending=False).head(5))\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Predicted Name</th>\n",
       "      <th>Predicted Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>This discussion on \"objective\" seems to be falling into solipsism (Eg: the\\nrecent challenge from Frank Dwyer, for someone to prove that he can actually\\nobserve phenomena).  Someones even made th...</td>\n",
       "      <td>discussion objective seem fall solipsism eg recent challenge frank dwyer someone prove actually observe phenomenon someone even make statement science subjective even atom subjective get bit silly...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.972266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>\\n\\nAtoms are not objective.  They aren't even real.  What scientists call\\nan atom is nothing more than a mathematical model that describes \\ncertain physical, observable properties of our surrou...</td>\n",
       "      <td>atom not objective not even real scientist call atom nothing mathematical model describe certain physical observable property surrounding subjective objective though approach scientist take discus...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.935916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>\\nI think that if a theist were truly objective and throws out the notion that\\nGod definitely exists and starts from scratch to prove to themselves that\\nthe scriptures are the whole truth then t...</td>\n",
       "      <td>think theist truly objective throw notion god definitely exist start scratch prove scripture whole truth person would no longer theist miss something people convert non theism theism bring non the...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.820338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6020</th>\n",
       "      <td>\\n\\n[\"it\" is Big Bang]\\n\\nSince you asked... from the Big Bang to the formation of atoms is about\\n10E11 seconds. As for the \"color\": bright. Very very bright. \\n\\n\\nI don't. I believe the curren...</td>\n",
       "      <td>big bang since ask big bang formation atom e second color bright bright not believe current theory cosmology fairly well support observational evidence not well support say evolution relativity an...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.788279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>In &lt;1ren9a$94q@morrow.stanford.edu&gt; salem@pangea.Stanford.EDU (Bruce Salem) \\n\\n\\n\\nThis brings up another something I have never understood.  I asked this once\\nbefore and got a few interesting r...</td>\n",
       "      <td>renaqmorrow stanford edu salempangea stanford edu bruce salem bring another something never understand ask get interesting response somehow not seem satisfied would nt not consider good source may...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.728910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       Article  \\\n",
       "4706   This discussion on \"objective\" seems to be falling into solipsism (Eg: the\\nrecent challenge from Frank Dwyer, for someone to prove that he can actually\\nobserve phenomena).  Someones even made th...   \n",
       "914    \\n\\nAtoms are not objective.  They aren't even real.  What scientists call\\nan atom is nothing more than a mathematical model that describes \\ncertain physical, observable properties of our surrou...   \n",
       "11820  \\nI think that if a theist were truly objective and throws out the notion that\\nGod definitely exists and starts from scratch to prove to themselves that\\nthe scriptures are the whole truth then t...   \n",
       "6020    \\n\\n[\"it\" is Big Bang]\\n\\nSince you asked... from the Big Bang to the formation of atoms is about\\n10E11 seconds. As for the \"color\": bright. Very very bright. \\n\\n\\nI don't. I believe the curren...   \n",
       "2334   In <1ren9a$94q@morrow.stanford.edu> salem@pangea.Stanford.EDU (Bruce Salem) \\n\\n\\n\\nThis brings up another something I have never understood.  I asked this once\\nbefore and got a few interesting r...   \n",
       "\n",
       "                                                                                                                                                                                                 Clean Article  \\\n",
       "4706   discussion objective seem fall solipsism eg recent challenge frank dwyer someone prove actually observe phenomenon someone even make statement science subjective even atom subjective get bit silly...   \n",
       "914    atom not objective not even real scientist call atom nothing mathematical model describe certain physical observable property surrounding subjective objective though approach scientist take discus...   \n",
       "11820  think theist truly objective throw notion god definitely exist start scratch prove scripture whole truth person would no longer theist miss something people convert non theism theism bring non the...   \n",
       "6020   big bang since ask big bang formation atom e second color bright bright not believe current theory cosmology fairly well support observational evidence not well support say evolution relativity an...   \n",
       "2334   renaqmorrow stanford edu salempangea stanford edu bruce salem bring another something never understand ask get interesting response somehow not seem satisfied would nt not consider good source may...   \n",
       "\n",
       "       Target Label         Target Name Predicted Name  Predicted Confidence  \n",
       "4706             19  talk.religion.misc    alt.atheism              0.972266  \n",
       "914              19  talk.religion.misc    alt.atheism              0.935916  \n",
       "11820            19  talk.religion.misc    alt.atheism              0.820338  \n",
       "6020             19  talk.religion.misc    alt.atheism              0.788279  \n",
       "2334             19  talk.religion.misc    alt.atheism              0.728910  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "res_df = (test_df[(test_df['Target Name'] == 'talk.religion.misc')\n",
    "              & (test_df['Predicted Name'] == 'alt.atheism')]\n",
    "          .sort_values(by=['Predicted Confidence'], ascending=False).head(5))\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}