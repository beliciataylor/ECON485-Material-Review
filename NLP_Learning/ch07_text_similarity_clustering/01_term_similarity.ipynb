{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit88832310db0f46c6ba1b97f6fce48e8e",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Similarity\n",
    "## subsection of _Text Similarity and Clustering_\n",
    "\n",
    "* Analyzing Term Similarity\n",
    "    1. Hamming Distance\n",
    "    2. Manhattan Distance\n",
    "    3. Euclidean Distance\n",
    "    4. Levenshtein Edit Distance\n",
    "    5. Cosine Distance and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Term Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character vectorization: mapping each character of term to corresponding unique number\n",
    "import numpy as np\n",
    "\n",
    "# fxn takes list of words and terms, returns corresponding character vector for words\n",
    "def vectorize_terms(terms):\n",
    "    terms = [term.lower() for term in terms]\n",
    "    terms = [np.array(list(term)) for term in terms]\n",
    "    terms = [np.array([ord(char) for char in term]) for term in terms]\n",
    "    return terms\n",
    "\n",
    "# example terms\n",
    "root = 'Believe'\n",
    "term1 = 'beleive'\n",
    "term2 = 'bargain'\n",
    "term3 = 'Elephant'\n",
    "\n",
    "terms = [root, term1, term2, term3]\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# perform character vectorization on strings\n",
    "# view representation in dataframe\n",
    "\n",
    "# character vectorization\n",
    "term_vectors = vectorize_terms(terms)\n",
    "\n",
    "# show vector representation\n",
    "vec_df = pd.DataFrame(term_vectors, index=terms)\n",
    "print(vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store above info\n",
    "root_term = root\n",
    "other_terms = [term1, term2, term3]\n",
    "\n",
    "root_term_vec = vec_df[vec_df.index == root_term].dropna(axis=1).values[0]\n",
    "other_term_vecs = [vec_df[vec_df.index == term].dropna(axis=1).values[0]\n",
    "                      for term in other_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming Distance\n",
    "* distance measured between two strings under the assumption that they are of equal length\n",
    "* number of positions that have different characters or symbols between two strings of equal length\n",
    "\n",
    "* consider two terms $u$ and $v$ of length $n$; denote hamming distance as follows:\n",
    "\n",
    "$$hd(u,v) = \\sum_{i=1}^n (u_i \\neq v_i)$$\n",
    "\n",
    "* can normalize by dividing the number of mismatches by the total length of the terms\n",
    "\n",
    "$$norm\\_hd(u,v) = \\frac{\\sum_{i=1}^n (u_i \\neq v_i)}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function computes hamming distance between two terms and can compute normalized distance\n",
    "def hamming_distance(u, v, norm=False):\n",
    "    if u.shape != v.shape:\n",
    "        raise ValueError('The vectors must have equal lengths.')\n",
    "    return (u != v).sum() if not norm else (u != v).mean()\n",
    "\n",
    "# measure hamming distance between root term and other terms\n",
    "# compute Hamming distance\n",
    "for term, term_vector in zip(other_terms, other_term_vecs):\n",
    "    print('Hamming distance between root: {} and term: {} is {}'.format(\n",
    "        root_term, term, hamming_distance(root_term_vec, term_vector, norm=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute normalized hamming distance\n",
    "for term, term_vector in zip(other_terms, other_term_vecs):\n",
    "    print('Normalized Hamming distance between root: {} and term: {} is {}'.format(\n",
    "        root_term, term, round(hamming_distance(root_term_vec, term_vector, norm=True),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan Distance\n",
    "* instead counting the number of mismatches, we subtract the difference between each pair of characters at each position of the two strings\n",
    "* the distance between two points in a grid based on a strictly horizontal or vertical path\n",
    "* mathematically denoted, where u and v are term terms of length n, as the following:\n",
    "\n",
    "$$ md(u,v) = ||u-v||_l = \\sum_{i=1}^{n} |u_i - v_i|$$\n",
    "\n",
    "* same assumption of hamming distance: two terms have equal lengths\n",
    "* can also have normalized manhattan distance\n",
    "\n",
    "$$norm\\_md(u,v) = \\frac{||u-v||_l}{n} = \\frac{\\sum_{i=1}^{n} |u_i - v_i|}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating manhattan distance with capability of calculating normalized manhattan distance\n",
    "def manhattan_distance(u, v, norm=False):\n",
    "    if u.shape != v.shape:\n",
    "        raise ValueError('The vectors must have equal lengths.')\n",
    "    return abs(u - v).sum() if not norm else abs(u - v).mean()\n",
    "\n",
    "# compute Manhattan distance\n",
    "for term, term_vector in zip(other_terms, other_term_vecs):\n",
    "    print('Manhattan distance between root: {} and term: {} is {}'.format(\n",
    "        root_term, term, manhattan_distance(root_term_vec, term_vector, norm=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute normalized Manhattan distance\n",
    "for term, term_vector in zip(other_terms, other_term_vecs):\n",
    "    print('Normalized Manhattan distance between root: {} and term: {} is {}'.format(\n",
    "        root_term, term, round(manhattan_distance(root_term_vec, term_vector, norm=True), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance\n",
    "* the shortest straight-line distance between two points\n",
    "* the formula, where u and v are vectorized text terms with length n, is the following:\n",
    "\n",
    "$$ed(u,v) = ||u-v||_2 = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function helps us compute the Euclidean distance between two terms\n",
    "def euclidean_distance(u,v):\n",
    "    if u.shape != v.shape:\n",
    "        raise ValueError('The vectors must have equal lengths.')\n",
    "    distance = np.sqrt(np.sum(np.square(u-v)))\n",
    "    return distance\n",
    "\n",
    "# compute Euclidean distance\n",
    "for term, term_vector in zip(other_terms, other_term_vecs):\n",
    "    print('Euclidean distance between root: {} and term: {} is {}'.format(\n",
    "        root_term, term, round(euclidean_distance(root_term_vec, term_vector), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Edit Distance\n",
    "* measure the distance between two sequences of strings based on their difference, similar to the concept behind the Hamming distance\n",
    "* the minimum number of edits needed in the form of additions, deletions, or subsitutions to change or convert one term to the other\n",
    "* character-based subsitutions: a single character can be edited in a single operation\n",
    "* length of two terms need not be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following pseudocode, implement levenshtein edit distance\n",
    "# returns final Levenshtein Edit distance and complete edit matrix between u and v\n",
    "# past terms directly in raw string not vector representations\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "def levenshtein_edit_distance(u, v):\n",
    "    # convert to lower case\n",
    "    u = u.lower()\n",
    "    v = v.lower()\n",
    "    # base cases\n",
    "    if u == v: return 0\n",
    "    elif len(u) == 0: return len(v)\n",
    "    elif len(v) == 0: return len(u)\n",
    "    # initialize edit distance matrix\n",
    "    edit_matrix = []\n",
    "    # initialize two distance matrices \n",
    "    du = [0] * (len(v) + 1)\n",
    "    dv = [0] * (len(v) + 1)\n",
    "    # du: the previous row of edit distances\n",
    "    for i in range(len(du)):\n",
    "        du[i] = i\n",
    "    # dv : the current row of edit distances    \n",
    "    for i in range(len(u)):\n",
    "        dv[0] = i + 1\n",
    "        # compute cost as per algorithm\n",
    "        for j in range(len(v)):\n",
    "            cost = 0 if u[i] == v[j] else 1\n",
    "            dv[j + 1] = min(dv[j] + 1, du[j + 1] + 1, du[j] + cost)\n",
    "        # assign dv to du for next iteration\n",
    "        for j in range(len(du)):\n",
    "            du[j] = dv[j]\n",
    "        # copy dv to the edit matrix\n",
    "        edit_matrix.append(copy.copy(dv))\n",
    "    # compute the final edit distance and edit matrix    \n",
    "    distance = dv[len(v)]\n",
    "    edit_matrix = np.array(edit_matrix)\n",
    "    edit_matrix = edit_matrix.T\n",
    "    edit_matrix = edit_matrix[1:,]\n",
    "    edit_matrix = pd.DataFrame(data=edit_matrix,\n",
    "                               index=list(v),\n",
    "                               columns=list(u))\n",
    "    return distance, edit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Levenshtein Edit distance between example terms\n",
    "for term in other_terms:\n",
    "    edit_d, edit_m = levenshtein_edit_distance(root_term, term)\n",
    "    print('Computing distance between root: {} and term: {}'.format(root_term, term))\n",
    "    print('Levenshtein edit distance is {}'.format(edit_d))\n",
    "    print('The complete edit distance matrix is depicted below')\n",
    "    print(edit_m)\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance and Similarity\n",
    "* have two terms represented in vectorized forms, cosine similarity gives us the measure of the cosine of the angle between them when they're represented as non-zero positive vectors in an inner product space\n",
    "* term vectors with similar orientation have scores closer to 1 (cos(0))\n",
    "* term vectors with a similarity score close to 0 (cos(90)) indicate unrelated terms\n",
    "* mathematical definition: let u and v be two term vectors. Then,\n",
    "$$u \\cdot v = ||u|| ||v|| cos(\\theta)$$\n",
    "* can derive cosine similarity from formula:\n",
    "$$cs(u,v) = cos(\\theta) = \\frac{u\\cdot v}{||u|| ||v||} = \\frac{\\sum_{i=1}^n u_i v_i}{\\sqrt{\\sum_{i=1}^n u_i^2}\\sqrt{\\sum_{i=1}^n v_i^2}}$$\n",
    "* use bag of characters vectorization to build these term vectors and $n$ is the number of unique characters across the terms under analysis\n",
    "* bag of characters vectorization is similar to bag of words models except here we compute frequency of each character in word\n",
    "* sequence or word orders are not taken into account here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "\n",
    "# take a list of words or terms and extract unique characters from it\n",
    "# from list of unique_chars get count for each character in each word\n",
    "# build bag of characters vectors\n",
    "def boc_term_vectors(word_list):\n",
    "    word_list = [word.lower() for word in word_list]\n",
    "    unique_chars = np.unique(\n",
    "                        np.hstack([list(word) \n",
    "                                   for word in word_list]))\n",
    "    word_list_term_counts = [{char: count \n",
    "                                  for char, count in np.stack(\n",
    "                                      np.unique(list(word), return_counts=True), axis=1)} \n",
    "                                     for word in word_list]\n",
    "    boc_vectors = [np.array([int(word_term_counts.get(char, 0)) \n",
    "                             for char in unique_chars]) \n",
    "                   for word_term_counts in word_list_term_counts]\n",
    "    return list(unique_chars), boc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of character vectorization\n",
    "import pandas as pd\n",
    "\n",
    "feature_names, feature_vectors = boc_term_vectors(terms)\n",
    "boc_df = pd.DataFrame(feature_vectors, columns=feature_names, index=terms)\n",
    "print(boc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store these in specific variables before computing cosine distances\n",
    "root_term_boc = boc_df[vec_df.index == root_term].values[0]\n",
    "other_term_bocs = [boc_df[vec_df.index == term].values[0] \n",
    "                   for term in other_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if using bag of characters based character frequencies for terms or bag of words based word frequencies for documents, the score will range from 0 to 1 because frequency vectors can never be negative; angle between two vectors cannot exceed 90 degrees\n",
    "* cosine distance is complementary to the similarity score and can be computed by following formula:\n",
    "$$cd(u,v) = 1-cs(u,v) = 1 - cos(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine distance based on formula\n",
    "def cosine_distance(u,v):\n",
    "    distance = 1.0 - (np.dot(u,v) / (np.sqrt(sum(np.square(u))) * np.sqrt(sum(np.square(v))))\n",
    "                     )\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test similarity between example terms using bag of character representations\n",
    "for term, boc_term in zip(other_terms, other_term_bocs):\n",
    "    print('Analyzing similarity between root: {} and term: {}'.format(root_term, term))\n",
    "    distance = round(cosine_distance(root_term_boc, boc_term), 2)\n",
    "    similarity = round(1 - distance, 2)\n",
    "    print('Cosine distance is {}'.format(distance))\n",
    "    print('Cosine similarity is {}'.format(similarity))\n",
    "    print('-'*40)"
   ]
  }
 ]
}