{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "* Unsupervised Lexicon-Based Models\n",
    "    1. Bing Liu's Lexicon\n",
    "    2. MPQA Subjectivity Lexicon\n",
    "    3. Pattern Lexicon\n",
    "    4. TextBlob Lexicon\n",
    "    5. AFINN Lexicon\n",
    "    6. SentiWordNet Lexicon\n",
    "    7. VADER Lexicon\n",
    "* Classifying Sentiment with Supervised Learning\n",
    "* Traditional Supervised Machine Learning Models\n",
    "* Newer Supervised Deep Learning Models\n",
    "* Advanced Supervised Deep Learning Models\n",
    "* Analyzing Sentiment Causation\n",
    "    1. Interpreting Predictive Models\n",
    "    2. Analyzing Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Lexicon-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "\n",
    "# import dataset\n",
    "dataset = pd.read_csv('movie_reviews.csv')\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_review_ids = [7626, 3533, 13010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment polarity: -0.3625\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: 0.16666666666666674\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: -0.037239583333333326\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TextBlob Lexicon\n",
    "import textblob\n",
    "\n",
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', textblob.TextBlob(review).sentiment.polarity)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7669\n",
      "Precision: 0.767\n",
      "Recall: 0.7669\n",
      "F1 Score: 0.7668\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.78      0.77      7510\n",
      "    negative       0.77      0.76      0.76      7490\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     15000\n",
      "   macro avg       0.77      0.77      0.77     15000\n",
      "weighted avg       0.77      0.77      0.77     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       5835     1675\n",
      "        negative       1822     5668\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(meu)\n",
    "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 0.1 else 'negative' \n",
    "                            for score in sentiment_polarity]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment polarity: -7.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: -3.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## AFINN Lexicon\n",
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True)\n",
    "\n",
    "# compute polarity of chosen four sample reviews\n",
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', afn.score(review))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7118\n",
      "Precision: 0.7289\n",
      "Recall: 0.7118\n",
      "F1 Score: 0.7062\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.85      0.75      7510\n",
      "    negative       0.79      0.57      0.67      7490\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     15000\n",
      "   macro avg       0.73      0.71      0.71     15000\n",
      "weighted avg       0.73      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6376     1134\n",
      "        negative       3189     4301\n"
     ]
    }
   ],
   "source": [
    "# predict sentiment on complete test dataset\n",
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]\n",
    "\n",
    "# evaluate model performance using utility function\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Polarity Score: 0.875\n",
      "Negative Polarity Score: 0.125\n",
      "Objective Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "## SentiWordNet Lexicon\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
    "print('Positive Polarity Score:', awesome.pos_score())\n",
    "print('Negative Polarity Score:', awesome.neg_score())\n",
    "print('Objective Score:', awesome.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review, verbose=False):\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if senti-synset if found\n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    # aggregate final score\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        # to display results in a nice table\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n",
    "                                         norm_neg_score, norm_final_score]], \n",
    "            columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], ['Predicted Sentiment', 'Objectivity', \n",
    "                                                                 'Positive', 'Negative', 'Overall']], \n",
    "                                  labels=[[0,0,0,0,0], [0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            negative        0.74      0.1     0.17   -0.07\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.74      0.2     0.06    0.14\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.81     0.12     0.07    0.05\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6296\n",
      "Precision: 0.6722\n",
      "Recall: 0.6296\n",
      "F1 Score: 0.6049\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.59      0.88      0.70      7510\n",
      "    negative       0.76      0.38      0.51      7490\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     15000\n",
      "   macro avg       0.67      0.63      0.60     15000\n",
      "weighted avg       0.67      0.63      0.60     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6601      909\n",
      "        negative       4647     2843\n"
     ]
    }
   ],
   "source": [
    "norm_test_reviews = tn.normalize_corpus(test_reviews)\n",
    "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VADER Lexicon\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment_vader_lexicon(review, \n",
    "                                    threshold=0.1,\n",
    "                                    verbose=False):\n",
    "    # pre-process text\n",
    "    review = tn.strip_html_tags(review)\n",
    "    review = tn.remove_accented_chars(review)\n",
    "    review = tn.expand_contractions(review)\n",
    "    \n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >= threshold else 'negative'\n",
    "    \n",
    "    if verbose:\n",
    "        # display detailed sentiment statistics\n",
    "        positive = str(round(scores['pos'], 2)*100)+'%'\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
    "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive, negative, neutral]],                     columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                    ['Predicted Sentiment', 'Polarity Score', 'Positive', 'Negative', 'Neutral']], \n",
    "               labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            negative           -0.8     0.0%    40.0%   60.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                                     \n",
      "  Predicted Sentiment Polarity Score Positive             Negative Neutral\n",
      "0            negative          -0.16    16.0%  14.000000000000002%   69.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            positive           0.49    11.0%    11.0%   77.0%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7109\n",
      "Precision: 0.7235\n",
      "Recall: 0.7109\n",
      "F1 Score: 0.7067\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.83      0.74      7510\n",
      "    negative       0.78      0.59      0.67      7490\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     15000\n",
      "   macro avg       0.72      0.71      0.71     15000\n",
      "weighted avg       0.72      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6235     1275\n",
      "        negative       3061     4429\n"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) \n",
    "                            for review in test_reviews]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiment with Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "import nltk\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "\n",
    "# import dataset\n",
    "dataset = pd.read_csv('data/movie_reviews.csv')\n",
    "\n",
    "# take a peek at the data\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "#train_reviews = reviews[:35000]\n",
    "#train_sentiments = sentiments[:35000]\n",
    "#test_reviews = reviews[35000:]\n",
    "#test_sentiments = sentiments[35000:]\n",
    "\n",
    "# normalize datasets\n",
    "#stop_words = nltk.corpus.stopwords.words('english')\n",
    "#stop_words.remove('no')\n",
    "#stop_words.remove('but')\n",
    "#stop_words.remove('not')\n",
    "\n",
    "#norm_train_reviews = tn.normalize_corpus(train_reviews)\n",
    "#norm_test_reviews = tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save objects\n",
    "filename1 = path_to_users + '/LearningCode/NLP_Learning/data/norm_train_reviews.pkl'\n",
    "#pickle.dump(norm_train_reviews, open(filename1, 'wb'))\n",
    "filename2 = path_to_users + '/LearningCode/NLP_Learning/data/norm_test_reviews.pkl'\n",
    "#pickle.dump(norm_test_reviews, open(filename2, 'wb'))\n",
    "\n",
    "# load objects\n",
    "norm_train_reviews = pickle.load(open(filename1, 'rb'))\n",
    "norm_test_reviews = pickle.load(open(filename2, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (35000, 2116271)  Test features shape: (15000, 2116271)\n",
      "TFIDF model:> Train features shape: (35000, 2116271)  Test features shape: (15000, 2116271)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2), sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)\n",
    "\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, \n",
    "      ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, \n",
    "      ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize logistic regression and support vector machines\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8979\n",
      "Precision: 0.898\n",
      "Recall: 0.8979\n",
      "F1 Score: 0.8979\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.89      0.90      0.90      7510\n",
      "    negative       0.90      0.89      0.90      7490\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6780      730\n",
      "        negative        801     6689\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regressioin model on BOW features\n",
    "lr_bow_predictions = meu.train_predict_model(classifier=lr, train_features=cv_train_features, \n",
    "                                             train_labels=train_sentiments, test_features=cv_test_features, \n",
    "                                             test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions, \n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8886\n",
      "Precision: 0.8889\n",
      "Recall: 0.8886\n",
      "F1 Score: 0.8886\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.88      0.90      0.89      7510\n",
      "    negative       0.90      0.87      0.89      7490\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6777      733\n",
      "        negative        938     6552\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "lr_tfidf_predictions = meu.train_predict_model(classifier=lr, train_features=tv_train_features, \n",
    "                                               train_labels=train_sentiments, test_features=tv_test_features, \n",
    "                                               test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions, \n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newer Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
