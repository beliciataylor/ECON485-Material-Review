{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Processing and Understanding Text_\n",
    "# Text Preprocessing and Wrangling\n",
    "    1. Removing HTML Tags\n",
    "    2. Text Tokenization\n",
    "    3. Removing Accented Characters\n",
    "    4. Expanding Contradictions\n",
    "    5. Removing Special Characters\n",
    "    6. Case Conversions\n",
    "    7. Text Corrections\n",
    "    8. Stemming\n",
    "    9. Lemmatization\n",
    "    10. Removing Stopwords\n",
    "    11. Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a name=\"generator\" content=\"Ebookmaker 0.8.9 by Project Gutenberg\"/>\\r\\n</head>\\r\\n  <body><p id=\"id00000\">Project Gutenberg EBook The Bible, King James, Book 1: Genesis</p>\\r\\n\\r\\n<p id=\"id00001\">Copyright laws are changing all over the world. Be sure to check the\\r\\ncopyright laws for your country before downloading or redistributing\\r\\nthis or any other Project Gutenberg eBook.</p>\\r\\n\\r\\n<p id=\"id00002\">This header should be the first thing seen when viewing this Project\\r\\nGutenberg file.  Please do not remove it.  Do not change or edit the\\r\\nheader without written permission.</p>\\r\\n\\r\\n<p id=\"id00003\">Please read the \"legal small print,\" and other information about the\\r\\neBook and Project Gutenberg at the bottom of this file.  Included is\\r\\nimportant information about your specific rights and restrictions in\\r\\nhow the file may be used.  You can also find out about how to make a\\r\\ndonation to Project Gutenberg, and how to get involved.</p>\\r\\n\\r\\n<p id=\"id00004\" style=\"margin-top: 2em\">**Welcome To The World of Free Plain Vanilla Electronic Texts'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.content\n",
    "\n",
    "# hard to see content with html tags\n",
    "print(content[1163:2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\n",
      "This eBook was produced by David Widger\n",
      "with the help of Derek Andrew's text from January 1992\n",
      "and the work of Bryan Taylor in November 2002.\n",
      "Book 01        Genesis\n",
      "01:001:001 In the beginning God created the heaven and the earth.\n",
      "01:001:002 And the earth was without form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "           light from the darkness.\n",
      "01:001:005 And God called the light Day, and the darkness he called\n",
      "           Night. And the evening and the morning were the first day.\n",
      "01:001:006 And God said, Let there be a firmament in the midst of the\n",
      "           waters,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# remove unnecessary tags\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[1163:2045])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Text Tokenization_\n",
    "sentences\n",
    "* sentence tokenization\n",
    "* default sentence tokenizer\n",
    "* pretrained sentence tokenizer models\n",
    "* punktsentencetokenizer\n",
    "* regextokenizer\n",
    "\n",
    "words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US Has unveiled the world's most powerful supercomputer called 'Summit, beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# loading text corpora\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US Has unveiled the world's most powerful supercomputer called 'Summit, \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \" \n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \" \n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \" \n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total characters in Alice in Wonderland\n",
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 100 characters in the corpus\n",
    "alice[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample text: 4\n",
      "Sample text sentences :-\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US Has unveiled the world's most powerful supercomputer called 'Summit, beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
      " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
      " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
      " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "\n",
    "print('Total sentences in sample text:', len(sample_sentences))\n",
    "print('Sample text sentences :-')\n",
    "print(np.array(sample_sentences))\n",
    "\n",
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:-')\n",
    "print(np.array(alice_sentences[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Sentence Tokenizer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids = 'ep-00-01-17.de')\n",
    "# total characters in corpus\n",
    "print(len(german_text))\n",
    "# first 100 characters in the corpus\n",
    "print(german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# default sentence tokenizer\n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "\n",
    "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
    "german_tokenizer = nltk.data.load(resource_url = 'tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "\n",
    "# verify the type of german_tokenizer\n",
    "# should be PunktSentence Tokenizer\n",
    "print(type(german_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if results of both tokenizers match\n",
    "# should be True\n",
    "print(german_sentences_def == german_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .'\n",
      " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .'\n",
      " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .'\n",
      " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .'\n",
      " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
     ]
    }
   ],
   "source": [
    "# print first 5 sentences of the corpus\n",
    "print(np.array(german_sentences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US Has unveiled the world's most powerful supercomputer called 'Summit, beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "print(np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US Has unveiled the world's most powerful supercomputer called 'Summit, beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern = SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "print(np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'Has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
       "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'Has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway',\n",
       "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TokTokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'Has',\n",
       "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'\", 'Summit', ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'\", 's', 'Sunway',\n",
       "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "words = tokenizer.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', 's', 'most', 'powerful', 'supercomputer',\n",
       "       'beats', 'China', 'The', 'US', 'Has', 'unveiled', 'the', 'world',\n",
       "       's', 'most', 'powerful', 'supercomputer', 'called', 'Summit',\n",
       "       'beating', 'the', 'previous', 'record', 'holder', 'China', 's',\n",
       "       'Sunway', 'TaihuLight', 'With', 'a', 'peak', 'performance', 'of',\n",
       "       '200', '000', 'trillion', 'calculations', 'per', 'second', 'it',\n",
       "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
       "       'which', 'is', 'capable', 'of', '93', '000', 'trillion',\n",
       "       'calculations', 'per', 'second', 'Summit', 'has', '4', '608',\n",
       "       'servers', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts'], dtype='<U13')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern to identify tokens themselves\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern = TOKEN_PATTERN, gaps = False)\n",
    "\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
       "       'beats', 'China.', 'The', 'US', 'Has', 'unveiled', 'the',\n",
       "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
       "       \"'Summit,\", 'beating', 'the', 'previous', 'record-holder',\n",
       "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
       "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
       "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
       "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
       "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
       "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
       "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern to identify tokens by using gaps between tokens\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern = GAP_PATTERN, gaps = True)\n",
    "\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (3, 10), (11, 18), (19, 23), (24, 32), (33, 47), (48, 53), (54, 60), (61, 64), (65, 67), (68, 71), (72, 80), (81, 84), (85, 92), (93, 97), (98, 106), (107, 120), (121, 127), (128, 136), (137, 144), (145, 148), (149, 157), (158, 171), (172, 179), (180, 186), (187, 198), (199, 203), (204, 205), (206, 210), (211, 222), (223, 225), (226, 233), (234, 242), (243, 255), (256, 259), (260, 267), (268, 270), (271, 273), (274, 278), (279, 284), (285, 287), (288, 292), (293, 295), (296, 302), (303, 314), (315, 320), (321, 323), (324, 331), (332, 334), (335, 341), (342, 350), (351, 363), (364, 367), (368, 375), (376, 382), (383, 386), (387, 392), (393, 401), (402, 407), (408, 418), (419, 423), (424, 426), (427, 430), (431, 435), (436, 438), (439, 442), (443, 449), (450, 457)]\n",
      "['US' 'unveils' \"world's\" 'most' 'powerful' 'supercomputer,' 'beats'\n",
      " 'China.' 'The' 'US' 'Has' 'unveiled' 'the' \"world's\" 'most' 'powerful'\n",
      " 'supercomputer' 'called' \"'Summit,\" 'beating' 'the' 'previous'\n",
      " 'record-holder' \"China's\" 'Sunway' 'TaihuLight.' 'With' 'a' 'peak'\n",
      " 'performance' 'of' '200,000' 'trillion' 'calculations' 'per' 'second,'\n",
      " 'it' 'is' 'over' 'twice' 'as' 'fast' 'as' 'Sunway' 'TaihuLight,' 'which'\n",
      " 'is' 'capable' 'of' '93,000' 'trillion' 'calculations' 'per' 'second.'\n",
      " 'Summit' 'has' '4,608' 'servers,' 'which' 'reportedly' 'take' 'up' 'the'\n",
      " 'size' 'of' 'two' 'tennis' 'courts.']\n"
     ]
    }
   ],
   "source": [
    "# how to obtain token boundaries for each token\n",
    "word_indices = list(regex_wt.span_tokenize(sample_text))\n",
    "print(word_indices)\n",
    "print(np.array([sample_text[start:end] for start, end in word_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inherited Tokenizers from RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPunktTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'Has',\n",
       "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'\", 'Summit', ',', 'beating', 'the',\n",
       "       'previous', 'record', '-', 'holder', 'China', \"'\", 's', 'Sunway',\n",
       "       'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200',\n",
       "       ',', '000', 'trillion', 'calculations', 'per', 'second', ',', 'it',\n",
       "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
       "       ',', 'which', 'is', 'capable', 'of', '93', ',', '000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4', ',',\n",
       "       '608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the',\n",
       "       'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhitespaceTokenizer\n",
    "* tokenizes sentences into words based on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
       "       'beats', 'China.', 'The', 'US', 'Has', 'unveiled', 'the',\n",
       "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
       "       \"'Summit,\", 'beating', 'the', 'previous', 'record-holder',\n",
       "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
       "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
       "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
       "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
       "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
       "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
       "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Robest Tokenizers with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
       "       list(['The', 'US', 'Has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
       "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
       "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "sents = tokenize_text(sample_text)\n",
    "np.array(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'Has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
       "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for sentence in sents for word in sentence]\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([US unveils world's most powerful supercomputer, beats China.,\n",
       "       The US Has unveiled the world's most powerful supercomputer called 'Summit, beating the previous record-holder China's Sunway TaihuLight.,\n",
       "       With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.,\n",
       "       Summit has 4,608 servers, which reportedly take up the size of two tennis courts.],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use spaCy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "\n",
    "text_spacy = nlp(sample_text)\n",
    "\n",
    "sents = np.array(list(text_spacy.sents))\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
       "       list(['The', 'US', 'Has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
       "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
       "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_words = [[word.text for word in sent] for sent in sents]\n",
    "np.array(sent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'Has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'\", 'Summit', ',', 'beating', 'the',\n",
       "       'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway',\n",
       "       'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of',\n",
       "       '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it',\n",
       "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
       "       ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word.text for word in text_spacy]\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
