{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit88832310db0f46c6ba1b97f6fce48e8e",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "* Data Retrieval\n",
    "* Data Preprocessing and Normalization\n",
    "* Building Train and Test Datasets\n",
    "* Feature Engineering Techniques\n",
    "    1. Traditional\n",
    "    2. Advanced\n",
    "* Classification Models\n",
    "    1. Multinomial Naive Bayes\n",
    "    2. Logistic Regression\n",
    "    3. Support Vector Machines\n",
    "    4. Ensemble Models\n",
    "    5. Random Forest\n",
    "    6. Gradient Boosting Machines\n",
    "* Evaluating Classification Models\n",
    "    1. Confusion Matrix\n",
    "* Building and Evaluating Our Text Classifier\n",
    "    1. Bag of Words Features with Classification Models\n",
    "    2. TF-IDF Features with Classification Models\n",
    "    3. Comparative Model Performance Evaluation\n",
    "    4. Word2Vec Embeddings with Classification Models\n",
    "    5. GloVe Embeddings with Classification Models\n",
    "    6. FastText Embeddings with Classification Models\n",
    "    7. Model Tuning\n",
    "    8. Model Performance Evaluation\n",
    "\n",
    "GitHub Link for Text_Classifier: [https://github.com/Apress/text-analytics-w-python-2e/blob/master/Ch05%20-%20Text%20Classification/text_normalizer.py]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "data_labels_map = dict(enumerate(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(18846, 3)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             Article  Target Label  \\\n0  \\n\\nI am sure some bashers of Pens fans are pr...            10   \n1  My brother is in the market for a high-perform...             3   \n2  \\n\\n\\n\\n\\tFinally you said what you dream abou...            17   \n3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...             3   \n4  1)    I have an old Jasmine drive which I cann...             4   \n5  \\n\\nBack in high school I worked as a lab assi...            12   \n6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...             4   \n7  \\n[stuff deleted]\\n\\nOk, here's the solution t...            10   \n8  \\n\\n\\nYeah, it's the second one.  And I believ...            10   \n9  \\nIf a Christian means someone who believes in...            19   \n\n                Target Name  \n0          rec.sport.hockey  \n1  comp.sys.ibm.pc.hardware  \n2     talk.politics.mideast  \n3  comp.sys.ibm.pc.hardware  \n4     comp.sys.mac.hardware  \n5           sci.electronics  \n6     comp.sys.mac.hardware  \n7          rec.sport.hockey  \n8          rec.sport.hockey  \n9        talk.religion.misc  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n      <th>Target Label</th>\n      <th>Target Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>My brother is in the market for a high-perform...</td>\n      <td>3</td>\n      <td>comp.sys.ibm.pc.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n      <td>17</td>\n      <td>talk.politics.mideast</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n      <td>3</td>\n      <td>comp.sys.ibm.pc.hardware</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1)    I have an old Jasmine drive which I cann...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>\\nIf a Christian means someone who believes in...</td>\n      <td>19</td>\n      <td>talk.religion.misc</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# building the dataframe\n",
    "corpus, target_labels, target_names = (data.data, data.target, [data_labels_map[label] for label in data.target])\n",
    "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels, 'Target Name': target_names})\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Empty documents: 515\n"
    }
   ],
   "source": [
    "total_nulls = data_df[data_df.Article.str.strip() == ''].shape[0]\n",
    "print(\"Empty documents:\", total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(18331, 3)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# remove all recrods with no textual content \n",
    "data_df = data_df[~(data_df.Article.str.strip() == '')]\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulled from github\n",
    "import nltk\n",
    "import spacy\n",
    "import unicodedata\n",
    "import contractions\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "#from textblob import Word\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    if bool(soup.find()):\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    else:\n",
    "        stripped_text = text\n",
    "    return stripped_text\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]  \n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, stopwords=stopword_list):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "\n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = contractions.fix(doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # stem text\n",
    "        if text_stemming and not text_lemmatization:\n",
    "        \tdoc = simple_porter_stemming(doc)\n",
    "\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "         # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a17b2b3477b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# normalize our corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnorm_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml_stripping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_expansion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccented_char_removal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lemmatization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_stemming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_char_removal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_digits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopword_removal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean Article'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-9d2c3f0fed51>\u001b[0m in \u001b[0;36mnormalize_corpus\u001b[0;34m(corpus, html_stripping, contraction_expansion, accented_char_removal, text_lower_case, text_stemming, text_lemmatization, special_char_removal, remove_digits, stopword_removal, stopwords)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# expand contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontraction_expansion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# lemmatize text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# just to keep negation if any in bi-grams\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "# normalize our corpus\n",
    "norm_corpus = normalize_corpus(corpus=data_df['Article'], html_stripping=True, contraction_expansion=True, accented_char_removal=True, text_lower_case=True, text_lemmatization=True, text_stemming=False, special_char_removal=True, remove_digits=True, stopword_removal=True, stopwords=stopword_list)\n",
    "\n",
    "data_df['Clean Article'] = norm_corpus\n",
    "\n",
    "# view sample data\n",
    "data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}